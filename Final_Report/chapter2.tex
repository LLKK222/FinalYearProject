% Chapter 2

\chapter{Background}

\label{cha:background}

%----------------------------------------------------------------------------------------

In this chapter we will describe all the technical background required to understand and detail the different settings we investigate.
Initially, we will highlight some basic results in graph theory.
Then, we will outline the problem of community detection and present statistical models used to generate random graphs with community structure to be used as a testing playground for algorithms.
Following this, we will discuss basic concepts within finance required to understand the behaviour of financial assets that will provide the motivation for applying community detection algorithms to financial networks.

%-----------------------------------------------------
%   Preliminaries in Networks Background Section
%-----------------------------------------------------

\section{Graph Theory Preliminaries}
\label{sec:graphTheoryBackground}

We assume the reader is familiar with some basic concepts in linear algebra such as matrix multiplication, eigenvectors and eigenvalues of matrices. Rather, we will cover some basic tools within spectral graph theory using definitions from \cite{For10,New06a, Spi12, Spi07}. Spectral graph theory is the study of graphs through the eigenvalues and eigenvectors of matrices associated with the graphs \cite{Spi12}. We begin by defining some basic notions about graphs.
\begin{definition}
	\label{def:graph}
	A \emphT{graph} $\graphvar{G}$ is a pair of sets (V,E), where V is a set of \emphT{vertices} or \emphT{nodes} and $E \subset V^{2}$, the set of unordered pairs of elements of V. The elements of E are called \emphT{edges} or \emphT{links}.
\end{definition}
\begin{definition}
	\label{def:undirectedGraph}
	A graph $\graphvar{G} = (V,E)$ is called \emphT{undirected} if for all $v,w \in V$: $(v,w) \in E \iff (w,v) \in E$. Otherwise, $\graphvar{G}$ is called \emphT{directed}.
\end{definition}
\begin{definition}
	\label{def:weightedGraph}
	A \emphT{weighted} graph is a graph where a number (weight) is assigned to each edge.
\end{definition}
We will assume, without loss of generality, that $V = \{1,\dots,n\}$. See \cref{fig:exampleGraphSmall} for an example of an undirected graph with seven vertices and eleven edges.
\begin{definition}
	\label{def:subGraph}
	A graph $\graphvar{G}^{\prime} = (V^{\prime},E^{\prime})$ is a \emphT{subgraph} of $\graphvar{G} = (V,E)$ if $V^{\prime} \subset V$ and $E^{\prime} \subset E$. If $\graphvar{G}^{\prime}$ contains all edges of $\graphvar{G}$ that join vertices of $V^{\prime}$, one says that the subgraph $\graphvar{G}^{\prime}$ is \emphT{induced} or \emphT{spanned} by $V^{\prime}$.
\end{definition}
\begin{definition}
	\label{def:cuts}
	A partition of the vertex set V in two subsets S and $V-S$ is called a \emphT{cut}. The \emphT{cut size} is the number of edges of $\graphvar{G}$ joining vertices of S with vertices of $V-S$.
\end{definition}
\begin{definition}
	\label{def:neighbourhoodNode}
	Two vertices are \emphT{adjacent} or \emphT{neighbours} if they are connected by an edge. The set of neighbours of a vertex $v$ is called \emphT{neighbourhood}, and denoted by $\Gamma(v)$.
\end{definition}
\begin{definition}
	\label{def:degreeNode}
	The \emphT{degree} $d_{v}$ of a vertex $v$ is the number of its neighbours, $\abs{\Gamma(v)}$.
\end{definition}
We will be interested in using certain graphs in the models, such as bipartite graphs.
\begin{definition}
	\label{def:bipartiteGraph}
	A \emphT{bipartite} graph, is a graph whose vertices can be decomposed into two disjoint sets such that no two vertices within the same set are adjacent.
\end{definition}
\begin{definition}
	\label{def:clique}
	A \emphT{clique} of an undirected graph is a subset of its vertices such that every pair of vertices in the subset are adjacent in the graph.
\end{definition}
An example of an undirected bipartite graph with nine vertices and eight edges is shown in \cref{fig:exampleGraphBipartite}, whilst an example of a clique within an undirected graph is shown in \cref{fig:exampleGraphClique}.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.6\linewidth]{figures/exampleGraphSmall.png}
		\caption{}
		\label{fig:exampleGraphSmall}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.6\linewidth]{figures/exampleGraphBipartite.png}
		\caption{}
		\label{fig:exampleGraphBipartite}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.6\linewidth]{figures/exampleGraphClique.png}
		\caption{}
		\label{fig:exampleGraphClique}
	\end{subfigure}
	\caption[Visualisations of example undirected graphs.]{\label{fig:exampleGraphs} A set of visualisations of undirected graphs. In \subref{fig:exampleGraphSmall}, the graph has seven nodes and eleven edges. In \subref{fig:exampleGraphBipartite}, a bipartite graph, with nine nodes (elements of disjoint sets are coloured black and red denoting membership) and eight edges, is shown. In \subref{fig:exampleGraphClique}, an undirected graph, with six nodes and six edges is shown, where the nodes coloured red form a clique within the graph.}
\end{figure}

There is a very close connection between graphs and matrices, since the whole information about the topology of a graph can be entailed in matrix form.
\begin{definition}
	\label{def:adjacencyMatrix}
	The \emphT{adjacency matrix}, $\matvar{A} \in \{0,1\}^{n \times n}$ of a graph $\graphvar{G} = (V,E)$, is a $n\times n$ matrix whose element $A_{ij}$ equals 1 if there exists an edge joining vertices i and j in $\graphvar{G}$, and zero otherwise.
\end{definition}
From \cref{def:adjacencyMatrix}, it follows that elements of the adjacency matrix, $\matvar{A}$, can be written as
\begin{equation}
	A_{ij} =
	\begin{cases}
		1 & \text{if } (i,j) \in E\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
Note that the sum of elements of the $i$-th row of the adjacency matrix yields the degree of node $i$ of the graph, $d_{i} = \sum_{j} A_{ij}$. Also, the adjacency matrix is symmetric if the graph is undirected.
\begin{definition}
	\label{def:weightedAdjacencyMatrix}
	The \emphT{weighted adjacency matrix}, $\matvar{A} \in \realsR^{n \times n}$ of a weighted graph $\graphvar{G} = (V,E)$, is a $n\times n$ matrix whose element $A_{ij}$ equals the weight of the edge connecting nodes $i$ and $j$, if it exists, and zero otherwise. 
\end{definition}

There are other matrices that have also been studied extensively in spectral graph theory, including the Laplacian which is applied in topics such as graph partitioning, synchronisation and graph connectivity \cite{For10}.
\begin{definition}
	\label{def:degreeMatrix}
	The \emphT{degree matrix}, $\matvar{D}$, of a graph $\graphvar{G} = (V,E)$, is a $n \times n$ diagonal matrix whose element $D_{ii}$ equals the degree of vertex $i$.
\end{definition}
From \cref{def:degreeMatrix}, it follows that elements of the degree matrix, $\matvar{D}$, can be written as
\begin{equation}
	 D_{ij} =
	\begin{cases}
		d_{i} & \text{if } i = j\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
\begin{definition}
	\label{def:unnormalisedLaplacianMatrix}
	The matrix $\matvar{L} = \matvar{D}  - \matvar{A} $ is called the \emphT{unnormalised Laplacian matrix}.
\end{definition}
From \cref{def:unnormalisedLaplacianMatrix}, it follows that elements of the unnormalised Laplacian matrix of a graph $\graphvar{G} = (V,E)$, $\matvar{L}$, can be written as
\begin{equation}
	L_{ij} =
	\begin{cases}
		d_{i} & \text{if } i = j\\
		-1 & \text{if } i \neq j \text{ and }  (i,j) \in E\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
\begin{definition}
	\label{def:normalisedLaplacianMatrix}
	The matrix $\widetilde{\matvar{L}} = \matvar{I}  - \matvar{D}^{-1/2}\matvar{A}\matvar{D}^{-1/2}$ is called the \emphT{normalised Laplacian matrix}, where $\matvar{I}$ is the $n \times n$ identity matrix.
\end{definition}
Note that from \cref{def:unnormalisedLaplacianMatrix,def:normalisedLaplacianMatrix}, the normalised Laplacian matrix can also be written as $\widetilde{\matvar{L}} = \matvar{D}^{-1/2}\matvar{L}\matvar{D}^{-1/2}$.

An important property of matrices is their spectra, which we shall analyse for certain matrices later in the report to motivate community detection algorithms.
\begin{definition}
	\label{def:spectrum}
	The \emphT{spectrum} of a graph $\graphvar{G}$ is the set of eigenvalues of its adjacency matrix, $\{\lambda_{1},\dots,\lambda_{n}\}$.
\end{definition}
\begin{definition}
	\label{def:spectralRadius}
	Let $\lambda_{1},\dots,\lambda_{n}$ be the eigenvalues of a matrix $\matvar{M} \in \realsR^{n \times n}$. The \emphT{spectral radius} is defined as $\rho(\matvar{M}) = \max\limits_{i} \abs{\lambda_{i}}$.
\end{definition}


%-----------------------------------------------------
%   Community Structure in Networks Background Section
%-----------------------------------------------------

\section{Community Structure in Networks}
\label{sec:communityStructureBackground}

An intuitive notion of communities within graphs involves the assignment of nodes to communities such that there are denser connections between nodes belonging to the same community, and sparser connections between nodes belonging to different communities.
If a graph exhibits this property, it is said to contain assortative community structure \cite{New06a,DKM+13,For10,New06b}.
For instance, within social networks, where nodes are users and edges between nodes represent interactions between the users, community structure within the graph corresponds to real-life communities consisting of the users and friendship circles.
The aim of community detection algorithms is to estimate or recover the node assignments.
The algorithms need to be efficient due to the moderate size of graphs realised in many real-world applications, so we require the computationally complexity to not be worse than nearly linear in the number of edges in the graph (approximately $\bigO{n^{2}\natlog{n}}$ where $n$ represents the number of nodes in the graph).
It is crucial to note that we are seeking to identify communities in graphs of a moderate size, and not very large scale graphs as are now increasingly studied in research for social networks applications (e.g. Facebook, Twitter, Google networks).
The reason for this decision lies in the very different approaches taken by researches for large scale applications than for small or moderate size networks.

Within the literature, the terms \emphT{groups} and \emphT{clusters} are synonymous with communities, and as such we will use all three terms interchangeably through the report; so the reader should note all these terms refer to the same notion of communities in graphs.
Moreover we also refer to the process of estimating node assignments as \emphT{partitioning} the network.

In order to help provide a setting where different algorithms may be compared, we wish to study particular models which generate random graphs.
One popular model is called the stochastic block model.
Many special cases of this model have been studied, but we consider two versions, both widely used in the literature.
Firstly, there is a model considered by Decelle et al. \cite{DKM+13} and Nadakuditi et al. \cite{NN12}, also known as the \emphT{planted partition model}.
Secondly, there is a model used by Montanari \cite{DM13,Mon13}, which we will refer to as the \emphT{hidden clique model}. 
We emphasise that we do not exclusively focus on detecting cliques for the latter model, but the name is simply convenient for reference in this report.

Let us define the stochastic block model following Decelle et al. \cite{DKM+13}.
The stochastic block model has parameters: $k$, $n_{a}$, $\matvar{P}$. $k$ represents the number of communities (or groups), $n_{a}$ refers to the expected fraction of nodes within each group $a$, for $1 \leq a \leq k$, and $\matvar{P}$ is a $k \times k$ matrix whose element $P_{ab}$ equals the probability of an edge occurring between nodes belonging to groups $a$ and $b$. It is known as the \emphT{affinity matrix}.
We proceed to generate a random directed graph, $\graphvar{G}$, consisting of $n$ nodes.
Firstly, though, assign, to each node $i$ of the graph, $\sigma_{i} \in \{1,\dots,k\}$, a label indicating which community the node belongs to.
These labels are chose independently, where, for each node $i$, $\probability{\sigma_{i} = a} = n_{a}$.
Let $\vecvar{u} = \transpose{[\sigma_{1},\dots,\sigma_{n}]}$ be the \emphT{ground-truth node assignments} of the graph.
The random graph is generated to have an adjacency matrix, $\matvar{A}$, whose elements are defined by
\begin{equation}
	\label{def:sbmAdjacencyMatrix}
	A_{ij} =
	\begin{cases}
		0 & \text{if } i = j\\
		X & \text{otherwise}
	\end{cases}
\end{equation}
where $X \sim \bernoulli{P_{\sigma_{i},\sigma_{j}}}$.

This formulation matches the intuition of community structure, that the connectivity between two nodes depends solely on the community memberships of the two nodes.
Note, also, that we do not allow self-loops.

The framework for testing community detection algorithms, which we will use, can now be summarised.
Firstly, we generate synthetic datasets, by creating random graphs from the models described in \cref{subsec:plantedPartitionModel,subsec:hiddenCliqueModel}, with varying parameters and known underlying ground-truth node assignments.
Then we use the synthetically-generated graphs as input to various algorithms (an appropriate model is chosen for each algorithm), which provides, as output, an estimate to the community assignments.
Finally, for each algorithm, we compare the estimated community assignments to the ground-truth values.
This provides a notion of performance and accuracy to compare between the algorithms.

We now describe two models; one is a special case of the stochastic block model, created by imposing specific properties on the parameters, and the other a slightly modified version.

We also stress to the reader that we are only considering \emphT{non-overlapping communities}, where each node may only belong to one particular community.

%-----------------------------------
%   Planted Partition Model Sub Section
%-----------------------------------

\subsection{Planted Partition Model}
\label{subsec:plantedPartitionModel}

We will consider the formulation of the planted partition model as given by Decelle et al. \cite{DKM+13}.
To construct the planted partition model, we consider the stochastic block model, with $n_{a} = 1/k$, and the affinity matrix defined by
\begin{equation}
	\label{def:ppmAffinityMatrix}
	P_{ab} =
	\begin{cases}
		p_{in} & \text{if } a = b\\
		p_{out} & \text{otherwise}
	\end{cases}
\end{equation}
These properties essentially result approximately equal number of nodes and edges within each community.
From this definition, $p_{in}$ represents the probability of an edge occurring between two nodes belonging to the same community and $p_{out}$ represents the probability of an edge occurring between two nodes belonging to the different communities.
From now on, we refer to $p_{in}$ and $p_{out}$ as the \emphT{edge occurrence probabilities} for this model.
We assume assortative structure so that $p_{in} > p_{out}$. This just matches the intuition of edges more likely to appear between nodes belonging to the same community than between nodes belonging to different communities.

An example of a random graph generated by the planted partition model is shown in \cref{fig:ppmUnlabelledAdjacencyMatrixPlot}.
We labelled nodes using $\sigma_{i} = 1 + (i \bmod{k})$ for $i = 1,\dots,n$ and generated the graph with $n = 300$, $k = 3$, $p_{in} = 0.7$, $p_{out} = 0.3$.
The adjacency matrix of this graph is plotted with a pixel shaded red if the element in the adjacency matrix, corresponding to the location of the pixel, equals 1; while a pixel is shaded white if the element equals 0.
Since we know the ground truth labelling of nodes, we can, without loss of generality, reorder the rows and columns of the adjacency matrix, such that it consists of blocks of nodes associated with the node community memberships.
This is plotted in \cref{fig:ppmLabelledAdjacencyMatrixPlot}. Note that since $k = 3$, there are $3 \times 3 = 9$ blocks, where the blocks are denser along the main diagonal since these correspond to edges between nodes belonging to the same community and $p_{in} > p_{out}$.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/ppmAdjacencyMatrix.png}
		\caption{}
		\label{fig:ppmUnlabelledAdjacencyMatrixPlot}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/ppmLabelledAdjacencyMatrix.png}
		\caption{}
		\label{fig:ppmLabelledAdjacencyMatrixPlot}
	\end{subfigure}
	\caption[Plots of adjacency matrices of graph generated by planted partition model.]{\label{fig:ppmAdjacencyMatricesPlots} A set of plots for unlabelled, \subref{fig:ppmUnlabelledAdjacencyMatrixPlot}, and labelled, \subref{fig:ppmLabelledAdjacencyMatrixPlot}, adjacency matrices for random graph generated by planted partition model. The graphs were generated with $n = 300$, $k = 3$, $p_{in} = 0.7$ and $p_{out} = 0.3$.}
\end{figure}

In \cref{fig:ppmExampleGraph}, we show a visualisation of an instance of a random graph generated by the planted partition model with parameters with $n = 240$, $k = 3$, $p_{in} = 0.2$, $p_{out} = 0.01$.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{figures/ppmExampleGraph.png}
	\caption[Visualisation of a graph generated by the planted partition model.]{\label{fig:ppmExampleGraph} A visualisation of an instance of a random graph generated by the planted partition model with $n = 240$, $k = 3$, $p_{in} = 0.2$ and $p_{out} = 0.01$.}
\end{figure}

Decelle et al. \cite{DKM+11} conjectured a phase transition for sparse graphs generated from the planted partition model, using non-rigorous ideas from statistical physics \cite{MNS12}.
Nadakuditi et al. \cite{NN12} used methods from random matrix theory to present an asymptotic analysis of spectra of random graphs to also demonstrate the presence of a phase transition.
Essentially, we can distinguish between a \textit{detectable} phase where it is possible to learn node assignments in a way that is correlated with the ground-truth node assignments of the graph, and an \textit{undetectable} phase, where learning is impossible.

Let us define, for convenience, the variables $c_{in} = np_{in}$ and $c_{out} = np_{out}$.
Consider a graph, generated by the planted partition model, and following the argument of \cite{NN12}, which we will not explain, one finds a transition occurring at the point
\begin{equation}
\label{eq:ppmPhaseTransitionK}
	c_{in} - c_{out} = \sqrt{k(c_{in} + (k-1)c_{out})}.
\end{equation}
In particular, let us consider the case where $k = 2$, so we find a transition at
\begin{equation}
\label{eq:ppmPhaseTransitionK=2}
	c_{in} - c_{out} = \sqrt{2(c_{in} + c_{out})}.
\end{equation}
Mossel et al. \cite{MNS12} proved the undetectable phase region of the conjecture given by equation \cref{eq:ppmPhaseTransitionK=2}.
That is to say, it is impossible to meaningfully recover the node assignments when $ c_{in} - c_{out} < \sqrt{2(c_{in} + c_{out})}$.
Massouli\'e \cite{Mas13} and then, independently using a different proof, Mossel et al. \cite{MNS13b} proved the detectable phase region of the conjecture, meaning it is possible to recover node assignments positively correlated with the ground-truth when $ c_{in} - c_{out} > \sqrt{2(c_{in} + c_{out})}$.
The techniques used to prove these results are beyond the scope of this report, however these results provide a very important limit on the ability of algorithms to detect communities (for the case of two underlying ground-truth communities).
This motivates the development of algorithms which can efficiently (in nearly linear time) detect communities, in the sparse regime, up to this limit.

%--------------------------------
%   Hidden Clique Model Sub Section
%--------------------------------

\subsection{Hidden Clique Model}
\label{subsec:hiddenCliqueModel}

We will consider the following formulation as explained by Montanari \cite{DM13,Mon13}.
To construct the hidden clique model, we consider the stochastic block model with some modifications.
We proceed to generate a graph with $n$ nodes and $k$ communities but with the affinity matrix, $\matvar{P}$, becoming a $(k+1) \times (k+1)$ matrix defined by
\begin{equation}
	\label{def:hcmAffinityMatrix}
	P_{ab} =
	\begin{cases}
		p_{in} & \text{if } a = b \text{, } a \leq k \text{, } b \leq k\\
		p_{out} & \text{otherwise}
	\end{cases}
\end{equation}
Another tweak is that we now consider the variable $n_{a}$ to represent the number of nodes within community $a$ (rather than the expected fraction of nodes). Note that the number of nodes within each community does not necessarily sum to $n$, since we consider them to be `hidden' within the graph.
Also, we are interested in the regime where the size of these communities is small relative to the size of the graph.
Once more, we assume assortative structure within the hidden communities so that $p_{in} > p_{out}$.

An example of a random graph generated by the hidden clique model is shown in \cref{fig:hcmUnlabelledAdjacencyMatrixPlot}.
We generated the graph with $n = 300$, $k = 3$, $p_{in} = 0.8$, $p_{out} = 0.2$, $n_{1} = 50$, $n_{2} = 40$, $n_{3} = 20$.
The adjacency matrix of this graph is plotted with a pixel shaded red if the element in the adjacency matrix, corresponding to the location of the pixel, equals 1; while a pixel is shaded white if the element equals 0.
Since we know the ground truth labelling of nodes, we can, without loss of generality, reorder the rows and columns of the adjacency matrix, such that it consists of blocks of nodes associated with the node community memberships.
This is plotted in \cref{fig:hcmLabelledAdjacencyMatrixPlot}. We can see three dense blocks of size $50$, $40$ and $20$ respectively, corresponding to the three hidden communities.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/hcmAdjacencyMatrix.png}
		\caption{}
		\label{fig:hcmUnlabelledAdjacencyMatrixPlot}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/hcmLabelledAdjacencyMatrix.png}
		\caption{}
		\label{fig:hcmLabelledAdjacencyMatrixPlot}
	\end{subfigure}
	\caption[Plots of adjacency matrices of graph generated by hidden clique model.]{\label{fig:hcmAdjacencyMatricesPlots} A set of plots for unlabelled, \subref{fig:hcmUnlabelledAdjacencyMatrixPlot}, and labelled, \subref{fig:hcmLabelledAdjacencyMatrixPlot}, adjacency matrices for random graph generated by hidden clique model. The graphs were generated with $n = 300$, $k = 3$, $p_{in} = 0.8$, $p_{out} = 0.2$, $n_{1} = 50$, $n_{2} = 40$, $n_{3} = 20$.}
\end{figure}

In \cref{fig:hcmExampleGraph}, we show a visualisation of an instance of a random graph generated by the hidden clique model with parameters with $n = 150$, $k = 1$, $n_{1} = 30$, $p_{in} = 0.3$, $p_{out} = 0.05$.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{figures/hcmExampleGraph.png}
	\caption[Visualisation of a graph generated by the hidden clique model.]{\label{fig:hcmExampleGraph} A visualisation of an instance of a random graph generated by the hidden clique model with $n = 150$, $k = 1$, $n_{1} = 30$, $p_{in} = 1.0$, $p_{out} = 0.1$. Nodes belonging to the hidden community, which in this case is in fact a clique, are coloured red whilst other nodes are coloured blue.}
\end{figure}

An interesting phase transition can also be derived for these models also. We follow the argument of Montanari \cite{DM13,Mon13} to show this. We consider the simplest case of the model with only one hidden community (i.e. $k = 1$).
We begin by generating a graph from the hidden clique model with $n$ nodes and one community.
Assume the $n_{1}$ nodes belonging to the hidden community make up a hidden community set, $\setvar{S}$ (so that $\cardinality{\setvar{S}} = n_{1}$). Define $\one_{n} \in \{1\}^{n}$ as the $n$-dimensional vector with every element equal to 1.
Also let $\one_{\setvar{S}}$ be the indicator variable for nodes belonging to the hidden community set.
Denote the adjacency matrix of the graph by $\matvar{A}$, where each element is defined by
\begin{equation}
	\label{def:hcmAdjacencyMatrix}
	A_{ij} \sim \bernoulli{p_{ij}}
\end{equation}
where 
\begin{equation}
	\label{def:hcmPijAdjacencyMatrix}
	p_{ij} =	
	\begin{cases}
		p_{in} & \text{if } i \in \setvar{S} \text{, } j \in \setvar{S} \\
		p_{out} & \text{otherwise}
	\end{cases}
\end{equation}
Then, we get the following
\begin{equation}
	\label{def:hcmExpectationAdjacencyMatrix}
	\expectation{\matvar{A}} = (p_{in} - p_{out})\one_{\setvar{S}}\transpose{\one_{\setvar{S}}} + p_{out}\one_{n}\transpose{\one_{n}}
\end{equation}
and
\begin{equation}
	\label{def:hcmVarianceAdjacencyMatrix}
	\variance{A_{ij}} = p_{out}(1-p_{out}) \text{ if } \{i,j\} \not\subseteq \setvar{S}
\end{equation}
Denote $\widetilde{\matvar{A}}$ as the \emphT{normalised adjacency matrix} of the graph, defined by
\begin{equation}
	\label{def:hcmNormlaisedAdjacencyMatrix}
	\widetilde{\matvar{A}} \equiv \frac{1}{\sqrt{np_{out}(1-p_{out})}} (\matvar{A} - p_{out}\one_{n}\transpose{\one_{n}})
\end{equation}
By taking the expectation and using \cref{def:hcmExpectationAdjacencyMatrix}, we obtain
\begin{equation}
	\label{def:hcmExpectationNormalisedAdjacencyMatrix}
	\expectation{\widetilde{\matvar{A}}} = \frac{1}{\sqrt{np_{out}(1-p_{out})}} (p_{in} - p_{out}) \one_{\setvar{S}}\transpose{\one_{\setvar{S}}}
\end{equation}
Let us write $\widetilde{\matvar{A}} = \expectation{\widetilde{\matvar{A}}} + (\widetilde{\matvar{A}} - \expectation{\widetilde{\matvar{A}}})$. Now using \cref{def:hcmExpectationNormalisedAdjacencyMatrix}, we get
\begin{equation}
	\label{def:hcmNormlaisedAdjacencyMatrix2}
	\widetilde{\matvar{A}} = \frac{1}{\sqrt{np_{out}(1-p_{out})}} (p_{in} - p_{out}) \one_{\setvar{S}}\transpose{\one_{\setvar{S}}} + (\widetilde{\matvar{A}} - \expectation{\widetilde{\matvar{A}}})
\end{equation}
Let us now define the following
\begin{equation}
	\label{def:hcmLambda}
	\lambda \equiv \frac{p_{in} - p_{out}}{\sqrt{np_{out}(1-p_{out})}}
\end{equation}
\begin{equation}
	\label{def:hcmU}
	\vecvar{u} \equiv \one_{\setvar{S}}
\end{equation}
\begin{equation}
	\label{def:hcmZ}
	\matvar{Z} \equiv \widetilde{\matvar{A}} - \expectation{\widetilde{\matvar{A}}}
\end{equation}
Then we can re-write \cref{def:hcmNormlaisedAdjacencyMatrix2} as
\begin{equation}
	\label{def:hcmNormlaisedAdjacencyMatrixDecomposed}
	\widetilde{\matvar{A}} = \lambda \vecvar{u}\transpose{\vecvar{u}} + \matvar{Z}
\end{equation}

One can interpret $\lambda$ as a signal-to-noise ratio, $\vecvar{u}$ as a signal (i.e. the ground-truth node assignments we which to infer) and $\matvar{Z}$ as zero-mean noise with \iid entries.
We have essentially represented the problem of inferring the hidden community from the graph by a problem of estimating a rank-1 matrix in noise. Notice that for the generalisation with $k$ communities, we would get a rank-k matrix plus noise for the normalised adjacency matrix.

Assume we generate a network associated with a normalised adjacency matrix, $\widetilde{\matvar{A}}$, defined in \cref{def:hcmNormlaisedAdjacencyMatrixDecomposed} and are interested in reconstructing the vector of node assignments, $\vecvar{u}$.
This problem has been investigated in many application under the guise of `Low-rank deformation of Wigner matrices' \cite{Mon13}.
Moreover much is known about the eigenvalue spectrum of such matrices. There is a very important spectral phase transition that exists: if $\lambda < 1$, the top eigenvector of the adjacency matrix, $\matvar{A}$, is orthogonal to the vector we wish to reconstruct (i.e. $\innerP{\vecvar{v}_{1}(\matvar{A}),\vecvar{u} } \approx 0$), whereas, if $\lambda > 1$, the top eigenvector of $\matvar{A}$ is correlated with the vector we wish to reconstruct and $\innerP{\vecvar{v}_{1}(\matvar{A}),\vecvar{u}} \approx (1 - \lambda^{-2})$ \cite{Mon13}.
In the latter regime, one eigenvector pops out of the semicircle lobe, as illustrated in \cref{fig:spectralPhaseTransitionWignerPlots}.
This particular eigenvector is associated with the eigenvalue $\lambda + \lambda^{-1}$ \cite{Mon13}.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/spectralPhaseTransitionWigner.png}
		\caption{}
		\label{fig:spectralPhaseTransitionWigner}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/spectralPhaseTransitionWigner2.png}
		\caption{}
		\label{fig:spectralPhaseTransitionWigner2}
	\end{subfigure}
	\caption[Plots illustrating spectral phase transition of Wigner Matrices.]{\label{fig:spectralPhaseTransitionWignerPlots} We plot the limiting spectral density of the normalised adjacency matrix under two regimes. We illustrate the case where $\lambda < 1$, in \subref{fig:spectralPhaseTransitionWigner} and the case where $\lambda > 1$, in \subref{fig:spectralPhaseTransitionWigner2}. The blue dot in \subref{fig:spectralPhaseTransitionWigner2} represents the eigenvalue ($\lambda + \lambda^{-1}$) associated with he eigenvector that pops-out of the main semicircle lobe. Both figures obtained from \cite{Mon13}.}
\end{figure}

This result is key since it specifically describes a threshold where traditional spectral methods such as standard Principal Component Analysis (PCA) will not work (i.e. when $\lambda < 1$) and when it will produce a reconstructed vector correlated with the ground-truth (i.e. when $\lambda > 1$).
Moreover, we now have sufficient motivation to investigate methods where we can do better; more specifically we wish to study algorithms where we can essentially `beat' this spectral threshold by producing a reconstructed vector correlated with the ground-truth in the regime where $\lambda < 1$.
There is hope of achieving the improvement over standard PCA since we observe that the structure of the principal eigenvector of the matrix has two special properties.
Firslty, it is \emphT{non-negative} (since the elements are node assignments or indicator variables and are thus either zero or one) and, secondly, it is \emphT{sparse} (since we are interested in the regime where the size of the hidden community or clique is small relative to the size of the graph).
We will see how to utilise of this observation in more detail in \cref{subsec:nonLinearPowerIteration} since it forms the basis of a class of algorithms.

%--------------------------------
%   Financial Background Section   
%--------------------------------

\section{Financial Networks}
\label{sec:financialNetworksBackground}

%-------------------------------------------
%   Prices Returns Financial Assets Sub Section
%-------------------------------------------

\subsection{Prices and Returns of Financial Assets}
\label{subsec:financialAssetsBackground}

Financial assets are instruments claiming to have monetary value that can be bought and sold. Financial assets can be separated into broad classes, with examples including stocks, bonds or real estate \cite{Kuh12d,BKM13}.
The values of these assets is reflected in their price, which varies with time. Investors may wish to decide between which asset classes to invest in at any time, a process known as \emphT{asset allocation} \cite{BKM13}.
Also, within a particular asset class, an investor wishes to allocate money to specific assets, a process known as \emphT{portfolio selection} \cite{BKM13}.
For this report, we will focus on portfolio selection of stocks in our application of community detection algorithms, due to data availability constraints; however a very similar scheme may be used to tackle asset allocation also.

Investors tend not to consider the prices of assets they have invested in, but rather the \emphT{return} generated. Let us consider a financial asset whose price at time $t$ is $p(t)$. One popular measure of return is called the \emphT{rate of return} \cite{Kuh12e,BKM13} at a time $t$, denoted by $r(t)$, which is defined as
\begin{equation}
	\label{eq:rateOfReturn}
	r(t_{0}) = \frac{p(t_{1}) - p(t_{0})}{p(t_{0})}
\end{equation}
where we can interpret $t_{1}$ as the time when the investor sold the asset, and $t_{0}$ as the time when the investor bought the asset.

Critically, the rate of return is sensitive to large changes for longer time horizons \cite{Onn02}. In particular, we can consider a different measure of return, which is equivalent to a return with a constant interest rate \cite{Onn02}. We can generalise the concept interest rates with the simple example of an investor placing money (investing) in a bank account, as explained in \cite{Kuh12c,Lue98}.
The amount of money initially invested is referred to as the \emphT{principal}. We then assume money grows by a multiplicative factor, where the gain is paid into the account by the bank. This process is often called \emphT{compounding}. The time at which the interest is compounded, is called the \emphT{compounding period}.
\emphT{Compound interest} involves interest being paid on both the principal and the accumulated interest up to the present \cite{Onn02}. Typically, we are interested in the number of compounding periods in one year (i.e. the number of times the interest on our principal is compounded each year) \cite{Kuh12c}. Denote the principal by $w_{0}$, the amount in the account at time $t$ by $w_{t}$, the interest rate by $y$ and the number of compounding periods in a year by $m$. Then the amount within the account holdings after 1 year is given by
\begin{equation}
	\label{eq:compoundInterest}
	w_{1} = w_{0} (1 + (y/m))^{m}
\end{equation}

We can imagine diving a year into infinitesimally small compounding periods, and then determine the effect of this continuous compounding by taking the limit of ordinary compounding \cite{Lue98}. Notice the total number of compounding periods in a length of $t$ years is given by $mt$. Thus the effect of continuous compounding is
\begin{equation}
	\label{eq:continuousCompounding}
	w_{t} = \lim_{m {\to} \infty} w_{0}(1 + (y/m))^{mt} = w_{0}\exp{yt}
\end{equation}

If we divide \cref{eq:continuousCompounding} by the initial investment $w_{0}$, and take the natural logarithm, we get a representation for the return, $r = yt$. This indicates that taking the natural logarithm results in a constant interest rate.
We have thus arrived at another measure for return, called the \emphT{logarithmic return}, which is defined by
\begin{equation}
	\label{eq:logarithmicReturn}
	r(t_{0}) = \natlog{p(t_{1})} - \natlog{p(t_{0})}
\end{equation}
where, once more, we can interpret $t_{1}$ as the time when the investor sold the asset, and $t_{0}$ as the time when the investor bought the asset.

There are several advantages to using logarithmic returns, as explained in \cite{QuaWp}, which we will briefly summarise.
Firstly, if we assume asset prices have a \emphT{log normal} distribution, then the logarithmic returns are conveniently normally distributed \cite{QuaWp}. The reasons why assuming a log normal distribution may be appropriate for dynamic pricing of assets is beyond the scope of this report.
Secondly, for small rates of return, the logarithmic return is approximately equal to the rate of return \cite{QuaWp}. To see this, consider the approximation result from \cref{eq:logarithApproximation} combined with \cref{eq:rateOfReturn,eq:logarithmicReturn}.
\begin{equation}
	\label{eq:logarithApproximation}
	\natlog{1 + x} \approx x \text{, for } x \ll 1
\end{equation}
Thirdly, we benefit from numerical stability since the addition of small numbers is numerically stable, whilst multiplying small numbers is subject to \emphT{arithmetic underflow} \cite{QuaWp}.

However, there are disadvantages to using the logarithmic return, including the issue that the derivation is only correct if the interest rate is constant \cite{QuaWp,Onn02}.
Nevertheless, the logarithmic return is widely used in the literature (e.g. see \cite{Onn02,OCK+02,OKK03,FPM+10,FPW+11,MG13}), and hence we shall use it for the rest of the report, and the reader should note we shall use the terms `return' and `logarithmic return' interchangeably from now on. An example plot of price and logarithmic return for a stock is shown in \cref{fig:priceAndLogReturn}.

%--- FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/priceAndLogReturnTimeSeries.png}
	\caption[Example plot for price and logarithmic return]{\label{fig:priceAndLogReturn} Plots of prices and logarithmic returns for Anglo American plc (AAL) between 2004 and 2014}
\end{figure}

%-------------------------------------------
%   Mean Variance Portfolio Theory Sub Section
%-------------------------------------------

\subsection{Mean-Variance Portfolio Theory}
\label{subsec:portfolioTheoryBackground}

The term \emphT{portfolio} relates to investing in a combination of different assets.
We can characterise a portfolio by \emphT{portfolio weights}, where the weight of an asset within the portfolio is given by the ratio of the value of the position in the asset divided by the total value of the portfolio.
We are particularly interested in the return of the portfolio, which is related to the mean of the returns of the individual assets that make up the portfolio and the risk of the portfolio, which is related to the variance of the returns of the individual assets.
This is the source of the term mean-variance portfolio theory.
We realise an intuitive and inherit trade-off between risk and return; if the investor wishes to realise a larger return, he must bear a higher risk.
A more detailed explanation of this relationship is given in \cite{Lue98,Kuh12e}.
Rather, we are simply interested in finding the most efficient, or \emphT{minimum-variance portfolio} for a given requested portfolio return (that is to say, the investor requests a specific expected return, and wishes to form a portfolio that has the lowest variance of all possible portfolio that can deliver the specified expected return).
We can formalise the mean-variance portfolio setting in the following way, which summarises the explanations from \cite{Onn02,Lue98,Kuh12e}.

Let $P$ be a portfolio comprising of $n$ assets, where the return of the portfolio is denoted by $r_{P}$ and the variance of the portfolio is denoted by $\sigma^{2}_{P}$.
Denote the return of asset $i$ by $r_{i}$, the variance of the asset $i$ return by $\sigma^{2}_{i} \equiv \variance{r_{i}}$, and the weight of asset $i$ in the portfolio by $w_{i}$.
Also let $X_{0}$ denote the total amount invested in the portfolio (i.e. initial investment in the portfolio by the investor) and $X_{0i}$ represent the the amount invested in asset $i$.
We select the amounts in the assets forming the portfolio such that
\begin{equation}
	\label{eq:portfolioAmounts}
	\sum_{i=1}^{n} X_{0i} = X_{0}
\end{equation}
We define the portfolio weights using
\begin{equation}
	\label{eq:portfolioWeights}
	w_{i} = \frac{X_{0i}}{X_{0}} \text{ for } i = 1,\dots,n
\end{equation}
meaning that
\begin{equation}
	\label{eq:portfolioWeightsSum}
	\sum_{i=1}^{n} w_{i} = 1
\end{equation}
Notice that a negative weight indicates a \emphT{short position} in that asset, and that the returns of the individual assets and portfolio are random variables.

In particular we can represent the return of the portfolio by
\begin{equation}
	\label{eq:portfolioReturn}
	r_{P} = \sum_{i=1}^{n} w_{i} r_{i}
\end{equation}
By using \cref{eq:portfolioReturn}, we obtain the expected return of the portfolio
\begin{equation}
	\label{eq:portfolioExpectedReturn}
	\expectation{r_{P}} = \sum_{i=1}^{n} w_{i} \expectation{r_{i}}
\end{equation}
and the variance of the portfolio return
\begin{equation}
	\label{eq:portfolioVariance}
	\variance{r_{P}} \equiv \sigma^{2}_{P}  = \sum_{i=1}^{n} \sum_{j=1}^{n} w_{i} w_{j} \rho_{ij} \sigma_{i} \sigma_{j}
\end{equation}
where $\rho_{ij}$ is the correlation coefficient of the returns of assets $i$ and $j$, defined as
\begin{equation}
	\label{eq:correlationCoefficient}
	\rho_{ij} = \frac{\covariance{r_{i},r_{j}}}{\sigma_{i} \sigma_{j}}
\end{equation}
Note that the standard deviation of the returns of the portfolio (or any asset) is often called its \emphT{volatility}.

This formulation serves a key question; given estimates of each assets returns, variances and covariances (which one can obtain from historical data), how does one pick a selection of these assets, for any given time period, in order to form the best portfolio for the investor?
We see in \cref{eq:portfolioVariance}, that by simply investing in assets which have a lower correlation with one another (i.e. a lower value of $\rho_{ij}$), we can reduce the variance, and thus, the risk of the portfolio.
This process is known as \emphT{diversification} \cite{Lue98,BKM13}.
Therefore, for any given time period (and possibly dynamically), finding groups of assets, where the returns have higher correlation within groups and lower correlation between groups would help by presenting `baskets' of assets that the investor can pick from knowing selecting from a range of baskets would be beneficial (of course which assets to select from inside the basket relates to the risk-return trade off).
This serves as the main motivation for the application of community detection algorithms within financial networks.

%-------------------------------------------
%   Constructing Financial Networks Sub Section
%-------------------------------------------

\subsection{Constructing Financial Networks}
\label{subsec:financialNetworksConstructionBackground}

From \cref{subsec:portfolioTheoryBackground}, we understand one way to help minimise risk in constructing portfolios involves analysing the correlation coefficients of returns between two assets.
In order to study all possible correlations between all available assets, we construct a weighted, undirected and fully-connected network of assets, which we call the \emphT{financial network}. The following model has been considered by \cite{PGR+99,OCK+02,OKK03,FPM+10,MG13}.

Let us consider the situation where the investor is faced with $n$ financial assets, and has access to historical price data for all these assets for $T$ time steps.
The time steps may be trading days, or weeks, for instance, and the appropriate choice will depend on the type of assets available.

We proceed to construct a graph with $n$ nodes, where each nodes represents an asset, and assign to the $i$-th node a single time series, denoted by $X_{i}$, which is defined as
\begin{equation}
	\label{eq:singleTimeSeries}
	X_{i} = \{x_{i}(1),\dots,x_{i}(T)\}
\end{equation}
where $x_{i}(t)$ describes the logarithmic return of asset $i$ at time $t$, defined by \cref{eq:logarithmicReturn}.
This time series describes the evolution of the logarithmic return of the asset over $T$ time steps.
We then model the weight of an edge connecting nodes $i$ and $j$ of the graph by the cross-correlation between the time series corresponding to assets $i$ and $j$.
We form a cross-correlation matrix, denoted by $\matvar{C}$, whose elements are defined by
\begin{equation}
	\label{eq:crossCorrelationMatrix}
	C_{ij} = \frac{\mean{X_{i} X_{j}} - \mean{X_{i}} \mean{X_{j}}}{\sqrt{\left[ \mean{X_{i}^{2}} - \mean{X_{i}}^{2} \right] \left[ \mean{X_{j}^{2}} - \mean{X_{j}}^{2} \right]}}
\end{equation}
where the $\mean{\cdots}$ notation denotes a time average, so that
\begin{equation}
	\label{eq:temporalMean}
	\mean{X_{i}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}(t)
\end{equation}
\begin{equation}
	\label{eq:temporalMeanSquare}
	\mean{X_{i}^{2}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}^{2}(t)
\end{equation}
\begin{equation}
	\label{eq:temporalMeanProduct}
	\mean{X_{i}X_{j}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}(t)x_{j}(t)
\end{equation}
We also assume each time series $X_{i}$ has been standardised (before we assign to node $i$) by using
\begin{equation}
	\label{eq:standardiseTimeSeries}
	X_{i} \coloneqq \frac{X_{i} - \mean{X_{i}}}{\sqrt{\mean{X_{i}^{2}} - \mean{X_{i}}^{2}}}
\end{equation}
so that
\begin{equation}
	\label{eq:zeroTemporalMean}
	\mean{X_{i}} = 0
\end{equation}
\begin{equation}
	\label{eq:unitTemporalVariance}
	\mean{X_{i}^{2}} - \mean{X_{i}}^{2} = 1
\end{equation}
Note that the cross correlation values is just a sample estimate for the correlation coefficient, $\rho_{ij}$, used in \cref{subsec:financialNetworksConstructionBackground}, calculated from the historical data.

We can then characterise the financial network by the correlation matrix, which we also refer to as the network's weighted adjacency matrix.

In \cref{fig:exampleCrossCorrelationMatrix}, we have plotted a correlation matrix using data of 80 stocks listed on the FTSE 100 (see \cref{app:listFTSE100Stocks} for a list) between 2011 and 2013. Notice that the main diagonal has all elements equal to one, as you would expect, and that there are very few negative elements (i.e. very few assets that are anti-correlated with one another).

%--- FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figures/correlationMatrix_FTSE100_n_80_T_2501.png}
	\caption[Example plot for a correlation matrix]{\label{fig:exampleCrossCorrelationMatrix} Example of a correlation matrix. Evaluated from an ensemble of 80 stocks listed on the FTSE 100 (see \cref{app:listFTSE100Stocks} for a list) using data between 01/01/2011 and 01/01/2013.}
\end{figure}

The problem statement can now summarised.
Given a financial network, how can we group nodes into communities where correlations are higher within the communities and lower between the communities?
Contrary to graphs with community structure described in \cref{sec:communityStructureBackground}, the weights of the edges rather than the topology of the network are crucial in determining community memberships.
In other words, we focus solely on the weighted adjacency matrix of the graph.
Also the reader should note that the correlations between asset returns will vary over time, and thus representing this relationship dynamically (rather than over a one long period of time) is very important since investors may wish to change their positions in order to react to the dynamics of market conditions.
