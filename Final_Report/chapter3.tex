% Chapter 3

\chapter{Community Detection Algorithms}

\label{cha:communityDetectionAlgorithms}

%----------------------------------------------------------------------------------------

In this chapter we introduce several community detection algorithms present in the literature that are based upon different approaches.
We introduce spectral clustering, modularity-based optimisation, non-linear power iteration and message-passing algorithms, with specific reference to their application on generative block models.
We seek to explain sufficient intuition motivating the algorithms in addition to summarising their mathematical derivations.

%-----------------------------------------------------
%   Spectral Clustering Section
%-----------------------------------------------------

\section{Spectral Clustering}
\label{sec:spectralClustering}

The basis of all spectral clustering algorithms is the transformation of a set of variables into the set of points in space whose coordinates are elements of eigenvectors of a matrix, and then the clustering of these points using well-known clustering algorithms \cite{Lux06,For10}.

We consider the spectral clustering algorithm described in \cite{Lux06,For10}, whose intuition is explained in \cite{Lux06,Spi07,For10}.

Firstly, we compute the Laplacian matrix of the network, using \cref{def:unnormalisedLaplacianMatrix}, where we assume $n$ nodes in the graph, as usual.
We then compute the $k$ eigenvectors of the Laplacian matrix associated with the $k$ largest eigenvalues.
Denote the eigenvectors by $\vecvar{u}_{1},\dots,\vecvar{u}_{k}$ and the eigenvalues by $\lambda_{1},\dots,\lambda_{k}$.
The eigenvectors represent a $k$-tuple of real numbers associated with each vertex in the graph. 
We think of this association as a mapping from the vertices into a $k$-dimensional space.
This embedding is characterised by $F : V \rightarrow \realsR^{k}$ where $F(i) = (\vecvar{u}_{1}^{(i)},\dots,\vecvar{u}_{k}^{(i)})$ and $\vecvar{u}_{j}^{(i)}$ denotes the $i$-th element of the $j$-th eigenvector.
We have essentially represented node $i$ of the graph as a point in a $k$-dimensional space where the coordinates are the $i$-th elements of all the top $k$ eigenvectors of the Laplacian matrix.
Finally, we apply the embedding as input to the popular \emphT{k-means clustering} algorithm.
The cluster memberships of the $n$ data points are precisely the estimated node assignments for the initial network.
Notice, from the definition of the Laplacian matrix, that the vector of all ones (i.e. a vector with every element equal to one) is the principal eigenvector, so these embedded values remain the same for all nodes.
Therefore, knowledge of this eigenvector does not help discriminate between different vertices, and hence the information is not useful.
Thus we shall only apply the embedding to the top $k-1$ eigenvectors (i.e. the top $k$ eigenvectors excluding the all-ones vector), and can therefore represent the mapping in a $k-1$ dimensional space.

To visualise what such an embedding looks like, refer to \cref{fig:SpectralClusteringEmbeddingVisualisation}, where we generated an example graph using the planted partition model and 3 communities (this is purposely chosen so we can easily identify the embedding in a 2-dimensional space).
The coordinates for each point are the corresponding entries in the 2 eigenvectors of the Laplacian matrix considered.
We labelled the ground-truth node assignments by colour in the figure (i.e. points with the same colour represent nodes belonging to the same community in the underlying graph), and we can see that a k-means clustering algorithm would be applied to return cluster memberships that match the ground-truth, since the data points corresponding to different clusters are well separated.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figures/embeddedVectorsModularityMethod_pin_0_8_pout_0_2.png}
	\caption[Visualisation of spectral clustering embedding.]{\label{fig:SpectralClusteringEmbeddingVisualisation} A visualisation of the embedding for the spectral clustering algorithm. The graph was generated using the planted partition model with $n=150$, $k=3$, $p_{in}=0.8$ and $p_{out}=0.2$. We have chosen $3$ distinct communities since we can easily display the embedding in a 2-dimensional space. The coordinates for each point are the corresponding entries in the 2 eigenvectors of the Laplacian matrix considered. We label the ground-truth node assignments by colour (i.e. points with the same colour represent nodes belonging to the same community in the graph), and we can see that a k-means clustering algorithm would be applied to return cluster memberships that match the true community memberships.}
\end{figure}

Note that the algorithm does require to compute $k$ eigenvectors of a matrix, and this can be achieved using the power method.
Also, the accuracy of the algorithm will largely depend on the k-means algorithm which has been shown to converge to local minima in a cost measure (rather than global), but despite this, has been shown to work well in practical applications \cite{Lux06,For10}.

%-----------------------------------------------------
%   Modularity-based Optimisation Section
%-----------------------------------------------------

\section{Modularity-based Optimisation}
\label{sec:modularityBasedOptimisation}
 
We introduce the algorithms, firstly considered by \cite{New06a,New06b}, by explaining the intuition behind `good' community partitions.
In essence, the key ingredient involves determining partitions of the network where there are fewer edges \emphT{than expected} between nodes belonging to different communities.
For instance, if the number of links between nodes associated between different communities is approximately the same as what one would expect to find given random assignment of edges within the network, then it is unlikely this provides evidence of meaningful community structure \cite{New06b}.
Moreover, we can also consider partitions where there are more edges than expected between nodes belonging to the same community.
\begin{definition}
	\label{def:nullModel}
	The \emphT{null model} with respect to a network, whose adjacency matrix is given by  $\matvar{A}$, is the random graph denoted by $\graphvar{G}$, where each edge has a probability of $\frac{d_{i}d_{j}}{2m}$ of occurring. $d_{i}$ is the degree of node $i$ and $2m \equiv \sum_{ij} A_{ij}$.
\end{definition}
The null model defined above is proposed as a baseline distribution if edges were randomly placed within the network.
\begin{definition}
	\label{def:modularity}
	Given a partition, $\vecvar{\sigma}$, of a network, the \emphT{modularity} is defined as: $Q(\vecvar{\sigma}) = \frac{1}{2m} \sum_{ij} \left(A_{ij} - \frac{d_{i}d_{j}}{2m} \right) \delta(\sigma_{i},\sigma_{j})$.
\end{definition}
The modularity is therefore considered a cost function for a partition of the network where larger modularity values indicate stronger community structure \cite{New06a}.
\begin{definition}
	\label{def:modularityMatrix}
	The \emphT{modularity matrix} is denoted by $\matvar{B}$, whose elements $B_{ij}$ are defined by $B_{ij} = A_{ij} - \frac{d_{i}d_{j}}{2m}$.
\end{definition}
The aim of modularity optimisation algorithms is to find a partition of the network with the maximum value of modularity associated.
Since searching over all possible partitions is exponential in the number of nodes of the network, the problem is NP-hard computationally \cite{New06b}.
Thus, we seek approximate methods that provide near-optimal solutions.

In the literature, there exists a variety of approximation algorithms for accurate and fast modularity optimisation, such as greedy algorithms, simulated annealing, spectral algorithms and extremal optimisation \cite{For10}.
Within this report, we describe all these algorithms but only implement and test the greedy agglomerative method on synthetic data, since we consider it to be a faster version whilst maintaining similar accuracy to other modularity optimisation algorithms.

%-------------------------------------------
%   Greedy Algorithms Sub Section
%-------------------------------------------

\subsection{Greedy Algorithm}
\label{subsec:greedyAlgorithm}

The greedy algorithm of Clauset et al. \cite{CNM04} starts with all nodes as single groups and successively mergers two groups to form a larger community such that the modularity of the new partition increases after the joining \cite{CNM04,For10}.
Moreover, the algorithm keeps, permanently, the merger with the largest increase in modularity (hence at each step we compute $\Delta Q$, the change in modularity, using \cref{def:modularity}).
This is iterated until no further increase in modularity is possible \cite{CNM04}.
Note that for a network with $n$ nodes and $m$ edges, the algorithm has complexity $\bigO{(m+n)n}$, or $\bigO{n^{2}}$ for a sparse graph \cite{For10}.

A different greedy algorithm has been proposed by Blondel et al. \cite{BGL+08}, that is also applicable for weighted networks \cite{For10}.
This algorithm is commonly known as the \emphT{Louvain method}, and we shall also often refer to the algorithm by this name.
We initialise each node to belong to an individual community, and then repeat the following two phases until there is no further increase in modularity possible.
In the first phase, we sequentially consider each node, and given node $i$, we compute the increase in modularity, $\Delta Q$, that results from moving node $i$ into a neighbour community, and then permanently select the transition that yields the greatest increase in modularity \cite{BGL+08,For10}.
In the second phase, two communities are connected if an edge exists between any node belonging to the communities \cite{BGL+08,For10}.
\Cref{fig:LouvainMethodIllustration} illustrates the two phases of the algorithm on an example network. This figure is taken from the Blondel et al. reference \cite{BGL+08}.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/louvainMethodIllustration.png}
	\caption[Illustration of greedy algorithm for modularity optimisation.]{\label{fig:LouvainMethodIllustration} Illustration of the two phases of the greedy method of \cite{BGL+08}. The first phase involves optimisation of modularity using local changes, and the second phase aggregates the nodes into communities. The two phases are repeated until no further improvement of modularity is possible. For this example, only two passes are required until termination. This figure is reprinted from the Blondel et al., reference \cite{BGL+08}.}
\end{figure}

%-------------------------------------------
%   Simulated Annealing Sub Section
%-------------------------------------------

\subsection{Simulated Annealing}
\label{subsec:simulatedAnnealing}

The simulated annealing algorithm of Kirkpatrick et al. \cite{KGV83} is an iterative procedure that explores a space of possible states looking for the global optimum of modularity, which we denote by $Q$ \cite{KGV83,For10}.
Updates from one state to another are accepted with probability 1 if the transition results in an increase in the modularity.
Otherwise, the update is only accepted with a small probability $\exp{-\beta \Delta Q}$, where $\Delta Q$ is the change in modularity (i.e. value of modularity after the transition minus the value before) and $\beta$ represents the inverse-temperature of the system \cite{KGV83,For10}.
The idea behind accepting a transition that results in a decrease in modularity with a small probability is to increase the chance of finding the global maximum (i.e. decrease the chance of converging towards local maxima) \cite{For10}.
The algorithm converges to a stable state at some point, depending on the number of states explored and how $\beta$ is varied, but it can be a good approximation for $Q$.

A more recent implementation by Guimera et al. \cite{GSA04} consists of iterations that involve both individual and collective steps.
Within the individual step, an individual node is moved to a community at random, whilst the collective step consists of merging two communities or splitting one community \cite{GSA04}.
Typically, each iteration involves $n^{2}$ individual steps and $n$ collective steps, where $n$ represents the number of nodes in the network \cite{For10}.
This method can approximate the true maximum of modularity very accurately, and note that, due to the variation of parameter selection (such as initial temperature and inverse-temperature chosen), an exact complexity cannot be estimated, but it is typically very slow and hence should only be used on small graphs \cite{For10}.

%-------------------------------------------
%   Extremal Optimisation Sub Section
%-------------------------------------------

\subsection{Extremal Optimisation}
\label{subsec:extremalOptimisation}

The extremal optimisation algorithm of Duch and Arenas \cite{DA05} is a heuristic search method that involves recursively bi-partitioning the network \cite{DA05,For10}.
It begins with a random partition and uses the contribution of each node to the modularity as a fitness measure with the movement of nodes with the lowest fitness value.
The fitness function value of a node $i$ is given by
\begin{equation}
	\label{eq:fitnessFunction}
	q_{i} = \kappa_{\sigma(i)} - d_{i}e_{\sigma(i)}
\end{equation}
where $d_{i}$ is the degree of node $i$, $\kappa_{\sigma(i)}$ is the number of neighbours node $i$ has in the community it belongs to, and $e_{\sigma(i)}$ is the fraction of edges in the network that connects at least one node which belongs to the community of node $i$ \cite{DA05,For10}.
Using this notation, one can re-write the modularity by $Q = \frac{1}{2m} \sum_{i} q_{i}$.
We also normalise the variables $q_{i}$ by dividing by $d_{i}$ to obtain
\begin{equation}
	\label{eq:normalisedFitnessFunction}
	\rho_{i} = \frac{\kappa_{\sigma(i)}}{d_{i}} - e_{\sigma(i)}
\end{equation}
so that $-1 \leq \rho_{i}  \leq 1$ for all $i$.
Therefore we have expressed the global cost function in terms of a sum over all vertices (through the local variables $\rho_{i}$) and, hence, we can optimise the global variable, $Q$, by optimising over the local variables \cite{For10}.
At each iteration of the algorithm we calculate $\rho_{i}$ for every node $i$ and move the node with the lowest value to the other community.
Note that this transition alters the overall partition so the fitness values need to be re-evaluated, whilst we repeat this process until no further improvement in the modularity is possible \cite{DA05,For10}.
Extremal optimisation has empirically shown to achieve similar accuracy to simulated annealing but with a faster run time \cite{For10}.

%-------------------------------------------
%   Spectral Algorithm Sub Section
%-------------------------------------------

\subsection{Spectral Algorithm}
\label{subsec:spectralAlgorithm}

We shall describe the spectral method of Newman \cite{New06a} using the derivation explained in \cite{New06a,New06b,For10}, and by firstly considering networks with only two ground-truth communities.
Recall the notation used for modularity and the modularity matrix in \cref{def:modularity,def:modularityMatrix}, and let $\vecvar{\sigma}$ represent the vector of node assignments, where $\sigma_{i} = 1$ if node $i$ belongs to class 1 and $\sigma_{i} = -1$ if it belongs to class 2.
Then the modularity can be written as
\begin{equation}
	\label{eq:rewriteModularity}
	\begin{split}
		&Q = \frac{1}{2m} \sum_{i,j} \left(A_{ij} - \frac{d_{i}d_{j}}{2m} \right) \delta(\sigma_{i},\sigma_{j}) \\
		&= \frac{1}{4m} \sum_{i,j} \left(A_{ij} - \frac{d_{i}d_{j}}{2m} \right) (\sigma_{i}\sigma_{j} + 1) \\
		&= \frac{1}{4m} \sum_{i,j} B_{ij} \sigma_{i} \sigma_{j} \\
		&= \frac{1}{4m} \transpose{\vecvar{\sigma}} \matvar{B} \vecvar{\sigma}
	\end{split}
\end{equation}
We can rewrite $\vecvar{\sigma}$ as a linear combination of the eigenvectors of $\matvar{B}$, which we denote by $\vecvar{u}_{1},\dots,\vecvar{u}_{n}$ (where we label in decreasing order corresponding to the magnitude of its associated eigenvalue), so that
\begin{equation}
	\label{eq:rewriteNodeAssignments}
	\vecvar{\sigma} = \sum_{i=1}^{n} (\transpose{\vecvar{u}_{i}} \vecvar{\sigma}) \vecvar{u}_{i}
\end{equation}
Using this result in \cref{eq:rewriteModularity} yields
\begin{equation}
	\label{eq:rewriteModularity2}
	Q = \sum_{i} (\transpose{\vecvar{u}_{i}} \vecvar{\sigma}) \transpose{\vecvar{u}_{i}} \matvar{B} \sum_{j} (\transpose{\vecvar{u}_{j}} \vecvar{\sigma}) \vecvar{u}_{j} = \sum_{i=1}^{n} (\transpose{\vecvar{u}_{i}} \vecvar{\sigma})^{2} \lambda_{i}
\end{equation}
where $\lambda_{i}$ is the eigenvalue of $\matvar{B}$ associated with the eigenvector $\vecvar{u}_{i}$.
We aim to maximise the modularity by choosing the elements of $\vecvar{\sigma}$. From \cref{eq:rewriteModularity2}, we see this can be achieved by increasing the weights of the largest (i.e. most positive) eigenvalues.
However, we cannot just set $\vecvar{\sigma}$ to be proportional to the largest eigenvector, $\vecvar{u}_{1}$, as we imposed each element to be either +1 or -1.
Instead, we seek an approximate method, where we proceed to set the values of $\sigma_{i}$ based upon the sign of the $i$-th component of $\vecvar{u}_{1}$.
Essentially, the algorithm involves computing the leading eigenvector of the modularity matrix and then partitioning the nodes of the network into two groups according to the signs of the corresponding elements in the eigenvector.

In order to extend this approach for networks which contain more than two communities, we repeat this procedure for dividing any one community into two communities until no further sub-division increases the value of modularity, at which point the algorithm terminates.

The spectral method for optimising modularity is quite fast, since computing the leading eigenvector of the modularity matrix can be computed using the well-known power method.
Due to the special structure of the modularity matrix, the computation of the leading eigenvector takes $\bigO{m+n}$ time, so that one partition of the network takes $\bigO{n(m+n)}$ time or $\bigO{n^{2}}$ for a sparse graph \cite{For10}.
As we need to repeatedly partition the network in order to optimise the modularity, the overall complexity is $\bigO{dn(m+n)}$ where $d$ represents the depth of the hierarchical division.
Typically, in practice $d \approx \natlog{n}$, so that for sparse graphs, the total time taken for this spectral algorithm is approximately $\bigO{n^{2}\natlog{n}}$.
The spectral method is faster than simulated annealing and extremal optimisation, although not as fast as the greedy algorithm, based on empirical results \cite{For10}.
An added benefit is the extensibility of the spectral method to applications with weighted networks.

%-----------------------------------------------------
%   Belief Propagation Algorithm Section
%-----------------------------------------------------

\section{Belief Propagation Algorithm}
\label{sec:beliefPropagationAlgorithm}

The belief propagation (BP) algorithm of \cite{Has06,DKM+13} is designed to infer the group assignment from an instance of a graph generated by the planted partition model.
We shall describe Decelle et al.'s BP algorithm \cite{DKM+13} for the particular case of the planted partition model, but note that message passing algorithms such as BP are very similar in nature and incredibly useful in many other applications outside the problem of community detection \cite{For10}.
We also notify the reader that this algorithm is derived from principles in statistical physics, that we do not wish to delve into too much detail (since it is mostly inconsequential to the motivation of this report), and therefore shall present a higher-level overview and less detailed summary of the derivation by Decelle et al. \cite{DKM+13}. 

Recall the notation used for introducing the planted partition model in \cref{subsec:plantedPartitionModel}.
We begin by realising the probability the planted partition model with parameters $\theta = \{k,n_{a},\{P_{ab}\}\}$ generates a graph $\graphvar{G}$ consisting of $n$ nodes, with an associated adjacency matrix $\matvar{A}$ and estimated node assignments ${q_{i}}$ is given by
\begin{equation}
	\label{eq:probabilityPPMGeneratingGraph}
	\probability{\graphvar{G},\{q_{i}\} \given \theta} = \prod_{i \neq j} \left[ P_{q_{i}q_{j}}^{A_{ij}} (1-P_{q_{i}q_{j}})^{1-A_{ij}} \right] \prod_{i} n_{q_{i}}
\end{equation}
Assuming we know the underlying parameters of the block model, $\theta$, as well as observing the graph $\graphvar{G}$, we form the probability distribution over the group assignments by applying Bayes' theorem
\begin{equation}
	\label{eq:probabilityDistributionGroupAssignments}
	\probability{\{q_{i}\} \given \graphvar{G},\theta} = \frac{\probability{\graphvar{G},\{q_{i}\} \given \theta}}{\sum_{t_{i}} \probability{\graphvar{G},\{q_{i}\} \given \theta}}
\end{equation}
where $\{ t_{i} \}$ now represent the ground-truth node assignments (instead of $\{ \sigma_{i} \}$) for convenience.

We make an important connection between this problem and a result studied in statistical physics.
The \emphT{Boltzmann distribution} of a generalised Potts model (consider $\{ \sigma \}$ simply as a set of discrete variables) with Hamiltonian $H(\{ \sigma \})$ at inverse temperature $\beta$ is given by
\begin{equation}
	\label{eq:boltzmannDistributionPottsModel}
	\mu(\{ \sigma \}) = \frac{\exp{- \beta H(\{ \sigma \})}}{\sum_{\{ \sigma \}} \exp{- \beta H(\{ \sigma \})}}
\end{equation}
where each assignment of the variable $\{ \sigma \}$ has a weight $\exp{- \beta H(\{ \sigma \})}$ known as the \emphT{Boltzmann weight} and $Z(\beta) = \sum_{\{ \sigma \}} \exp{- \beta H(\{ \sigma \})}$ is called the \emphT{partition function} (i.e. the sum of the Boltzmann weights over all possible configurations) \cite{Pru14a,Pru14b,Sus}.
We now use that \cref{eq:probabilityDistributionGroupAssignments} corresponds to a Boltzmann distribution of a generalised Potts model at unit temperature (i.e. $\beta = 1$) with Hamiltonian given by
\begin{equation}
	\label{eq:hamiltonianDistributionGroupAssignments}
	H(\{ q_{i} \} \given \graphvar{G}, \theta) = - \sum_{i} \natlog{n_{q_{i}}} - \sum_{i \neq j} \left[ A_{ij} \natlog{c_{q_{i}q_{j}}} + (1-A_{ij}) \natlog{1 - \frac{c_{q_{i}q_{j}}}{n}} \right]
\end{equation}
where we define $c_{ab} = n P_{ab}$ to be convenient notation for sparse networks.
The corresponding Boltzmann distribution is
\begin{equation}
	\label{eq:boltzmannDistributionGroupAssignments}
	\mu(\{q_{i}\} \given \graphvar{G}, \theta) = \probability{\{q_{i}\} \given \graphvar{G},\theta}  = \frac{\exp{- H(\{q_{i}\} \given \graphvar{G}, \theta)}}{\sum_{\{q_{i}\}} \exp{- H(\{q_{i}\} \given \graphvar{G}, \theta)}}
\end{equation}
and the corresponding partition function is
\begin{equation}
	\label{eq:partitionFunctionGroupAssignments}
	Z(\graphvar{G}, \theta) = \sum_{\{q_{i}\}} \exp{- H(\{q_{i}\} \given \graphvar{G}, \theta)}
\end{equation}

The BP algorithm is essentially an iterative produce used to compute the partition function by ignoring the correlation between neighbours of a node while conditioning on its label \cite{DKM+13}.
Moreover, \cite{DKM+13} states that these correlations do not exist if the network of interactions between nodes is a tree and that, if the network observed is locally treelike, then the correlation terms become negligible making the BP algorithm exact in the asymptotic limit (i.e. as $n \rightarrow \infty$).
We can derive the BP algorithm update equations for the case of an undirected network (the directed case is a little more complex with additional equations).
We write the BP algorithm as a set of \emphT{messages}, denoted by $m_{q_{i}}^{i \rightarrow j}$, that are essentially marginal probabilities of a node $i$ belonging to a community $q_{i}$ excluding the evidence from its neighbour node $j$ \cite{DKM+13}.
The messages that govern this algorithm (similar to the derivation for loopy belief propagation \cite{Gil14a}) are then given by
\begin{equation}
	\label{eq:beliefPropagationMessageEquation}
	m_{u_{i}}^{i \rightarrow j} = \frac{1}{Z^{i \rightarrow j}} n_{u_{i}} \prod_{k \in N(i) \setminus j} \left[ \sum_{u_{k}} c_{u_{i},u_{k}}^{A_{ik}} \left(1 - \frac{c_{u_{i},u_{k}}}{n} \right)^{1-A_{ik}} m_{u_{k}}^{k \rightarrow i} \right]
\end{equation}
where $N(i)$ denotes the neighbourhood of node $i$, $\{ u_{i} \}$ represents the estimated node assignments at a particular iteration step of the algorithm and $Z^{i \rightarrow j}$ is a constant that normalises the messages ensuring they form a probability distribution (i.e. so that $\sum_{u_{i}} m_{u_{i}}^{i \rightarrow j} = 1$) \cite{DKM+13}.
In essence, this is the normalised belief in the node $i$ belonging to a community $u_{i}$ excluding the evidence from the node $j$ \cite{Gil14a}.
The algorithm involves applying \cref{eq:beliefPropagationMessageEquation} iteratively until a fixed point is reached with messages $\{ m_{q_{i}}^{i \rightarrow j}  \}$ \cite{DKM+13}.
We can write the marginal probability (or normalised belief) of node $i$ belonging to community $u_{i}$, denoted by $b_{u_{i}}^{i}$, as
\begin{equation}
	\label{eq:beliefPropagationBeliefEquation}
	b_{u_{i}}^{i} = \frac{1}{Z^{i}} n_{u_{i}} \prod_{k \in N(i)} \left[ \sum_{u_{k}} c_{u_{i},u_{k}}^{A_{ik}} \left(1 - \frac{c_{u_{i},u_{k}}}{n} \right)^{1-A_{ik}} m_{u_{k}}^{k \rightarrow i} \right]
\end{equation}
where, once more, we use a normalisation constant $Z^{i}$ \cite{DKM+13}.

The update for each step iteratively has $\bigO{n^{2}}$ complexity, since there are messages between every pair of nodes, but, for large sparse networks (i.e. for large $n$ and $c_{ab} = \bigO{1}$), we ignore terms of order in $n$ \cite{DKM+13}.
This implies a node simply sends the same message to all non-adjacent nodes (which is described as an external field in statistical physics) and thus the algorithm only needs to consider a number of messages equal to twice the number of edges in the networks, hence each iteration requires $\bigO{n}$ time \cite{DKM+13}.
We can understand this by considering the messages for two cases. Firstly, if nodes $i$ and $j$ are not adjacent,
\begin{equation}
	\label{eq:beliefPropagationMessageEquation2}
	m_{u_{i}}^{i \rightarrow j} = \frac{1}{Z^{i \rightarrow j}} n_{u_{i}} \prod_{k \not\in N(i) \setminus j} \left[ 1 - \frac{1}{n} \sum_{u_{k}} c_{u_{k}u_{i}} m_{u_{k}}^{k \rightarrow i} \right] \prod_{k \in N(i)} \left[ \sum_{u_{k}} c_{u_{k}u_{i}} m_{u_{k}}^{k \rightarrow i} \right] = b_{u_{i}}^{i} + \bigO{\frac{1}{n}}
\end{equation} 
 where the messages are independent of $j$ to the leading order \cite{DKM+13}. Now, if nodes $i$ and $j$ are adjacent in the network
\begin{equation}
	\label{eq:beliefPropagationMessageEquation3}
	m_{u_{i}}^{i \rightarrow j} = \frac{1}{Z^{i \rightarrow j}} n_{u_{i}} \prod_{k \not\in N(i)} \left[ 1 - \frac{1}{n} \sum_{u_{k}} c_{u_{k}u_{i}} m_{u_{k}}^{k \rightarrow i} \right] \prod_{k \in N(i) \setminus j} \left[ \sum_{u_{k}} c_{u_{k}u_{i}} m_{u_{k}}^{k \rightarrow i} \right] 
\end{equation}
We can now re-write \cref{eq:beliefPropagationBeliefEquation}  as
\begin{equation}
	\label{eq:beliefPropagationMessageEquation4}
	m_{u_{i}}^{i \rightarrow j} = \frac{1}{Z^{i \rightarrow j}} n_{u_{i}} \exp{-h_{u_{i}}} \prod_{k \in N(i) \setminus j} \left[ \sum_{u_{k}} c_{u_{k}u_{i}} m_{u_{k}}^{k \rightarrow i} \right] 
\end{equation}
where we ignored the $\bigO{\frac{1}{n}}$ term and have defined the external field by
\begin{equation}
	\label{eq:beliefPropagationExternalField}
	h_{u_{i}} = \frac{1}{n} \sum_{k} \sum_{u_{k}} c_{u_{k}u_{i}} b_{u_{k}}^{k}
\end{equation}
as explained by \cite{DKM+13}. The marginal probabilities can now also be re-written as
\begin{equation}
	\label{eq:beliefPropagationBeliefEquation2}
	b_{u_{i}}^{i} = \frac{1}{Z^{i}} n_{u_{i}} \prod_{k \in N(i)} \exp{-h_{u_{i}}} \prod_{j \in N(i)} \left[ \sum_{u_{j}} c_{u_{j}u_{i}} m_{u_{j}}^{j \rightarrow i} \right] 
\end{equation}
\Cref{eq:beliefPropagationMessageEquation4,eq:beliefPropagationExternalField,eq:beliefPropagationBeliefEquation2} define the crucial steps in each iteration of the BP algorithm, which is called `\textsc{BP-INFERENCE}' in \cite{DKM+13}.
The precise details can be seen in \cite{DKM+13}, but we summarise the key steps.
We start with the messages as a random vector, compute the initial marginal probabilities and external field.
We then iteratively, until a convergence criterion is met and/or for a fixed number of steps, apply the three update equations.
Finally, we output the estimated group assignment by using $\{ q_{i} \} = \argmax_{q} b_{q}^{i}$ (i.e. make the assignment of node $i$ to the group that maximises the marginal probabilities of node $i$ belonging to any of the groups).
As \cite{DKM+13} have analysed, the main body of the algorithm has $\bigO{n}$ complexity, while the number of iterations required until convergence is not known exactly and varies for different networks, so choosing an appropriate value for the maximum number of iterations based on a training set of networks would be practical.

Recall that, for the above derivations, we assumed knowledge of the underlying parameters of the block model, $\theta$.
However, in practice, given a new network, this is not the case, so we need to be able to learn the underlying parameters of the network before applying the BP inference algorithm.
Decelle et al. \cite{DKM+13} do provide another BP algorithm for learning or estimating the parameters that is based on the popular \emphT{expectation-maximisation} (EM) algorithm.
We do not aim to derive the update equations for this case, though, but will rather explain the intuition behind it.
In order to infer the parameters, we can aim to find the maximum a-posteriori estimator.
Let $\widehat{\theta}$ denote the estimator for the parameters, then $\widehat{\theta} = \argmax_{\theta} \probability{\theta \given \graphvar{G}}$.
Applying Bayes' theorem we find that
\begin{equation}
	\label{eq:inferenceParametersBayesTheorem}
	\probability{\theta \given \graphvar{G}} = \frac{\probability{\graphvar{G} \given \theta} \probability{\theta}}{\probability{\graphvar{G}}} = \frac{\probability{\theta}}{\probability{\graphvar{G}}} \sum_{\{ q_{i} \}} \probability{\graphvar{G},\{q_{i}\} \given \theta} 
\end{equation}
where $\{ q_{i} \}$, again, represents the estimated group assignments.
Therefore $\widehat{\theta}$ can also be found by maximising the partition function defined in \cref{eq:partitionFunctionGroupAssignments} over $\theta$.
By minimising this function for $\theta$ results in stationarity conditions known as the \emphT{Nishimori conditions} in statistical physics \cite{DKM+13}.
There is an iterative method for learning the parameters based on the Nishimori conditions and can be written in terms of messages to be used in a BP algorithm
\begin{equation}
	\label{eq:inferenceParametersBPMessage1}
	n_{a} = \frac{1}{n} \sum_{i} b_{a}^{i}
\end{equation}
\begin{equation}
	\label{eq:inferenceParametersBPMessage2}
	c_{ab} = \frac{1}{n_{a} n_{b} n} \sum_{(i,j) \in E} \frac{c_{ab} (m_{a}^{i \rightarrow j}m_{b}^{j \rightarrow i} + m_{b}^{i \rightarrow j}m_{a}^{j \rightarrow i})}{Z^{ij}}
\end{equation}
where we denote the set of edges of $\graphvar{G}$ by $E$ and denote $Z^{ij}$ as the BP estimate for the partition function defined by
\begin{equation}
	\label{eq:partitionFunctionBPEstimate}
	Z^{ij} = \sum_{a<b} c_{ab} (m_{a}^{i \rightarrow j}m_{b}^{j \rightarrow i} + m_{b}^{i \rightarrow j}m_{a}^{j \rightarrow i}) + \sum_{a} c_{aa} m_{a}^{i \rightarrow j} m_{a}^{j \rightarrow i} \text{ for } (i,j) \in E
\end{equation}
The EM algorithm uses the above BP update equations for the expectation step starting with random initialisation for $\theta$.
Combining the parameter learning steps with the inference of node assignments enables a complete BP algorithm, which Decelle et al. have described in \cite{DKM+13} and called `\textsc{BP-LEARNING}'.

For the purposes of our testing using the synthetically generated data from the planted partition model, we shall use the C++ implementation of this algorithm provided by Decelle et al. \cite{DKM+13} which is available from \cite{ModeNet}.

%-----------------------------------------------------
%   NLPI and AMP Algorithms Section
%-----------------------------------------------------

\section{NLPI and AMP Algorithms}
\label{sec:NLPIAndAMPAlgorithmsCommunityDetection}

The following algorithms aim to partition networks based upon the hidden clique model described in \cref{subsec:hiddenCliqueModel}, in order to identify the underlying hidden community.

%-------------------------------------------
%   Non-linear Power Iteration Sub Section
%-------------------------------------------

\subsection{Non-linear Power Iteration}
\label{subsec:nonLinearPowerIteration}

The intuition behind the non-linear power iteration (NLPI) method is fairly straightforward.
Recall we wish to reconstruct the node assignment vector denoted by $\vecvar{u}$. \Cref{def:hcmNormlaisedAdjacencyMatrixDecomposed} shows $\vecvar{u}$ is the principal eigenvector of a rank-1 matrix in noise, called the normalised adjacency matrix, and denoted by $\widetilde{\matvar{A}}$.
We use the standard power iteration algorithm with one extra step; we additionally apply a separable non-linear function that acts component-wise.
We choose the non-linear function to force the reconstructed vector to adhere to one of the properties desired. In particular we can apply `positive-part thresholding' \cite{Mon13}, where we keep only the positive elements of the vector (and set the negative elements to zero) at each iteration.
The following is a recursive definition of one iteration of the general approach, where $t$ indexes the iteration
\begin{equation}
	\label{eq:NLPIAlgorithm}
	\begin{split}
		&\vecvar{z}^{t+1} = \widetilde{\matvar{A}} \widehat{\vecvar{u}}^{t} \\
		&\widehat{\vecvar{u}}^{t} = f_{t}(\vecvar{z}^{t})
	\end{split}
\end{equation}
where
\begin{equation}
	\label{eq:NLPIAlgorithmConditions}
	\begin{split}
		&\widehat{\vecvar{u}}^{0} = \transpose{[1,\dots,1]} \\
		&\vecvar{z} = \transpose{[z_{1},\dots,z_{n}]} \\
		&f_{t}(\vecvar{z}) = \transpose{[f_{t}(z_{1}),\dots,f_{t}(z_{n})]}
	\end{split}
\end{equation}
Since we will consider positive-part thresholding,
\begin{equation}
	\label{eq:NLPIAlgorithmThresholding}
	f_{t}(z_{i}) =
	\begin{cases}
		z_{i}& \text{if } z_{i} > 0\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
for all $i=1,\dots,n$.

This algorithm takes advantage of the fact that the leading eigenvector is non-negative, an observation identified in \cref{subsec:hiddenCliqueModel}.
We will test this algorithm based on positive-part thresholding empirically for synthetically generated networks for varying SNR and sizes of the hidden community in \cref{sec:NLPIAndAMPAlgorithms}.
However, it is also important to analyse the algorithm theoretically, for instance by quantifying the (possible) improvement over spectral methods in different regimes.
Unfortunately, analysing this algorithm in terms of precise asymptotics is very difficult since there are dependencies existent after any number of iterations \cite{Mon13}.

%-------------------------------------------
%   Approximate Message Passing Sub Section
%-------------------------------------------

\subsection{Approximate Message Passing}
\label{subsec:approximateMessagePassing}

Although the NLPI method works well in practice (and we shall show this empirically later), we still seek an algorithm that can also be analysed theoretically. The \emphT{approximate message passing} (AMP) algorithm involves one modification to the NLPI, where a memory term is subtracted.
The following is a recursive definition of one iteration of the general approach, where $t$ again indexes the iteration
\begin{equation}
	\label{eq:AMPAlgorithm}
	\begin{split}
		&\vecvar{z}^{t+1} = \widetilde{\matvar{A}} \widehat{\vecvar{u}}^{t} - b_{t}\widehat{\vecvar{u}}^{t-1}\\
		&\widehat{\vecvar{u}}^{t} = f_{t}(\vecvar{z}^{t})
	\end{split}
\end{equation}
where we define
\begin{equation}
	\label{eq:AMPAlgorithmConditions}
	\begin{split}
		&\widehat{\vecvar{u}}^{-1} = \transpose{[0,\dots,0]} \\
		&\widehat{\vecvar{u}}^{0} = \transpose{[1,\dots,1]} \\
		&\vecvar{z} = \transpose{[z_{1},\dots,z_{n}]} \\
		&f_{t}(\vecvar{z}) = \transpose{[f_{t}(z_{1}),\dots,f_{t}(z_{n})]} \\
		&b_{t} \equiv \frac{1}{n}\sum_{i=1}^{n}f_{t}^{'}(z_{i})
	\end{split}
\end{equation}
and again consider positive-part thresholding.
We remark that the explicit formula for $b_{t}$ is chosen since it cancels the statistical bias (i.e. decorrelates) on $\widehat{\vecvar{u}}^{t+1}_{i}$ due to $\widehat{\vecvar{u}}^{\leq t}_{i}$.
The explanation for this result is beyond the scope of this report, but we refer the reader to \cite{DMM09,MDM10,BM11,Mon11,BKS13} for details.
