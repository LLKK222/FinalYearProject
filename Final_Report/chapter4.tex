% Chapter 4

\chapter{Experiments on Synthetic Data}

\label{cha:experimentsOnSyntheticData}

%----------------------------------------------------------------------------------------

In this chapter we aim to experiment with community detection algorithms on synthetically generated data.
We shall consider each algorithm described in the previous chapter, and experiment with data created from the appropriate generative model.
The goal of the experiments on synthetic data is to understand how the underlying
network structure, and the variation of parameters therein, affects the performance of different algorithms \cite{RLK12}.
In general the experiments will proceed as follows. We generate a network with the appropriate block model and specified parameters, with an underlying ground-truth node assignments. We then measure the accuracy of the specific algorithm investigated as we vary model parameters. This then allows us to draw conclusions regarding the relative performance of community detection algorithms in controlled conditions given by networks with common properties.
We conclude by discussing the advantages and issues with all the algorithms investigated and provide recommendations for their use in certain circumstances.

%-----------------------------------------------------
%   NLPI and AMP Algorithms Section
%-----------------------------------------------------

\section{NLPI and AMP Algorithms}
\label{sec:NLPIAndAMPAlgorithms}

We shall use the procedure outlined in \cite{Mon13} to test the NLPI and AMP algorithms, where the synthetic data used is simply an instance of a normalised adjacency matrix. Recall \cref{def:hcmNormlaisedAdjacencyMatrixDecomposed}, where we decomposed the matrix into signal (weighted by a signal-to-noise ratio term) plus noise.
We will consider the case of a network generated by the hidden clique model with $n$ nodes and one hidden community consisting of $k$ nodes. We may choose the community memberships arbitrarily. Using previous notation, let us denote $\vecvar{u}$ as the indicator variable for nodes belonging to the hidden community (i.e. so that precisely $k$ elements of $\vecvar{u}$ are equal to one and the rest equal zero), $lambda$ as the SNR and $\matvar{Z}$ as the noise term.
We set the elements of $\matvar{Z}$ to be $Z_{ij} \sim \normal{0,1/n}$ i.i.d. entries. Therefore given a value of $\lambda$ and our ground-truth community assignments, we can construct the normalised adjacency matrix, once more denoted by $\widetilde{\matvar{A}}$, using \cref{def:hcmNormlaisedAdjacencyMatrixDecomposed}.
This matrix will serve as input to the NLPI and AMP algorithms, which produce an output representing their reconstructed node assignments, which we shall denote by $\widehat{\vecvar{u}}$.
We shall represent the accuracy of the algorithms by the inner product of the ground-truth and reconstructed node assignments, $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$.
Notice this value lies between 0 and 1, and a larger number indicating improved reconstruction and better accuracy.

Now, with the framework set up, we can construct our own tests, seeking the accuracy as both the SNR and the size of the hidden community are varied.
Define a new variable $\varepsilon \equiv k/n$ representing the proportion of all nodes that belong to the hidden community, then we can construct a grid of points representing different values of $\lambda$ and $\varepsilon$.
We, finally, plot the values of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for each point on the grid to analyse the behaviour.

Utilising this approach with $n=500$ nodes, varying $\lambda$ between 0 and 1.2, choosing a grid resolution of 100 and running 50 iterations of the NLPI algorithm, we show in \cref{fig:NLPIAccuracyPlot}, the accuracy of the NLPI algorithm plotted against $\lambda$ and $\varepsilon$.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/NLPIMontanariSyntheticDataScalarProducts.png}
	\caption[Plot of accuracy for NLPI algorithm]{\label{fig:NLPIAccuracyPlot} A plot of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for the NLPI algorithm for different values of $\lambda$ and $\varepsilon$. The synthetic data is generated from a network with $n=500$ nodes and a grid with resolution 100, whilst the algorithm is run for 50 iterations.}
\end{figure}

The motivation for the NLPI algorithm is to detect the node assignments more accurately than standard PCA for small-sized hidden communities. The results can be analysed by focusing on the accuracy for small values of $\varepsilon$ and all ranges of $\lambda$.
We have previously discussed that the transition for standard PCA algorithms for small-size hidden communities is $\lambda = 1$, where, for values above this threshold, reconstruction is possible, and below, it is impossible.
\Cref{fig:NLPIAccuracyPlot} illustrates that for small values of $\varepsilon$ (e.g. between 0 and 0.2), the reconstructed vector is correlated with ground-truth (i.e. $\innerP{\vecvar{u},\widehat{\vecvar{u}}} > 0$).
In particular, we can get good reconstruction for values of $\lambda$ all the way down to 0.8. Thus we have shown to beat the spectral threshold!

Although the NLPI algorithm can be shown to beat the spectral threshold empirically by using the synthetic data strategy of \cite{Mon13}  that we used above, as we have already noted, analysing its asymptotic behaviour mathematically is not trivial.
This served as the motivation for the AMP algorithm where precise asymptotics could be derived theoretically (however this is beyond the scope of this report).
We shall now analyse the performance of the AMP algorithm using identical synthetic data (i.e. the same normalised adjacency matrix input for every value pair $(\lambda,\varepsilon)$ in the grid) as we tested the NLPI algorithm with. We also chose to run the AMP algorithms for 50 iterations.
We have plotted the accuracy for the AMP algorithm in \cref{fig:AMPAccuracyPlot}.

%---   ERROR
%%% need to add picture of (correct) AMP algorithm accuracy 

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/AMPMontanariSyntheticDataScalarProducts.png}
	\caption[Plot of accuracy for AMP algorithm]{\label{fig:AMPAccuracyPlot} A plot of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for the AMP algorithm for different values of $\lambda$ and $\varepsilon$. The synthetic data is generated from the same network with $n=500$ nodes as used in \cref{fig:NLPIAccuracyPlot}, and a grid with resolution 100, whilst the algorithm is run for 50 iterations.}
\end{figure}

\Cref{fig:AMPAccuracyPlot} illustrates that for small values of $\varepsilon$ (e.g. between 0 and 0.2), the reconstructed vector is correlated with ground-truth.
In particular, we get good reconstruction for values of $\lambda$ all the way down to 0.8. Thus we have shown to beat the spectral threshold once more!
We also note that the striking similarity between \cref{fig:NLPIAccuracyPlot} and \cref{fig:AMPAccuracyPlot} is to be expected given the formulations of these algorithms.

We have seen how NLPI and AMP algorithms can be used to detect hidden communities in networks generated by a hidden clique model.
In particular we have seen the empirical improvements over standard spectral methods such as PCA.
Moreover, the NLPI approach of applying a suitable non-linear function as an extra step to traditional power-iteration algorithm may be used for any problem where an eigenvector with special properties (e.g. sparsity or non-negativity) needs to be found.
Although the application is beyond the scope of this report, the AMP algorithm has had much success when applied to \emphT{compressed sensing}; we refer the reader to \cite{DMM09,MDM10,BM11,Mon11,BKS13} for more details. 