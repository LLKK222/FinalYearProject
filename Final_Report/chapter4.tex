% Chapter 4

\chapter{Experiments on Synthetic Data}

\label{cha:experimentsOnSyntheticData}

%----------------------------------------------------------------------------------------

In this chapter we aim to experiment with community detection algorithms on synthetically generated data.
We shall consider the algorithms described in the previous chapter, and use data created from the appropriate generative block model.
The goal of the experiments on synthetic data is to understand how the underlying
network structure, and the variation of parameters therein, affects the performance of different algorithms \cite{RLK12}.
In general the experiments will proceed as follows.
We generate a network with the appropriate block model and specified parameters, with underlying ground-truth node assignments set.
We then measure the accuracy of the specific algorithm investigated as we vary model parameters.
These parameters include the sparsity of the graph and the edge occurrence probabilities.
This then allows us to draw conclusions regarding the relative performance of community detection algorithms in controlled conditions given by networks with common properties.
We conclude by comparing the algorithms investigated by summarising their advantages and issues as well as providing recommendations for their use in particular empirical circumstances.

%----------------------------------------------------------------
%   Spectral Clustering and Modularity-Optimisation Algorithms Section
%----------------------------------------------------------------

\section{Spectral Clustering and Modularity-Optimisation Algorithms}
\label{sec:spectralClusteringModularityOptimisationAlgorithms}

We shall test the spectral clustering algorithm of \cref{sec:spectralClustering} and the greedy modularity optimisation method of \cref{subsec:greedyAlgorithm} using identical synthetic data generated using the planted partition model described in \cref{subsec:plantedPartitionModel}.
We aim to understand the quality (i.e. accuracy) of the partitions generated by the two algorithms as we vary both the sparsity of the network and the edge occurrence probabilities.
We have found very similar performance for all the algorithms as we increase the number of communities, and therefore, in the interests of being concise in our analysis, we only consider the case of graphs with two ground-truth communities.
Furthermore, we wish to test, empirically, how close we can reliably detect communities up to the phase transition point.
Additionally, we assume assortative community structure throughout.
We apply each of generated networks as input to the Laplacian spectral clustering and greedy modularity method, and measure the accuracy of reconstruction of the node assignments by computing the \emphT{overlap} using the definition by Decelle et al. \cite{DKM+11,DKM+13}.
Denote the overlap by $T$, and let the estimated node assignments be given by $\{q_{i}\}$ with ground-truth node assignments given by $\{\sigma_{i}\}$, then we define $T$ by
\begin{equation}
	\label{eq:overlap}
	T = \max_{\pi} \frac{\frac{1}{n} \sum_{i} \delta(\sigma_{i},\pi(q_{i})) - \frac{1}{k}}{1 - \frac{1}{k}}
\end{equation}
where $\pi$ ranges over the permutations on $q$ elements and $k$ represents the number of communities.
This definition means that $0 \le T \le 1$ with a higher value implying improved reconstruction and more accurate results, and an overlap of 0 meaning the algorithm is, on average, no better than random uniform selection of node assignments.

The traditional spectral clustering algorithm  has been implemented in MATLAB based on the description given in \cref{sec:spectralClustering}, whilst the MATLAB implementation for the greedy modularity algorithm has been obtained from Le Martelot \cite{ELM}.

We begin by considering the dense regime and generating 100 networks from the planted partition model with common parameters $n=200$, $k=2$, $p_{in}=0.9$.
However, we vary $p_{out}$ for each of the 100 networks to get a different value for the ratio $c_{out}/c_{in}$, signifying a variation in the edge occurrence probabilities.
The results are shown in \cref{fig:syntheticDataResultsPin0.9}, where the overlap is plotted in the vertical axis and the horizontal axis is the value for the ratio $c_{out}/c_{in}$.
We see very similar performance for both algorithms in this regime and can also identify the phase transition region described in \cref{subsec:plantedPartitionModel}, since there is a very sharp drop in overlap for a value of $c_{out}/c_{in} \approx 0.85$.
Overlap values for both algorithms for $c_{out}/c_{in} < 0.85$ are approximately 1 implying perfect reconstruction of the ground-truth node assignments, whilst for $c_{out}/c_{in} > 0.85$, the overlap drops very sharply towards 0.
Using \cref{eq:ppmPhaseTransitionK=2} we find that, for this specification of the model, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8564$.
Therefore we realise the performance of both algorithms matches up to the theoretical phase transition point in this dense regime.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/spectralClusteringSyntheticDataErrors_pin_0_9.png}
	\caption[Plot of overlap for spectral clustering and modularity methods in the dense regime.]{\label{fig:syntheticDataResultsPin0.9} A plot for the overlap of reconstructed node assignments of the Laplacian spectral clustering (blue) and greedy modularity optimisation methods (red) as the relative edge occurrence probabilities ($c_{out}/c_{in}$) are varied. 100 networks were generated with common parameters, $n=200$, $k=2$, $p_{in}=0.9$. Both algorithms perform very similarly in this regime and we can clearly identify the detectable and undetectable phases since the there is a very sharp drop in the overlap values for both algorithms. Using \cref{eq:ppmPhaseTransitionK=2} we find that, for this specification of the model, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8564$.}
\end{figure}

Let us now consider a sparser regime, where we again generate 100 networks from the planted partition model but now with common parameters $n=200$, $k=2$, $p_{in}=0.6$.
The results are shown in \cref{fig:syntheticDataResultsPin0.6}.
Notice that both algorithms perform similarly well for $c_{out}/c_{in} < 0.5$ but the modularity method seems to perform better until we get the sharp drop in overlap values.
This is a very important observation to note, that the sharp drop in overlap values, for both algorithms, occurs at $c_{out}/c_{in} \approx 0.75$.
Using \cref{eq:ppmPhaseTransitionK=2} we find that, for this specification of the model, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8256$, suggesting that, as the network gets sparser, both algorithms performance declines with respect to obtaining perfect reconstruction within the detectable phase.
These observations should not be a surprise given the derivation of the algorithms, and we will now explain known issues with both these algorithms that cause this decline in accuracy.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/spectralClusteringSyntheticDataErrors_pin_0_6.png}
	\caption[Plot of overlap for spectral clustering and modularity methods in the sparse regime.]{\label{fig:syntheticDataResultsPin0.6} A plot for the overlap of reconstructed node assignments of the Laplacian spectral clustering (blue) and greedy modularity optimisation methods (red) as the relative edge occurrence probabilities ($c_{out}/c_{in}$) are varied. 100 networks were generated with common parameters, $n=200$, $k=2$, $p_{in}=0.6$. Both algorithms perform similarly for $c_{out}/c_{in} < 0.5$ but the modularity method seems to perform better until we get the sharp drop in overlap values due to the phase transition. Using \cref{eq:ppmPhaseTransitionK=2} we find that, for this specification of the model, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8256$.}
\end{figure}

Recall the spectral clustering algorithm requires applying a k-means algorithm to the embedded vectors in a $k-1$ dimensional subspace.
For the case with dense graphs, the eigenvalue spectrum of the Laplacian is separated and the embedded vectors can be seen to be arbitrarily well separated also.
However, for the case where the graph is sparse, the eigenvector elements (i.e. coordinates in the subspace) are very similar, and the k-means algorithm performance will be very poor, results in much lower accuracy for the spectral clustering algorithm.
This observation is summarised as an illustration in \cref{fig:spectralClusteringEmbeddingVisualisationPlots}.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/embeddedVectorsModularityMethod_pin_0_8_pout_0_2.png}
		\caption{}
		\label{fig:spectralClusteringEmbeddingVisualisationPin08}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/embeddedVectorsModularityMethod_pin_0_08_pout_0_02.png}
		\caption{}
		\label{fig:spectralClusteringEmbeddingVisualisationPin008}
	\end{subfigure}
	\caption[Visualisation of spectral clustering embedding in sparse and dense regimes.]{\label{fig:spectralClusteringEmbeddingVisualisationPlots} A visualisation of the embedding for the spectral clustering algorithm in the dense, \subref{fig:spectralClusteringEmbeddingVisualisationPin08}, and sparse, \subref{fig:spectralClusteringEmbeddingVisualisationPin008}, regimes. The graph for the dense regime was generated using the planted partition model with $n=150$, $k=3$, $p_{in}=0.8$ and $p_{out}=0.2$, whilst the graph for the sparse regime was generated using the same values of $n$ and $k$ but with $p_{in}=0.08$ and $p_{out}=0.02$. We label the ground-truth node assignments by colour (i.e. points with the same colour represent nodes belonging to the same community in the graph). Notice that, for the dense regime, all data points with different colours are well separated so the k-means algorithm can accurately recover the cluster memberships. However, for the sparse regime, every data (red green and blue) are concentrated in the same region in the 2-dimensional embedding space, so the k-means algorithm cannot recover the cluster memberships accurately at all. This illustrates an issue with the Laplacian spectral clustering method.}
\end{figure}

For the dense regime, we generate a network from the planted partition model with $n=150$, $k=3$, $p_{in}=0.8$ and $p_{out}=0.2$, whilst for the sparse regime, we generate a network with the same values for $n$ and $k$, but $p_{in}=0.08$ and $p_{out}=0.02$.
Note the important regulation that, for this illustration, the ratio $c_{out}/c_{in}$ is identical for both regimes, and is below the theoretical phase transition (i.e. we are in the detectable region).
We see that, for the dense regime, all data points belonging to different communities are well separated, and points belonging to the same community are clustered together, so the k-means algorithm can accurately recover the cluster memberships.
However, for the sparse regime, all data points are clustered together, so the k-means algorithm cannot recover the cluster memberships accurately, and the overlap for the spectral clustering method will be very low indeed.

We now consider an issue associated with all modularity optimisation algorithms, known as the \emphT{resolution limit} \cite{GMC10,For10}.
Essentially, communities that are small when compared to the whole graph may not be distinguished even though they are well defined communities or even cliques, and therefore this problem has an impact on practical applications \cite{For10}.
As \cite{GMC10,For10} explain, if the change in modularity, denoted by $\Delta Q$, arising by merging two communities is positive, then the two groups will be clustered together.
Let $e_{i}$ be the number of edges within community $i$ and $e_{ij}$ be the number of edge between communities $i$ and $j$.
The expression for the change in modularity is then given by
\begin{equation}
	\label{eq:resolutionLimit}
	\Delta Q_{ij} = \frac{e_{ij}}{m} -2 \left( \frac{d_{i}}{2m} \right) \left( \frac{d_{j}}{2m} \right)
\end{equation}
where $d_{i}$ is the sum of the degrees of the vertices that belong to community $i$.
Notice that $\Delta Q_{ij} > 0$ (and therefore the two communities will be merged) if and only if $e_{ij} > \frac{d_{i}d_{j}}{2m}$ \cite{GMC10}.
This is problematic because of the null model considered; the modularity tends to expect a value of $e_{ij} < 1$, which makes just a single edge between the communities (i.e. $e_{ij}=1$) unexpected, so that two communities will be merged even though there is just one edge between them \cite{GMC10}.

Another problem with modularity optimisation is known as \emphT{extreme near-degeneracy} \cite{GMC10,For10}.
Good et al. \cite{GMC10} explain that the number of partitions with near-optimal modularity (i.e close with respect to the global maximum modularity value) grows exponentially with the number of nodes in the network \cite{For10}.
This results in extreme degeneracies in the modularity which causes problems in determining the partition which maximises modularity as well as finding high modularity partitions, since partitions that have similar high modularity values associated are not necessarily similar to one another \cite{For10,GMC10}.
This means that, the more modular a network is (i.e. the more communities it contains), it is actually becomes more difficult to determine the optimum partition among the suboptimal ones, which is a counter-intuitive result \cite{GMC10}.

To summarise, both spectral clustering and modularity optimisation algorithms can detect communities up to the phase transition in the dense regime.
However, we have seen how the performance declines, if the network is sparse, for both algorithms.
We have explained some of the issues observed that can account for the disappointing results, although there is no deep understanding regarding the modularity optimisation methods \cite{For10}.
Describing the behaviour of the modularity methods is important, though, and there have been many situations where the algorithm works well in practical applications \cite{New06a,New06b,For10,GMC10}.
This should not be startling given how the synthetic data we have used for our experimental tests are, in general, not well representative of real-world networks considered in practice.
Since the greedy algorithm is very fast and will be favourable for larger networks, we would recommend using this algorithm instead of other modularity optimisation methods as well as traditional spectral clustering as it achieves similar or better accuracy with faster run time.
For the case of small sized networks, the pragmatic option seems most suitable, where these different algorithms may be run and then compared in training cases, with the best one evaluated used for further investigation in test applications.

%----------------------------------------------------------------
%   Belief Propagation Algorithm Section
%----------------------------------------------------------------

\section{Belief Propagation Algorithm}
\label{sec:beliefPropagationAlgorithmTest}

%---   TODO
%%% in particular, need to precisely specify the phase transitions points predicted by the conjecture.
We wish to find a class of algorithms that, empirically, seem to reliably detect communities in networks generated by the planted partition model up to the phase transition in the sparse regime as well as the dense regime.
Both the spectral clustering and modularity optimisation methods could not achieve this, but we wish test whether the BP algorithm of Decelle et al. \cite{DKM+13} will succeed.
We shall analyse the accuracy of the partitions generated by the BP algorithm as we vary the following parameters: size of the graph, sparsity of the graph, number of communities and the edge occurrence probabilities.
All of the test networks generated from the block model will be used as input to the algorithm and the accuracy of reconstruction will, once more, be measured by computing the overlap (given by \cref{eq:overlap}).
The algorithm has been implemented in C++ by Decelle et al. \cite{DKM+13}, and we have obtained the code from \cite{ModeNet}.

We begin by considering the effect on the overlap as the edge occurrence probabilities are varied for small sized networks all consisting of two communities but with different sparsity.
We therefore generate three sets of 100 networks using the planted partition model with common parameters $n = 200$ and $k = 2$ but with a different value for the parameter for $p_{in}$ for each set.
The three different values are $p_{in} = \{0.1, 0.5, 0.9\}$ and they represent different levels of sparsity for the generated graph.
For each set of networks (and thus each value for $p_{in}$), we vary the parameter $p_{out}$ in steps for each of the 100 networks to obtain a different value for the ratio $c_{out}/c_{in}$, signifying the variation in the edge occurrence probabilities for different levels of sparsity on small-sized graphs.
The results are plotted in \cref{fig:BPAlgorithmResultsDifferentSparsity}.
We see the BP algorithm performs similarly well across the different regimes of sparsity.
This can be inferred from the points at which the overlap sharply declines from a value near 1 to a value close to 0 is approximately those predicted by the phase transition.
This indicates good performance on small-sized networks, especially compared to the spectral clustering and greedy modularity algorithms.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_1_q_2.png}
		\caption{}
		\label{fig:BPAlgorithmResultsDifferentSparsity_pin_0_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_5_q_2.png}
		\caption{}
		\label{fig:BPAlgorithmResultsDifferentSparsity_pin_0_5}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_9_q_2.png}
		\caption{}
		\label{fig:BPAlgorithmResultsDifferentSparsity_pin_0_9}
	\end{subfigure}
	\caption[Plot of overlap for belief propagation algorithm on small-sized networks with three different levels of sparsity.]{\label{fig:BPAlgorithmResultsDifferentSparsity} Plots for the overlap of reconstructed node assignments from the belief propagation algorithm as the ratio $c_{out}/c_{in}$ is varied for three different levels of sparsity. Three different values of $p_{in}$ are considered, each one representing a different level of sparsity, whilst 100 networks were generated with common parameters, $n = 200$ and $k = 2$, for each set. The results with $p_{in} = 0.1$ is shown in \subref{fig:BPAlgorithmResultsDifferentSparsity_pin_0_1}, the results with $p_{in} = 0.5$ is shown in \subref{fig:BPAlgorithmResultsDifferentSparsity_pin_0_5}, whilst the results with $p_{in} = 0.9$ is shown in \subref{fig:BPAlgorithmResultsDifferentSparsity_pin_0_9}. We see the BP algorithm performs similarly well up to the phase transition point across the three levels of sparsity, given this small-sized network test case.}
\end{figure}

In order to see if this effect is also true for small-sized networks but with more than two ground-truth communities, we generate two sets of 100 networks using the planted partition model with common parameters $n = 200$ and $k = 4$, again, with a different value for the parameter for $p_{in}$ for each set of networks.
Once more, we vary the parameter $p_{out}$ in steps for each of the 100 networks to obtain a different value for the ratio $c_{out}/c_{in}$, signifying the variation in the edge occurrence probabilities for different levels of sparsity on small-sized graphs (but this time with 4 ground truth communities rather than 2).
The plots are shown in \cref{fig:BPAlgorithmResults4Communities}.
These plots for overlap against $c_{out}/c_{in}$ looks very similar to those shown in \cref{fig:BPAlgorithmResultsDifferentSparsity_pin_0_5,fig:BPAlgorithmResultsDifferentSparsity_pin_0_9} .
This indicates that the performance of the BP algorithm does not vary in small-sized networks for differing number of communities.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_5_q_4.png}
		\caption{}
		\label{fig:BPAlgorithmResults4Communities_pin_0_5}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_9_q_4.png}
		\caption{}
		\label{fig:BPAlgorithmResults4Communities_pin_0_9}
	\end{subfigure}
	\caption[Plot of overlap for belief propagation algorithm on small-sized networks with three different levels of sparsity and four ground-truth communities.]{\label{fig:BPAlgorithmResults4Communities} Plots for the overlap of reconstructed node assignments from the belief propagation algorithm as the ratio $c_{out}/c_{in}$ is varied for two different levels of sparsity and four ground-truth communities. Two different values of $p_{in}$ are considered, each one representing a different level of sparsity, whilst 100 networks were generated with common parameters, $n = 200$ and $k = 4$, for each set. The results with $p_{in} = 0.5$ is shown in \subref{fig:BPAlgorithmResults4Communities_pin_0_5}, whilst the results with $p_{in} = 0.9$ is shown in \subref{fig:BPAlgorithmResults4Communities_pin_0_9}. We see the BP algorithm performs similarly well up to the phase transition point across the three levels of sparsity, given this small-sized network test case.}
\end{figure}

We also noted, through the simplistic derivation in \cref{sec:beliefPropagationAlgorithm}, that the algorithm should also work well for large sparse graphs since each step of the algorithm takes $\bigO{n}$ time.
To test this result empirically, we generate two sets of sparse and larger networks, differing only in the number of ground-truth communities.
We generate two sets of 100 networks using the planted partition model with common parameters $n = 10000$ and $p_{in} = 0.01$ but with two different values of $k$.
The plots are shown in \cref{fig:BPAlgorithmResultsLargeNetwork}.
Using \cref{eq:ppmPhaseTransitionK} we find that, for the specification of the model with $k=2$, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8098$, whilst, for the specification of the model with $k=4$, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.6555$.
The results do indicate the BP algorithm accurately detects communities for different values of $k$ up to their respective predicted phase transition points for large and sparse networks.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_01_q_2_n_10000.png}
		\caption{}
		\label{fig:BPAlgorithmResultsLargeNetwork_k_2}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/beliefPropagationSyntheticDataErrors_pin_0_01_q_4_n_10000.png}
		\caption{}
		\label{fig:BPAlgorithmResultsLargeNetwork_k_4}
	\end{subfigure}
	\caption[Plot of overlap for belief propagation algorithm on large network for two and four ground-truth communities.]{\label{fig:BPAlgorithmResultsLargeNetwork} Plots for the overlap of reconstructed node assignments from the belief propagation algorithm as the ratio $c_{out}/c_{in}$ is varied for both two and four ground-truth communities in an example of a large, sparse graph. Two different values of $k$ are considered, whilst 100 networks were generated with common parameters, $n = 10000$ and $p_{in} = 0.01$, for each set. The results with $k = 2$ is shown in \subref{fig:BPAlgorithmResultsLargeNetwork_k_2}, whilst the results with $k = 4$ is shown in \subref{fig:BPAlgorithmResultsLargeNetwork_k_4}. Using \cref{eq:ppmPhaseTransitionK} we find that, for the specification of the model with $k=2$, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.8098$, whilst, for the specification of the model with $k=4$, the predicted phase transition point occurs at $c_{out}/c_{in} \approx 0.6555$. Hence, we observe the BP algorithm performs well up to the phase transition points for both values of $k$ in the large, sparse regime.}
\end{figure}

Combining the implications of our empirical testing for the belief propagation algorithm and comparing it with the synthetic data results for the spectral clustering and greedy modularity methods in \cref{sec:spectralClusteringModularityOptimisationAlgorithms}, we do find the BP algorithm has several advantages for the specific test case of the planted partition model.
Firstly, we can detect communities reliably in the sparse regime up to the corresponding phase transition point in addition to the dense regime.
Also, the algorithm is more scalable in the sparse regime due to faster run time.
%---   TODO
%%% find reference of Leskovec for poor representation of block models of real-world networks
However, these remarks should not be a surprise given that the BP algorithm was specifically designed to detect communities from graphs generated by the planted partition model and, therefore, we could have concerns about the generalisation achieved, since statistical block models do not generate graphs with similar properties to those found in real world networks (i.e. they are not well representative) \cite{DKM+13}.

The BP algorithm has been applied to real-world networks in \cite{DKM+13}, but they considered only small-sized networks that are not very practical, such as the popular Zachary's karate club network as well as a network of political books.
%---   TODO
%%% find reference for modularity methods performing well on real-world networks
Whereas, modularity methods have been tested on a wide array of data sets and networks in the literature, and have performed well.
Therefore we conclude that, for applications involving moderately-sized real-world networks, modularity optimisation methods should be a first point of call.
We also suggest possible tweaks could be made to certain modularity methods that tailor them to the specific application at hand (similarly to how the BP algorithm we considered has been tailored to the planted partition model), which could also improve performance.
An example application of this is a financial network, as we shall discuss in \cref{cha:communityDetectionFinancialNetworks,cha:temporalEvolutionFinancialNetworks}.

%-----------------------------------------------------
%   NLPI and AMP Algorithms Section
%-----------------------------------------------------

\section{NLPI and AMP Algorithms}
\label{sec:NLPIAndAMPAlgorithms}

We shall use the procedure outlined by Montanari in \cite{Mon13} to test the NLPI and AMP algorithms, where the synthetic data used is simply an instance of a normalised adjacency matrix. Recall \cref{def:hcmNormlaisedAdjacencyMatrixDecomposed}, where we decomposed the matrix into a signal term (weighted by a signal-to-noise ratio term) plus a noise term.
We will consider the case of a network generated by the hidden clique model with $n$ nodes and one hidden community consisting of $k$ nodes.
We may choose the community memberships arbitrarily. Using previous notation, let us denote $\vecvar{u}$ as the indicator variable for nodes belonging to the hidden community (i.e. so that precisely $k$ elements of $\vecvar{u}$ are equal to one and the rest equal zero), $\lambda$ as the SNR and $\matvar{Z}$ as the noise term.
We set the elements of $\matvar{Z}$ to be $Z_{ij} \sim \normal{0,1/n}$ i.i.d. entries.
Therefore, given a value of $\lambda$ and our ground-truth community assignments, we can construct the normalised adjacency matrix, denoted by $\widetilde{\matvar{A}}$, using \cref{def:hcmNormlaisedAdjacencyMatrixDecomposed}.
This matrix will serve as input to the NLPI and AMP algorithms, which produce an output representing their reconstructed node assignments, which we shall denote by $\widehat{\vecvar{u}}$.
We represent the accuracy of the algorithms by the inner product of the ground-truth and reconstructed node assignments, $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$.
Notice this value lies between 0 and 1, where a larger value indicates improved reconstruction and better accuracy.

Now, with the framework set up, we can construct our own tests, seeking the accuracy as both the SNR and the size of the hidden community are varied.
Define a new variable $\varepsilon \equiv k/n$ representing the proportion of all nodes that belong to the hidden community, then we can construct a grid of points representing different values of $\lambda$ and $\varepsilon$.
We, finally, plot the values of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for each point on the grid to analyse the behaviour of the algorithms.

Utilising this approach with $n=500$ nodes, varying $\lambda$ between 0 and 1.2, choosing a grid resolution of 100 and running 50 iterations of the NLPI algorithm, we show in \cref{fig:NLPIAccuracyPlot}, the accuracy of the NLPI algorithm plotted against $\lambda$ and $\varepsilon$.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/NLPIMontanariSyntheticDataScalarProducts.png}
	\caption[Plot of accuracy for NLPI algorithm.]{\label{fig:NLPIAccuracyPlot} A plot of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for the NLPI algorithm for different values of $\lambda$ and $\varepsilon$. The synthetic data is generated from a network with $n=500$ nodes and a grid with resolution 100, whilst the algorithm is run for 50 iterations.}
\end{figure}

The motivation for the NLPI algorithm is to detect the node assignments more accurately than standard PCA for small-sized hidden communities. The results can be analysed by focusing on the accuracy for small values of $\varepsilon$ and all ranges of $\lambda$.
We have previously discussed that the transition for standard PCA algorithms for small-size hidden communities is $\lambda = 1$, where, for values above this threshold, reconstruction is possible, and below, it is impossible.
\Cref{fig:NLPIAccuracyPlot} illustrates that for small values of $\varepsilon$ (e.g. between 0.05 and 0.2), the reconstructed vector is correlated with ground-truth (i.e. $\innerP{\vecvar{u},\widehat{\vecvar{u}}} > 0$).
In particular, we can get good reconstruction for values of $\lambda$ all the way down to 0.8. Thus we have shown to beat the spectral threshold!

Although the NLPI algorithm can be shown to beat the spectral threshold empirically by using the synthetic data strategy of \cite{Mon13}  that we used above, analysing its asymptotic behaviour mathematically is not trivial, as we have already noted.
This served as the principal motivation for the AMP algorithm where precise asymptotics could be derived theoretically (however this is beyond the scope of this report).
We shall now analyse the empirical performance of the AMP algorithm using identical synthetic data (i.e. the same normalised adjacency matrix input for every value pair $(\lambda,\varepsilon)$ in the grid) as we tested the NLPI algorithm with. We also chose to run the AMP algorithms for 50 iterations.
We have plotted the accuracy for the AMP algorithm in \cref{fig:AMPAccuracyPlot}.

%---   TODO
%%% need to add picture of (correct) AMP algorithm accuracy 

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/AMPMontanariSyntheticDataScalarProducts.png}
	\caption[Plot of accuracy for AMP algorithm.]{\label{fig:AMPAccuracyPlot} A plot of $\innerP{\vecvar{u},\widehat{\vecvar{u}}}$ for the AMP algorithm for different values of $\lambda$ and $\varepsilon$. The synthetic data is generated from the same network with $n=500$ nodes as used in \cref{fig:NLPIAccuracyPlot}, and a grid with resolution 100, whilst the algorithm is run for 50 iterations.}
\end{figure}

\Cref{fig:AMPAccuracyPlot} illustrates that for small values of $\varepsilon$ (e.g. between 0.05 and 0.2), the reconstructed vector is correlated with ground-truth.
In particular, we get good reconstruction for values of $\lambda$ all the way down to 0.8. Thus we have shown to beat the spectral threshold once more!
We also note that the striking similarity between \cref{fig:NLPIAccuracyPlot} and \cref{fig:AMPAccuracyPlot} is to be expected given the formulations of these algorithms.

We have seen how NLPI and AMP algorithms can be used to detect hidden communities in networks generated by a hidden clique model.
In particular we have seen the empirical improvements over standard spectral methods such as PCA.
Moreover, the NLPI approach of applying a suitable non-linear function as an extra step to traditional power-iteration algorithm may be used for any problem where an eigenvector with special properties (e.g. sparsity or non-negativity) needs to be found.
Although the application is beyond the scope of this report, the AMP algorithm has had much success when applied to other problems such as \emphT{compressed sensing}; we refer the reader to \cite{DMM09,MDM10,BM11,Mon11,BKS13} for more details. 