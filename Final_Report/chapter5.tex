% Chapter 5

\chapter{Community Detection in Financial Networks}

\label{cha:communityDetectionFinancialNetworks}

%----------------------------------------------------------------------------------------

In this chapter we aim to motivate and study modified modularity algorithms to detect communities within real-world financial networks and compare their performance with baseline methods.
Firstly, we explain the process of gathering the empirical data to create the financial network.
Then we investigate and apply algorithms on both synthetically generated and the empirical data.
We focus on modularity optimisation and study two naive modularity maximisation methods studied in previous chapters as well as two modified modularity optimisation techniques that are tailored to detect communities in our specific setting of financial networks based on empirical correlation matrices.
By determining the `best' method for the static financial network, we make an important step in identifying communities in time-evolving (dynamic) financial networks.

%-----------------------------------------------------
%   Financial Data Processing Section
%-----------------------------------------------------

\section{Constructing the Real-world Financial Network}
\label{sec:realWorldFinancialNetwork}

The dataset we use consists of daily closing prices of 80 stocks in the FTSE 100 index, which we obtained from \cite{LSE,YahFi}.
The time period considered is between the beginning of 2004 to the end of 2013, a total of 2501 prices. 
This is the data we obtained after the removal of a few data points due to incomplete data across different stocks.
The complete list of stocks is given in \cref{app:listFTSE100Stocks}.
We then calculated the logarithmic return for each stock and for each time period.
We generated a time series of these returns and associated each stock with a single time series.
By using the method described in \cref{subsec:financialNetworksConstructionBackground}, we proceeded to construct a financial network represented by a fully-connected, undirected and weighted graph.
There are 80 nodes in this network (each one representing a stock), and the weights on the edges connecting any two nodes is the cross-correlation between the time series of returns associated with the stocks represented by the two nodes.
We stress, at this point, the data of the whole period (01/01/2004 - 11/11/2013) is currently represented by one single network.
\Cref{fig:expectedReturnsAndVolatility} shows a plot of the expected return against the volatility for each stock considered during this period.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figures/expectedReturnsAndVolatility.png}
	\caption[Plot of expected return against volatility for 80 FTSE 100 stocks.]{\label{fig:expectedReturnsAndVolatility} A plot of expected return against volatility of the 80 stocks in the FTSE 100 index considered. Data points obtained from price data during the period 01/01/2004 - 11/11/2013.}
\end{figure}

%-------------------------------------------
%   Random Matrix Theory Sub Section
%-------------------------------------------

\subsection{Random Matrix Theory}
\label{subsec:randomMatrixTheory}

The correlation matrix is representing the weighted adjacency matrix of the network, and in order to better understand the weights in the network, we wish to refer to an important result from Random Matrix Theory (RMT) that has been outlined in \cite{SM99,PGR+99,PBL05,MG13}.
In particular, we wish to distinguish between random and non-random properties of empirical correlation matrices.

A correlation matrix created from $n$ random time series of length $T$, in the limits $n \rightarrow +\infty$ and $T \rightarrow +\infty$ with $1 < T/n < +\infty$, has a specific distribution of eigenvalues known as the \emphT{Sengupta-Mitra distribution} \cite{SM99,PBL05,FPW+11,MG13}. This distribution is defined by 
\begin{equation}
	\label{def:senguptaMitraDistribution}
	\rho(\lambda) =	
	\begin{cases}
		\frac{T}{n}\frac{\sqrt{(\lambda_{+} - \lambda)(\lambda - \lambda_{-})}}{2\pi\lambda}& \text{if } \lambda_{-} \leq \lambda \leq \lambda_{+} \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
where the maximum and minimum eigenvalues ($\lambda_{+}$ and $\lambda_{-}$ respectively) are given by
\begin{equation}
\label{eq:maxEigenvalueSM}
	\lambda_{+} = \left(1+\sqrt{\frac{n}{T}}\right)^{2}
\end{equation}
and
\begin{equation}
\label{eq:minEigenvalueSM}
	\lambda_{-} = \left(1-\sqrt{\frac{n}{T}}\right)^{2}
\end{equation}
Therefore the set of eigenvalues of an empirical correlation matrix that lies within this distribution is considered to occur purely as a result of random noise \cite{PBL05,FPW+11,MG13}. Moreover, we may regard any eigenvalue larger than $\lambda_{+}$ to represent important structure within the data \cite{PBL05,FPW+11,MG13}.

Analysing the deviation of the eigenvalue spectrum of empirical correlation matrices constructed from real-world financial data from the RMT distribution constitutes an effective method to filter noise out from the data.
For example, we constructed the correlation matrix from the FTSE 100 data set (described in \cref{sec:realWorldFinancialNetwork}) and plotted the eigenvalue spectrum for this matrix alongside the corresponding Sengupta-Mistra distribution (i.e. the RMT prediction with $n = 80$ and $T = 2501$) in \cref{fig:eigenvalueSpectra}.
We observe two interesting regions of the eigenvalue spectrum outside the RMT prediction.
Firstly, the largest eigenvalue of the correlation matrix, which we shall denote by $\lambda_{m}$, is much larger than all other eigenvalues.
Also, the eigenvector associated with the largest eigenvalue, denoted by $v_{m}$, has all elements positive.
This has been observed in many previous studies of empirical correlation matrices, and this eigenvalue is also called the \emphT{market mode}, meaning this component acts as a common factor influencing all assets in the market \cite{FPW+11,MG13}.
Secondly, we observe a few eigenvalues just outside the RMT predicted region (i.e. eigenvalues just larger than $\lambda_{+}$ and much smaller than $\lambda_{m}$).
We believe these components reflect a mesoscopic level of groups of stocks within the market (i.e. neither at the level of individual stocks in the form of noise, nor at the level of the entire market in the form of the market mode eigenvalue) \cite{MG13}, hence we expect members of these groups of stocks to demonstrate similar underlying properties, such as related sector classifications.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/eigenvalueSpectra_n_80_T_2501.png}
		\caption{}
		\label{fig:eigenvalueSpectrumOriginal}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/eigenvalueSpectraZoomed_n_80_T_2501.png}
		\caption{}
		\label{fig:eigenvalueSpectrumZoomed}
	\end{subfigure}
	\caption[Plots of empirical and RMT predicted eigenvalue spectrum]{\label{fig:eigenvalueSpectra} Plots of eigenvalue spectra for empirical correlation matrix and RMT prediction, \subref{fig:eigenvalueSpectrumOriginal}, in addition to a zoomed-in version, \subref{fig:eigenvalueSpectrumZoomed}. The empirical correlation matrix was constructed from the daily log-returns of the FTSE 100 data set, and its eigenvalue spectrum is plotted in red. The RMT prediction is the Sengupta-Mitra distribution with appropriate parameters ($n = 80$, $T = 2501$), and is plotted in blue. The zoomed-in graph identifies the existence of eigenvalues outside of the region predicted by RMT, whilst the zoomed-out graph clearly shows the maximum eigenvalue (i.e the market mode eigenvalue) with a value of about 28.}
\end{figure}

We proceed to utilise the eigenvalue spectrum observed for the data set and the RMT prediction to filter out the empirical correlation matrix to reflect a mesoscopic structure, as achieved by \cite{MG13}. Recall the correlation matrix for our FTSE 100 data set is a $80 \times 80$ matrix denoted by $\matvar{C}$ and that we denote $\lambda_{i}$ as the $i-th$ eigenvalue of $\matvar{C}$ and $\vecvar{v_{i}}$ represents the eigenvector associated with $\lambda_{i}$. We are able to decompose this matrix as the sum of three matrices
\begin{equation}
\label{eq:decompositionCorrelationMatrix}
	\matvar{C} = \matvar{C}^{(r)} + \matvar{C}^{(g)} + \matvar{C}^{(m)}
\end{equation}
where $\matvar{C}^{(r)}$ represents the correlation matrix corresponding to the random components, defined by
\begin{equation}
\label{eq:randomCorrelationMatrix}
	\matvar{C}^{(r)} \equiv \sum_{i:\lambda_{i}\leq\lambda_{+}} \lambda_{i} \vecvar{v_{i}} \transpose{\vecvar{v_{i}}}
\end{equation}
$\matvar{C}^{(m)}$ represents the correlation matrix corresponding to the market mode component, defined by
\begin{equation}
\label{eq:marketModeCorrelationMatrix}
	\matvar{C}^{(m)} \equiv \lambda_{m} \vecvar{v_{m}} \transpose{\vecvar{v_{m}}}
\end{equation}
and $\matvar{C}^{(g)}$ represents the remaining correlations
\begin{equation}
\label{eq:remainingCorrelationMatrix}
	\matvar{C}^{(g)} \equiv \sum_{i:\lambda_{+} < \lambda_{i} < \lambda_{m}} \lambda_{i} \vecvar{v_{i}} \transpose{\vecvar{v_{i}}}
\end{equation}

Therefore, we now have a representation of a filtered empirical correlation matrix, $\matvar{C}^{(g)}$, which represents the mesoscopic (group level) correlations of the stocks, which we shall use, crucially, as the input to several community detection algorithms.
Simply, it can be thought of as a weighted adjacency matrix of a new filtered network.

%-----------------------------------------------------
%   Community Detection Algorithms Section
%-----------------------------------------------------

\section{Community Detection Algorithms}
\label{sec:communityDetectionAlgorithms}

So far we have been able to construct a financial network based on the correlations of daily logarithmic returns of stocks and, using RMT, a filtered correlation matrix that represents a new financial network with links (hopefully) representing group-level correlation.
Although, the question still remains, given either the initial or filtered correlation matrix, how does one produce a set of groups of stocks with greater correlations within a group than between groups?
From previous sections, we understand the notion of community detection within graphs, which we shall also refer to as \emphT{binary networks}, and have analysed several algorithms that tackle this problem.
However, in these problems we analysed adjacency matrices that contained binary elements (i.e. a `1' if an edge exists in the graph and a `0' otherwise), whereas, in this problem, we study a weighted adjacency matrix with elements as real numbers.
The reader should also note the adjacency matrix studied in previous sections is directly related to the structure of the network in question, whereas in this case, it is related to the weights of links between nodes.
This suggests the possibility of having to modify previously studied algorithms to tailor their behaviour for this scenario.

Given our comments in previous chapters, modularity optimisation has worked well in other practical applications and does not require prior knowledge of the number of or sizes of the communities to be detected \cite{HGH+12,MG13}.
Hence, we shall use modularity optimisation methods as a basis for community detection in financial networks.

%-------------------------------------------
%   Naive Modularity Methods Sub Section
%-------------------------------------------

\subsection{Modularity Optimisation Methods}
\label{subsec:modularityOptimisationMethods}

Recall in \cref{sec:modularityBasedOptimisation}, we introduced the notion of modularity optimisation as a method for community detection within networks.
We shall once more consider algorithms for modularity optimisation, but for the case of financial networks.

Let us denote a partition of $n$ nodes, in the financial network, into communities/groups by the vector $\vecvar{\sigma} = \transpose{[\sigma_{1},\dots,\sigma_{n}]}$, where $\sigma_{i}$ denotes the group to which node $i$ belongs to.
This is essentially the group to which stock $i$ belongs.
We then define the modularity for this partition, $Q(\vecvar{\sigma})$, by
\begin{equation}
\label{eq:modularityFinancialNetworks}
	Q(\vecvar{\sigma}) = \frac{1}{2m} \sum_{ij} \left(A_{ij} - \frac{k_{i}k_{j}}{2m} \right) \delta(\sigma_{i},\sigma_{j})
\end{equation}
where $\matvar{A}$ is the weighted adjacency matrix of the network, $k_{i} \equiv \sum_{j} A_{ij}$ and $2m \equiv \sum_{ij} A_{ij}$.
Note that in the case of binary networks, $A_{ij}$ represented the presence or absence of an edge between nodes $i$ and $j$, $d_{i}$ represented the degree of node $i$ and $m$ represented the total number of edges in the networks.
Since we are interested in financial networks, a naive approach would be to use the empirical correlation matrix, denoted by $\matvar{C}$, as the network's weighted adjacency matrix.
Note that we are essentially ignoring the results of \cref{subsec:randomMatrixTheory} for this naive approach.

For now, though, let us use the following relationship
\begin{equation}
\label{eq:weightedAdjacencyMatrixFinancialNetworks}
	A_{ij} = \frac{1}{2} \big( C_{ij}+1 \big) - \delta(i,j)
\end{equation}
where $C_{ij}$ denotes the elements of the correlation matrix, and the $\delta(i,j)$ term removes self edges.
From this definition, we simply note that $A_{ij} \in [0,1]$.

We now focus on finding the partition, $\vecvar{\sigma}$, that maximises the modularity.
We notice that a larger value of $A_{ij}$ implies larger correlation between the stocks $i$ and $j$, whilst a smaller value implies a lower correlation.
Recalling how, for the case with binary networks, we sought after denser connections within groups and sparser connections between groups, we realise the modularity maximisation algorithms used on binary networks should have the same effect on financial networks.
This is intuitive since, in both types of networks, we aim to find the partition which maximises the sum of correlations or number of edges between nodes within same community and minimises the sum of correlations or number of edges between nodes belonging to different communities.

We shall select two familiar approaches, widely known in the literature, as algorithms for the modularity maximisation.
We consider a greedy agglomerative method discussed earlier in the report in addition to a method that uses spectral relaxation.

Firstly, we use the greedy method of Clauset et al. \cite{CNM04} described in \cref{subsec:greedyAlgorithm}, which we noted is also applicable in this case of weighted networks, so we use \cref{eq:weightedAdjacencyMatrixFinancialNetworks} as input to the algorithm.
The algorithm we use is implemented in MATLAB and the code has been written by Le Martelot \cite{ELM}.

The other approach using the spectral relaxation can be summarised using the argument from \cite{DM}.
Let $\matvar{B}$ denote the modularity matrix, whose elements are defined by
\begin{equation}
\label{eq:modularityMatrixFinancialNetworks}
	B_{ij} = A_{ij} - \frac{d_{i}d_{j}}{2m}
\end{equation}
where $A_{ij}$, $d_{i}$ and $m$ are defined as before.
Also denote the set of nodes belonging to group $a$ by $\setvar{S}_{a} \equiv \{i:\sigma_{i} = a\}$.
The algorithm iterates by attempting to split the node members of a single group in an optimal fashion by using modularity.
Assume at one iteration there are $q$ groups so the partition is indexed by $[q] \equiv \{1,2,\dots,q\}$.
For some $a \in [q]$, let $\matvar{B}_{a}$ denote the submatrix restricted to nodes in $\setvar{S}_{a}$.
Let $\vecvar{v} \in \realsR^{\cardinality{\setvar{S}_{a}}}$ denote the sign vector given the algorithm operating on $\matvar{B}_{a}$. Then the change in modularity is given by
\begin{equation}
\label{eq:changeModularityFinancialNetworks}
	\begin{split}
		&\Delta Q = \frac{1}{2m} \left( \sum_{i,j \in \setvar{S}_{a}} B_{ij}\frac{(1+v_{i}v_{j})}{2} - \sum_{i,j \in \setvar{S}_{a}} B_{ij} \right) \\
		&= \frac{1}{4m} \left( \sum_{i,j \in \setvar{S}_{a}} B_{ij}v_{i}v_{j} - \sum_{i,j \in \setvar{S}_{a}} B_{ij} \right)  \\
		&= \frac{1}{4m} \transpose{\vecvar{v}} \widetilde{\matvar{B}}_{a} \vecvar{v}
	\end{split}
\end{equation}
where $\widetilde{\matvar{B}}_{a} = \matvar{B}_{a} - diag \left( \sum_{j \in \setvar{S}_{a}} B_{ij} \right)$.

The algorithm accepts the splitting of the group which maximises the modularity difference, and terminates when reaches a threshold regarding the size of the groups and the possible improvement in modularity at a given iteration.
We use the MATLAB implementation of this algorithm by Deshpande et al. \cite{DM}, stressing the input is defined by \cref{eq:weightedAdjacencyMatrixFinancialNetworks}.

%-------------------------------------------
%   Modified Modularity Method Sub Section
%-------------------------------------------

\subsection{Modified Modularity Optimisation Methods}
\label{subsec:modifiedModularityOptimisationMethods}

We move on to understand the potential issues with the naive approach hinted in the previous section. Intuitively, the term $d_{i}d_{j}/2m$ reflects the null hypothesis that the observed network structure is wholly based on the degrees of the nodes.
This idea is sound when applied to binary networks, however, these terms do not have a specific meaning when applied to financial networks since we are interested in the correlation matrix rather than the structure of the underlying graph.
The quantities $d_{i} \equiv \sum_{j} C_{ij}$ and $2m \equiv \sum_{ij} C_{ij}$ do not make up a clear notion of a null model when combined.
Furthermore, MacMahon and Garlaschelli \cite{MG13} have shown the naive approach is mathematically incorrect and leads to biased results.
In order to detect communities within the correlation matrix that leads to correct results, we seek improvement in the construction of the null model for the network.
We have already discussed, in \cref{subsec:randomMatrixTheory}, how to construct a filtered correlation matrix that takes into account the random noise at the level of individual stocks and a market-wide component.
Therefore, this filtered matrix, which we have denoted by $\matvar{C}^{(g)}$ and defined in \cref{eq:remainingCorrelationMatrix}, can be used as a modularity matrix, since it reflects a suitable null model (i.e. random noise plus market wide correlation) subtracted from the observed correlation matrix.

Our aim, here, is to use $\matvar{C}^{(g)}$ as input to an algorithm that can detect communities by maximising modularity.
We hope this provides better results than both of the naive modularity methods discussed previously.

We must first, though, introduce a new formulation of modularity given a partition, as used by \cite{MG13}, which we denote by $Q_{n}(\vecvar{\sigma})$ and call modified modularity,
\begin{equation}
\label{eq:newModularityFinancialNetworks}
	Q_{n}(\vecvar{\sigma}) = \frac{1}{C_{norm}} \sum_{ij} C^{(g)}_{ij} \delta \left( \sigma_{i},\sigma_{j} \right).
\end{equation}
where $C_{norm}$ is a normalisation term defined by 
\begin{equation}
\label{eq:newModularityNormalisationConstantFinancialNetworks}
	C_{norm} \equiv \sum_{ij} \abs{C^{(g)}_{ij}}
\end{equation}
which just ensures the value of the newly-defined modularity lies within the interval $[-1,+1]$.
The new formulation of modularity is specifically aimed to detect mesoscopic-level communities.

Our first modified modularity method is a spectral clustering algorithm based on the exact same technique considered for binary networks considered in \cref{sec:spectralClustering}, but instead we shall use the filtered correlation matrix, $\matvar{C}^{(g)}$, as input.
We simply find the eigenvectors of $\matvar{C}^{(g)}$, use them to construct the embedded vectors, which are then clustered using the familiar k-means clustering algorithm (same procedure as with binary networks).
We use different values for the number of groups (within an appropriate range), then run the algorithm for each one and choose the partition with the best value for the re-defined modularity defined in \cref{eq:newModularityFinancialNetworks}.

The second modified modularity method is a greedy algorithm based on the Louvain method proposed by Blondel et al. \cite{BGL+08}, that we discussed in \cref{subsec:greedyAlgorithm}.
However, we shall instead use the filtered correlation matrix, $\matvar{C}^{(g)}$, as the input to the algorithm.
We use the MATLAB implementation of this algorithm where the code has been written by Mucha and Porter \cite{GenLou}.
The intention of utilising this method over the spectral clustering approach just mentioned is twofold.
Firstly, we do not need to run the algorithm for various values for the number of groups, since the Louvain method generates the node assignments without requiring the number of communities in advance.
Secondly, this algorithm has a faster run time, and this will be important considering performance on time-evolving financial networks, which we shall discuss in \cref{cha:temporalEvolutionFinancialNetworks}.

To summarise, we will study four algorithms based on modularity optimisation; two are considered to be naive approach using a weighted adjacency matrix as input with the same null model as with conventional community detection block models, whereas the other two methods are tailored to detect communities in our application of financial networks by optimising the modified definition of modularity using the results from RMT.

%-----------------------------------------------------
%   Synthetic Data Testing Section
%-----------------------------------------------------

\section{Synthetic Data Testing}
\label{sec:syntheticDataTesting}

Before applying the four algorithms discussed previously to our financial network based on the FTSE 100 data set, we must run tests to confirm we can correctly detect correlated sets of time series in synthetically generated benchmark cases, as also outlined by \cite{MG13}.
We consider a benchmark data set of correlation matrices with 100 time series (i.e. we consider $n = 100$ stocks in the data set) divided into 10 communities of 10 correlated time series.
Note the length of the time series is chosen to be $T=2500$ to reflect similar conditions to the real data set and is also prescribed by RMT (i.e. $T>n$).
The set consists of correlation matrices generated with different levels of correlations between the groups and within a group reflecting different signal-to-noise ratios (SNR), where a lower value for SNR results in more difficulty in identifying the ground-truth partition.
We set the ground-truth partition, which we denote by $\vecvar{\sigma}^{*}$, to be the same across all correlation matrices.
We considered six such benchmarks within the correlation matrix set with varying SNR, and have illustrated two such examples in \cref{fig:benchmarkCorrelationMatrices}.
We confirmed that, in all the benchmarks, all four methods succeeded in identifying $\vecvar{\sigma}^{*}$ to a sufficiently high accuracy.

%---   FIGURE
\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/syntheticCorrelationMatrices_1.png}
		\caption{}
		\label{fig:benchmarkCorrelationMatrix1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/syntheticCorrelationMatrices_3.png}
		\caption{}
		\label{fig:benchmarkCorrelationMatrix3}
	\end{subfigure}
	\caption[Plots of synthetically generated benchmark correlation matrices]{\label{fig:benchmarkCorrelationMatrices} Plots of synthetically generated benchmark correlation matrices, one with low SNR (a value of 0.5), \subref{fig:benchmarkCorrelationMatrix1}, in addition to one with higher SNR (a value of 1), \subref{fig:benchmarkCorrelationMatrix3}. These are two examples from a set created from 100 time series divided into 10 communities of 10 correlation time series. The blocks along the diagonal represent cross-correlations between members of the same group, and are thus high (i.e. close to one), whereas off-diagonal blocks represent cross-correlations between time series belonging to different communities reflecting noise (system-wide or additional inter-community correlations).}
\end{figure}

%-----------------------------------------------------
%   Application to Real-world Financial Network Section
%-----------------------------------------------------

\section{Application to Real-world Financial Network}
\label{sec:applicationToRealWorldFinancialNetwork}

We now apply the modularity optimisation methods discussed in \cref{sec:communityDetectionAlgorithms} to the empirical data we collected.
We have two clear intentions.
Firstly, we wish to understand the community structure of the stocks, in particular with regard to the composition of each community based upon the industry sectors of the stocks.
Secondly, we wish to build a comparative analysis of the different algorithms based on empirical data.

We begin by looking at the communities generated by the four modularity methods discussed in \cref{sec:communityDetectionAlgorithms}, using all of the data.
\Cref{fig:outputFTSE100} displays the communities identified by all the algorithms by grouping the tickers of the stocks based upon label colouring.
We see from \cref{fig:outputCommunitiesGreedy,fig:outputCommunitiesSpectralRelaxation} that both traditional modularity methods, the greedy algorithm and the spectral relaxation method respectively, have identified two distinct communities; whilst from \cref{fig:outputCommunitiesSpectralClustering,fig:outputCommunitiesTestLouvainMethod}, we notice both the modified modularity based techniques have identified four communities.
This is the outcome we expected given the assumed null model discounted both random and market-wide correlations \cite{MG13}, meaning the spectral clustering algorithm was successful in finding previously undetected communities.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100FastNewmanMethod.png}
		\caption{}
		\label{fig:outputCommunitiesGreedy}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100MontanariMethod.png}
		\caption{}
		\label{fig:outputCommunitiesSpectralRelaxation}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100FinancialSpectralClusteringMethod.png}
		\caption{}
		\label{fig:outputCommunitiesSpectralClustering}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100TestLouvainMethod.png}
		\caption{}
		\label{fig:outputCommunitiesTestLouvainMethod}
	\end{subfigure}
	\caption[Communities of the FTSE 100 data generated using four different algorithms.]{\label{fig:outputFTSE100} Communities of the FTSE 100 data generated using four different algorithms. The name of each label represents the associated stock's ticker (see \cref{app:listFTSE100Stocks}) whilst the colour of the label represents community memberships (i.e. stocks associated with labels that have different colours belong to different communities and stocks whose labels have the same colour belong to the same community). The resulting communities after the greedy algorithm was applied are shown in \subref{fig:outputCommunitiesGreedy}, where there were two communities generated. The resulting communities after the algorithm based on a spectral relaxation was applied are shown in \subref{fig:outputCommunitiesSpectralRelaxation}, where there were also two communities generated. The resulting communities after the spectral clustering algorithm based on the modified modularity matrix was applied is shown are \subref{fig:outputCommunitiesSpectralClustering}, where there were four communities generated and, finally, resulting communities after the modified modularity Louvain method was applied are shown in \subref{fig:outputCommunitiesTestLouvainMethod}, where there were also four communities.}
\end{figure}

A first look at the tickers of stocks belonging to each community does not help to uncover any particular patterns.
Therefore we analyse the relative composition of each community identified based on the industry sector of each stock as shown in \cref{fig:outputFTSE100PieCharts}.
The industry sectors for each stock we consider is given in \cref{app:listFTSE100Stocks}.
Each community is represented by a single pie chart (labelled by letter), where the colour legend defined in \cref{tab:outputFTSE100PieChartsColourLegend} indicates the industry sector association.
We note that the actual sizes of the pie charts do not provide any information, only the fractions of the different coloured regions are important.

%---   TABLE
\begin{table}
	\caption{Colour representation for 10 industry sectors used to classify FTSE 100 stocks, to be used as a legend.}
	\label{tab:outputFTSE100PieChartsColourLegend}
	\centering
	\includegraphics[width=.6\linewidth]{figures/outputFTSE100PieChartsColourLegend.png}
\end{table}

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100IndustriesPieChartsFastNewmanMethod.png}
		\caption{}
		\label{fig:outputPieChartsCommunitiesGreedy}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100IndustriesPieChartsMontanariMethod.png}
		\caption{}
		\label{fig:outputPieChartsCommunitiesSpectralRelaxation}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100IndustriesPieChartsFinancialSpectralClusteringMethod.png}
		\caption{}
		\label{fig:outputPieChartsCommunitiesSpectralClustering}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/outputFTSE100IndustriesPieChartsTestLouvainMethod.png}
		\caption{}
		\label{fig:outputPieChartsCommunitiesTestLouvainMethod}
	\end{subfigure}
	\caption[Pie charts showing the relative composition of each generated community based on industry sectors of the FTSE 100 stocks for four different algorithms.]{\label{fig:outputFTSE100PieCharts} Pie charts showing the relative composition of each generate community based on the industry sectors of the stocks (see \cref{app:listFTSE100Stocks}). Individual communities are labelled by letter, where each community is represented by one pie chart, and the colour legend for all these pie charts is shown in \cref{tab:outputFTSE100PieChartsColourLegend}. The result for communities generated by the greedy algorithm is shown in \subref{fig:outputPieChartsCommunitiesGreedy}, whilst the result for communities generated by the spectral relaxation method is shown in \subref{fig:outputPieChartsCommunitiesSpectralRelaxation}. The output for communities generated by the modified modularity spectral clustering method is show in \subref{fig:outputPieChartsCommunitiesSpectralClustering}, whilst the result for communities generated by the Louvain method is shown in \subref{fig:outputPieChartsCommunitiesTestLouvainMethod}.}
\end{figure}

A few interesting observations can be made.
Certain industry sectors tend to subjugate their respective communities.
For example, stocks belonging to the Utilities, Basic Materials and Financials sectors seem to have remained correlated over the time period between 2004 and 2013.
But there also examples of sectors that are split amongst different communities.
A similar outcome has also been observed in \cite{MG13} with a data set consisting of stocks from the S\&P 500 index.
This suggests that subgroups of stocks within a sector are often more correlated with stock from different top-level sectors than from their own.
Unfortunately, we were not able to obtain sub-classifications for industry sectors due to lack of information available, however, we believe it would be beneficial to extract this form of qualitative information and combine with our quantitative approach identify interesting patterns in correlations.

We now introduce the notion of a renormalised filtered correlation between two communities $A$ and $B$ as described in \cite{MG13}.
We consider the renormalised filtered correlation matrix, denoted by $\widetilde{\matvar{C}}^{(g)}$, as a matrix whose dimension equal the number of communities detected with each element defined by
\begin{equation}
\label{eq:renormalisedFilteredCorrelationMatrix}
	\widetilde{C}^{(g)}_{AB} \equiv \sum_{i \in A} \sum_{j \in B} C^{(g)}_{ij}
\end{equation}
We shall use the renormalised filtered correlation to test whether different communities, generated are mutually less correlated than expected under the null model \cite{MG13}.
\Cref{fig:renormalisedFilteredCorrelationMatrices} plots the renormalised filtered correlation matrices for the communities generated by the four different algorithms.

%---   FIGURE
\begin{figure}
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/renormalisedFilteredCorrelationMatrixFastNewmanMethod.png}
		\caption{}
		\label{fig:renormalisedFilteredCorrelationMatrixGreedyAlgorithm}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/renormalisedFilteredCorrelationMatrixMontanariMethod.png}
		\caption{}
		\label{fig:renormalisedFilteredCorrelationMatrixSpectralRelaxation}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/renormalisedFilteredCorrelationMatrixFinancialSpectralClusteringMethod.png}
		\caption{}
		\label{fig:renormalisedFilteredCorrelationMatrixSpectralClustering}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/renormalisedFilteredCorrelationMatrixTestLouvainMethod.png}
		\caption{}
		\label{fig:renormalisedFilteredCorrelationMatrixTestLouvainMethod}
	\end{subfigure}
	\caption[Plots of renormalised filtered correlation matrices for the communities generated by four different algorithms.]{\label{fig:renormalisedFilteredCorrelationMatrices} Plots of renormalised filtered correlation matrices for the communities generated by four different algorithms. Each heat map represents the cross-correlations between the communities identified by each algorithm. Each off-diagonal element is calculated as the sum of the correlations between the corresponding pair of communities, whilst the diagonal entries represents the sum of correlations of nodes belonging to the specific community. The filtered correlation matrix for the output of the greedy algorithm is shown in \subref{fig:renormalisedFilteredCorrelationMatrixGreedyAlgorithm}, whilst the result for the communities generated by the spectral relaxation method is shown in \subref{fig:renormalisedFilteredCorrelationMatrixSpectralRelaxation}. The output for communities generated by the modified modularity spectral clustering method is show in \subref{fig:renormalisedFilteredCorrelationMatrixSpectralClustering}, whilst the result for the communities generated by the Louvain method is shown in \subref{fig:renormalisedFilteredCorrelationMatrixTestLouvainMethod}.}
\end{figure}

We see that all algorithms identify partitions that, on aggregate, are anti-correlated with one another using the definition of renormalised filtered correlation.

We conclude that introducing the notion of modified modularity has helped to detect more communities at the mesoscopic scale by applying either a spectral clustering algorithm or a greedy method.
The properties of these communities are similar, and the partitions are anti-correlated with one another.
The main benefit of applying the Louvain method over the spectral clustering approach is faster convergence to its generated node assignments.
Overall, as discussed in \cref{subsec:portfolioTheoryBackground}, this property is very useful for investors following mean-variance portfolio theory, since it provides a basis for picking stocks belonging to mesoscopic-level communities that are, on average over a specified period of time (decided by the investor and given by selective use of available data), negatively correlated with one another.

