% Chapter 6

\chapter{Temporal Evolution of Financial Networks}

\label{cha:temporalEvolutionFinancialNetworks}

%----------------------------------------------------------------------------------------

In this chapter we aim to understand the temporal evolution of a financial system with respect to the evolving correlation structure of the constituting assets.
In \cref{cha:communityDetectionFinancialNetworks} we considered a static financial network, where the weight of an edge represents the cross correlation between the two time series associated with the nodes connected by the edge, considering the entire time period to construct one financial network.
We then applied community detection techniques to uncover groups of stocks from the FTSE 100 that were more correlated than a null hypothesis suggests.
An issue with this approach is the static nature of the network, since investors wish to understand the strength of correlations in price movements in order to dynamically manage investment risk in their portfolios \cite{FPW+11}.
We build on the results given in \cref{cha:communityDetectionFinancialNetworks} and investigate community dynamics utilising time-dependent correlation structures with application to the same FTSE 100 data set, complementing the work of \cite{OCK+02,OKK03,BD10,FPM+10,FPW+11}.
This approach enables us to identify major changes in the underlying financial market, with the same ideas applicable to other financial markets and asset classes (by use of other available data sets), underlining the potential utility of the techniques considered.
We believe this setting of time-varying (weighted) network structure is an important application in regard to the process of adapting existing community detection algorithms to moderately-sized, real-world dynamic networks.

%-----------------------------------------------------
%   Financial Data Processing Section
%-----------------------------------------------------

\section{Constructing Time-evolving Financial Networks}
\label{sec:timeEvolvingFinancialNetwork}

Once more we consider the same FTSE 100 data set used earlier in the report, but we shift away from using the data of the whole period to construct one single network.
Instead, we examine the data for several overlapping time windows that collectively cover the whole time period.
We generate one financial network for each time window in the following way, as also considered by \cite{OCK+02,OKK03,BD10,FPM+10,FPW+11}.
Recall from \cref{subsec:financialNetworksConstructionBackground} that each node in the network (i.e. each asset) is associated with a single time series consisting of the daily logarithmic return.
We now let the number of time steps considered, $T$, equal the length of each time window rather than the length of the whole data set time period.
Proceeding to create a correlation matrix based upon the standardised time series, as before, we have developed one correlation matrix for each time window.
As previously, each cross-correlation (i.e. entry in the correlation matrix) between any two time series is the weight of the edge connecting the nodes associated with the time series in the network.
We have created a sequence of financial networks by rolling the time windows of returns through the whole data set, with each network describing the underlying correlation structure of the market at a particular time interval.
By shifting the time window there is an overlap in data in the consecutive windows, but this allows us to track the evolution of correlations and identify changes in the behaviour of the market at many different times during the whole time period \cite{FPW+11}.
This is particularly consequential given that our data set (01/01/2004 - 11/11/2013) includes some of the most significant periods for the state of developed economics worldwide in modern times.

Immediately, though, there is the requirement of deciding upon the values for both parameters, the window length and the length of the overlap (i.e. the duration of time any one window has data overlapped with the preceding time window).
Any particular choice of $T$ is a compromise between overly noisy and overly smoothed correlation coefficients \cite{OCK+02,FPW+11}.
We study the distribution of correlation coefficients for different choices of the parameters to decide on appropriate values.
For instance, \cref{fig:distributionCorrelationCoefficientsRollover1} compares the distribution of the correlation coefficients for three different values of the time window length, $T=100$, 150 and 200 (days), whilst fixing the overlap period to 1 (day).
Here we plot the mean, variance and skewness to characterise the distribution and realise these signals are quite noisy.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/distributionCorrelationCoefficientsRollover_1.png}
	\caption[Plots characterising the distribution of correlation coefficients for a fixed roll-over period of 1 day and varying window lengths.]{\label{fig:distributionCorrelationCoefficientsRollover1} Plots characterising the distribution of correlation coefficients over time for a fixed roll-over period of 1 day and varying window lengths, $T=100$, 150 and 200. The top graph plots the mean value, the centre graph plots the variance whilst the bottom graph plots the skewness all against the whole time period for the FTSE 100 data set.}
\end{figure}

Whereas, \cref{fig:distributionCorrelationCoefficientsRollover10} compares the distribution of the correlation coefficients for three different values of the time window length, $T=100$, $150$ and $200$ (days), whilst fixing the overlap period to 10 (days).
Once more we plot the mean, variance and skewness to characterise the distribution, but now realise these signals are smoother.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/distributionCorrelationCoefficientsRollover_10.png}
	\caption[Plots characterising the distribution of correlation coefficients for a fixed roll-over period of 10 days and varying window lengths.]{\label{fig:distributionCorrelationCoefficientsRollover10} Plots characterising the distribution of correlation coefficients over time for a fixed roll-over period of 10 days and varying window lengths, $T=100$, 150 and 200. The top graph plots the mean value, the centre graph plots the variance whilst the bottom graph plots the skewness all against the whole time period for the FTSE 100 data set.}
\end{figure}

We decide to trade-off the over-smoothing of shorter window lengths with the longer overlap period, and proceed with $T=100$ and a value of overlap period equal to 10 days.
These choices results, for our particular data set, in 240 correlation matrices (and hence financial networks) collectively spanning the period between 01/01/2004 and 11/11/2013.
This completes the construction of time evolving financial networks, which we can now use as an empirical test basis for our proceeding analysis of a community detection algorithm in dynamic networks.

%-----------------------------------------------------
%   Temporal Evolution of Correlation Section
%-----------------------------------------------------

\section{Temporal Evolution of Asset Correlations}
\label{sec:temporalEvolutionAssetCorrelations}

We have seen in \cref{subsec:randomMatrixTheory}, how the eigenvalue spectrum of the empirical correlation matrix generated by the aggregate set indicated correlations not compatible with the null model of a combination of random and market-wide components.
This result became a justification for seeking mesoscopic level communities by discovering patterns in the correlation structure of the financial network, and was a building block of our modified modularity approach.
We now wish to investigate the temporal evolution of the correlations between the stocks by analysing the behaviour of the respective correlation matrix eigenvalues and eigenvectors at each time window.
Having previously used tools from RMT, we now utilise a closely linked and widely used technique, known as principal component analysis (PCA) \cite{FPW+11}.
We mentioned PCA earlier in the report in regard to the detection of hidden communities generated by the hidden clique model, although, now we delve into the technical details of the procedure.
PCA is a statistical technique that uses an orthogonal transformation to represent the covariance structure of a set of variables through a smaller number of linear combinations of these variables. \cite{Jol02,FPW+11,Gil14b}.

We shall consider the following derivation based on \cite{Jol02,UIO03,FPM+10,FPW+11,Gil14b}.
Firstly, recall $T$ represents the time window length and $n$ represents the number of assets in the network (these values equal to 100 and 80, respectively, in our specific example).
Let us denote the matrix with entries consisting of the standardised logarithmic returns, for any one particular time window, by a $n \times T$ matrix denoted by $\matvar{X}$.
The empirical correlation matrix, also equal to the covariance matrix of $\matvar{X}$, is denoted by $\matvar{C}$ and defined by
\begin{equation}
	\label{eq:correlationMatrix}
	\matvar{C} = \frac{1}{T} \matvar{X} \transpose{\matvar{X}}
\end{equation}
where each element $C_{ij}$ represents the cross-correlation between time series $i$ and $j$.
The matrix $\matvar{X}$ constitutes the set of observed (original) variables, and the PCA method seeks to find a linear transformation, denoted by the matrix, $\matvar{\Omega}$, that maps $\matvar{X}$ into a set of uncorrelated variables given by $\matvar{Y}$.
$\matvar{Y}$ is thus an $n \times T$ matrix defined by
\begin{equation}
	\label{eq:uncorrelatedVariables}
	\matvar{Y} = \matvar{\Omega} \matvar{X} 
\end{equation}
where each row $\vecvar{y}_{i}$ (for $i =1,\dots,n$) corresponds to a principal component (PC) of $\matvar{X}$.
The first row of the matrix $\matvar{\Omega}$, denoted by $\vecvar{\omega}_{1}$, is selected so the first PC, $\vecvar{y}_{1}$, is aligned with the direction of maximal variance in the $n$-dimensional space defined by $\matvar{X}$.
Each subsequent PC accounts for the maximal variance of $\matvar{X}$ subject to the condition that the vectors $\vecvar{\omega}_{j}$ are mutually orthonormal.
This implies that
\begin{equation}
	\label{eq:PCCoefficients}
	 \vecvar{\omega}_{j} \transpose{\vecvar{\omega}_{k}} =
	\begin{cases}
		1 & \text{if } j = k\\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
for all $j,k = 1,\dots,n$.
The correlation matrix is an $n \times n$ symmetric and diagonalisable matrix, which can be written as 
\begin{equation}
	\label{eq:diagonalisationCorrelationMatrix}
	\matvar{C} = \matvar{\Phi} \matvar{\Lambda} \transpose{\matvar{\Phi}}
\end{equation}
where $\matvar{\Phi}$ is an orthogonal matrix of its eigenvectors and $\matvar{\Lambda}$ is diagonal matrix consisting of the associated eigenvalues.
From the result that the eigenvectors of the covariance matrix correspond to the directions of maximal variance, $\matvar{\Omega} = \transpose{\matvar{\Phi}}$, and thus we can determine the PCs using the decomposition given by \cref{eq:diagonalisationCorrelationMatrix} \cite{Jol02}.

\Cref{subsec:randomMatrixTheory} compares the eigenvalue spectra of an empirical correlation matrix (using our entire data set) against a correlation matrix created from $n$ random time series of length $T$ in the limiting case.
The analysis found significant features of the spectra of the empirical correlation matrix.
Most of the eigenvalues were contained in a region explained as random noise and given by the Sengupta-Mitra distribution.
However, a selection of eigenvalues lay outside this region, as illustrated in \cref{fig:eigenvalueSpectra}, suggesting some form of correlation structure between the microscopic and macroscopic exists.
We crucially note that the condition of $T/n > 1$ must be satisfied for the result to hold, but only requires appropriate selection of the parameter $T$.
We have achieved in satisfying this constraint and thus the result from RMT applies to all of our time-windowed financial networks as well.

We can now combine the results from PCA and RMT to study the temporal evolution of correlation structure, considering the approaches from \cite{UIO03,FPM+10,FPW+11}.
We denote the covariance matrix for the PC matrix $\matvar{Y}$ by $\matvar{\Sigma}$, which is defined as
\begin{equation}
	\label{eq:PCCovarainceMatrix}
	\matvar{\Sigma} = \frac{1}{T} \matvar{Y} \transpose{\matvar{Y}}
\end{equation}
Using \cref{eq:correlationMatrix,eq:uncorrelatedVariables,eq:diagonalisationCorrelationMatrix}, we can re-write $\matvar{\Sigma}$ as
\begin{equation}
	\label{eq:PCCovarainceMatrixRewritten}
	\matvar{\Sigma} = \frac{1}{T} \matvar{\Omega} \matvar{X} \transpose{\matvar{X}} \transpose{\matvar{\Omega}} = \matvar{\Omega} \matvar{C} \transpose{\matvar{\Omega}} = \transpose{\matvar{\Phi}} \matvar{C} \matvar{\Phi} = \matvar{\Lambda}
\end{equation}
We wish to find the total variance in the logarithmic returns for all assets.
Let us denote the $i$-th entry along the diagonal of $\matvar{\Lambda}$ by $\lambda_{i}$ (i.e. $\lambda_{i} = \Lambda_{ii}$).
Then the total variance for $\matvar{X}$ is given by
\begin{equation}
	\label{eq:totalVarianceLogarithmicReturns}
	\trace{\matvar{C}} = \sum_{i=1}^{n} \lambda_{i} = \trace{\matvar{\Lambda}} = n
\end{equation}
Therefore the proportion of the total variance in $\matvar{X}$ given by the $i$-th PC is given by
\begin{equation}
	\label{eq:proportionVarianceLogarithmicReturns}
	\frac{\Sigma_{ii}}{\trace{\matvar{C}}} = \frac{\lambda_{i}}{n}
\end{equation}
We can now analyse the time-varying nature of the proportion of variance of returns explained by certain PCs.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/eigenvalueContributionsPC.png}
	\caption[Plots of the contribution of the top 5 principal components to the total variance in returns against time.]{\label{fig:eigenvalueContributionsPC} Plots of the contribution of the top 5 principal components to the total variance in returns against time. Each data point corresponds to the term $\lambda_{i}/n$, defined in \cref{eq:proportionVarianceLogarithmicReturns}, for $i=1,\dots,5$ and a particular time window.}
\end{figure}

\Cref{fig:eigenvalueContributionsPC} shows the proportion of variance of returns explained by the top 5 PCs for each time window, so we plot $\lambda_{i}/n$ for $i=1,\dots,5$ and every time-windowed financial network.
The fraction of the variance explained by the first PC increased between 2005 and towards the end of 2006, then dipped for a few months through towards the middle of 2007.
There was a sharp rise beginning at the middle of 2007, around the time when the United States subprime mortgage industry collapsed \cite{GrWik}.
Also several central banks stepped in with lending to the interbank lending market in August 2007 in order to prevent a liquidity crisis \cite{BKM13,GrWik}.
The bursting of the United States housing bubble had a major impact on the health of major worldwide financial institutions due to their massive exposure to mortgage-backed securities on their balance sheets \cite{BKM13,GrWik}.
For instance, Merrill Lynch was taken over by Bank of America and Lehman Brother filed for bankruptcy on 15/09/2008, and we see the proportion of variance of returns contributed by the first PC increased from this date onwards towards 2011, a significant period of time during the recession \cite{FPW+11,GrWik}.
The large contribution in variance of returns by only one principal component indicates a significant amount of common variation in the market (i.e. in the FTSE 100 exchange in our case) \cite{FPW+11}.
In other words, the market, as a whole, became more correlated during these years, in particular a very distinct reaction to a major crisis to a few international financial institutions.
This information, implied by \cref{fig:eigenvalueContributionsPC}, relates to the intuition that the performance of stocks should decline, as a whole, during a financial crisis, so the returns will be more positively correlated during this period.
In addition, in 2004, the top twelve PCs contributed to 64.12\% of the variance of returns, whilst, in 2009, just the top six PCs contributed to 62.82\%.
The fact that fewer components are required to account for a similar amount of variation in returns, in 2009, compared to five years earlier suggests an increase in the common variation between stocks in the exchange.
Moreover, it also implies the correlation structure of this marker can be explained by many fewer factors than $n$ sets of asset time series.
The last observation supports the eigenvalue spectra analysis of \cref{subsec:randomMatrixTheory}.

%-----------------------------------------------------
%   Generalised Modularity Optimisation Section
%-----------------------------------------------------

\section{Generalised Modularity Optimisation}
\label{sec:generalisedModularityOptimisation}

We have seen in previous chapters how modularity optimisation algorithms can be used to partition static networks (for both unweighed and weighted cases) to uncover community structure.
Rather, we now we seek to detect communities in a time-dependent weighted network, for instance a network created by using the method outlined in \cref{sec:timeEvolvingFinancialNetwork}.
The previous setting for modularity optimisation is not sufficient for this regime, and thus we require a more general framework.
Mucha et al. \cite{MRM+10} have developed a framework that enables us to study the community structure of \emphT{multislice networks} by using a multislice modularity function.
Multislice networks are essentially combinations of networks (slices) coupled through links connecting each node of one slice to itself in other slices \cite{MRM+10}.
This is a general setting that includes time-evolving networks and is thus suitable for our application.
The idea considered by \cite{MRM+10} is to derive a generalisation of community detection to multislice networks by formulating an appropriate null model derived from Laplacian dynamics, a concept introduced by Lambiotte et al. \cite{LDB08}.
We shall provide a summary of the derivations available from \cite{LDB08,Lam10,MRM+10} that highlights the approach.
Then we will discuss one algorithm to optimise generalised modularity in time-dependent networks known as the \emphT{generalised Louvain method}, described also in \cite{MRM+10,MFF+11,BPW+13}.

%-----------------------------------------------------
%   Laplacian Dynamics and Stability Sub Section
%-----------------------------------------------------

\subsection{Laplacian Dynamics and Stability}
\label{subsec:laplacianDynamicsAndStability}

We first consider a quality function, called \emphT{stability}, developed by \cite{LDB08}.
One idea motivating stability is that the flow of probability on a network should be trapped in a community for a long period of time \cite{Lam10}.
Let $\graphvar{M}(t)$ be an ergodic Markov process on the network.
Due to the ergodicity property, any initial configuration will eventually reach the same steady state distribution \cite{LDB08}.
Suppose the process $\graphvar{M}(t)$ is the motion of a random walker on the network and consider a partition of the network given by $\graphvar{P} = \{ G_{1},\dots,G_{k} \}$, where $G_{i}$ represents the $i$-th community \cite{LDB08}.
We define the stability, denoted by $R_{\graphvar{M}(t)}$, by
\begin{equation}
	\label{eq:stability}
	R_{\graphvar{M}(t)} = \sum_{C \in \graphvar{P}} P(C,t) - P(C,\infty)
\end{equation}
where $P(C,t)$ is the probability of the walker being within the community $C$ both initially and at the time $t$ \cite{LDB08}.
This definition arises from the autocovariance function of a Markov process on the network, and, intuitively, stability measures the quality of a partition based on how persistent it is \cite{DYB08}.

Lambiotte et al. \cite{LDB08} consider normalised Laplacian dynamics to model an unbiased discrete-time random walker on the network, where the probability of moving from one node to another is proportional to the weight of the edge which connects them.
Therefore,
\begin{equation}
	\label{eq:densityRandomWalker}
	p_{i}(t+1) = \sum_{j} \frac{A_{ij}}{d_{j}} p_{j}(t)
\end{equation}
where $p_{i}(t)$ represents the density of a random walker on node $i$ at time $t$, $A_{ij}$ is the $(i,j)$-th element of the network's adjacency matrix and $d_{j} = \sum_{i} A_{ij}$ is the strength of node $j$ \cite{LDB08}.
In order to find the stationary distribution, we solve the following equation
\begin{equation}
	\label{eq:solveDensityRandomWalkerStationaryDistribution}
	p_{i}^{*} = \sum_{j} \frac{A_{ij}}{d_{j}} p_{j}^{*}
\end{equation}
which results in the steady state distribution given by $p_{i}^{*} = d_{i}/2m$, where $2m = \sum_{i} d_{i} = \sum_{ij} A_{ij}$ \cite{LDB08}.
Notice that this is the same null model considered for our derivation of the modularity function.
Proceeding to find the stability at time $t=1$,
\begin{equation}
	\label{eq:stabilityAtTime1}
	R(1) = \sum_{C \in \graphvar{P}} \sum_{i,j \in C} \left( \frac{A_{ij}}{d_{j}} \frac{d_{j}}{2m} - \frac{d_{i}}{2m} \frac{d_{j}}{2m} \right) = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_{i}d_{j}}{2m} \right) \delta(g_{i},g_{j})
\end{equation}
which is equivalent to our formulation of modularity, $Q$, considered for undirected networks.

In order to encompass time as a parameter within the formulation, we consider a continuous-time process associated with the random walker \cite{LDB08}.
Consider independent and identical homogeneous Poisson processes defined on each node of the graph \cite{LDB08}, so that the walkers moving at a constant rate from each node, then
\begin{equation}
	\label{eq:densityRandomWalkerContinuousTime}
	\timeDiff{p_{i}} = \sum_{j} \frac{A_{ij}}{d_{j}} p_{j} - p_{i}
\end{equation}
This equation is driven by the operator $A_{ij}/d_{j} - \delta_{ij} = -L_{ij}$, where we denote $\matvar{L}$ as the normalised Laplacian matrix we defined in \cref{def:normalisedLaplacianMatrix}, where we change our notation slightly for convenience \cite{MRM+10}.
The stationary distribution of this continuous-time process is the same as with the discrete-time process, $p_{i}^{*} = d_{i}/2m$,
Furthermore, the stability of the partition is given by
\begin{equation}
	\label{eq:stabilityContinuousTime}
	R(t) = \sum_{C \in \graphvar{P}} \sum_{i,j \in C} \left( e^{t \matvar{L}} p_{j}^{*} - p_{i}^{*} p_{j}^{*} \right) = \sum_{C \in \graphvar{P}} \sum_{i,j \in C} \left( e^{t \matvar{L}} \frac{d_{j}}{2m} - \frac{d_{i}}{2m} \frac{d_{j}}{2m} \right)
\end{equation}
By expanding the matrix exponential in \cref{eq:stabilityContinuousTime} to the first order in $t$ \cite{LDB08}, so that $\left( e^{t \matvar{L}} \right)_{ij} \approx \delta_{ij} + tL_{ij}$, and ignoring the $\delta_{ij}$ factors (since they always contribute to the sum and are thus irrelevant for identifying the optimal partition) the equation becomes
\begin{equation}
	\label{eq:stabilityContinuousTimeApproximation}
	R(t) \approx \frac{1}{2m} \sum_{ij} \left( tA_{ij} - \frac{d_{i}d_{j}}{2m} \right) \delta(g_{i},g_{j})
\end{equation}
By dividing by $t$, which does not affect the optimality of a partition given $t$, and defining a resolution parameter, $\gamma = 1/t$, we yield the following quality function
\begin{equation}
	\label{eq:modularityFromStabilityContinuousTimeApproximation}
	Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \gamma \frac{d_{i}d_{j}}{2m} \right) \delta(g_{i},g_{j})
\end{equation}

%-----------------------------------------------------
%   Stability for Dynamic Networks Sub Section
%-----------------------------------------------------

\subsection{Stability for Dynamic Networks}
\label{subsec:stabilityForDynamicNetworks}

So far we have focused the stability function for single, static networks.
We now consider the approach Mucha et al. \cite{MRM+10} used to extend this model to develop a modularity function for multislice networks.
For our application, we may only think of the case of \emphT{dynamic networks}, where the nodes or edges change over time.
An example for the most general case is illustrated in \cref{fig:exampleMultisliceNetwork}, which we have obtained from \cite{OxNet}.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/multislice_framework.png}
	\caption[Illustration of an example multislice network with three slices.]{\label{fig:exampleMultisliceNetwork} Illustration of an example multislice network with three slices. We can differentiate between the \emphT{inter-slice} connections represented by $C_{jrs}$ and the \emphT{intra-slice} connections represented by $A_{ijs}$. The couplings are only between adjacent slices, as would be appropriate for ordered slices. We have obtained this illustration from \cite{OxNet}.}
\end{figure}

This approach requires a generalisation of the stability function \cite{MRM+10}.
Replace the $p_{i}^{*} p_{j}^{*}$ independent contribution from \cref{eq:stabilityContinuousTimeApproximation} with a conditional independent contribution $\rho_{i \given j} p_{j}^{*}$, where $\rho_{i \given j}$ is the conditional probability at stationarity of jumping from node $j$ to node $i$ along the edge type allowed in the network (in our case, this will be edges between nodes in a given slice or between the same node in different slices) \cite{MRM+10}.
By considering the linearisation of the exponential \cite{MRM+10}, we can write the stability of a partition as
\begin{equation}
	\label{eq:multisliceStability}
	R(t) = \sum_{ij} \left( tL_{ij} p_{j}^{*} - \rho_{i \given j} p_{j}^{*} \right) \delta(g_{i},g_{j})
\end{equation}
We can represent each slice, $s$, of a multislice network by adjacencies $A_{ijs}$ between nodes $i$ and $j$ with the inter-slice couplings, denoted by $C_{jrs}$, representing the connection between node $j$ in slice $r$ to itself in slice $s$ \cite{MRM+10}.
Let us define the notation, $d_{js} = \sum_{i} A_{ijs}$ and $c_{js} = \sum_{r} C_{jsr}$, whilst defining the multislice strength of node $j$ in slice $s$ by $\kappa_{js} = k_{js} + c_{js}$ \cite{MRM+10}.
We can now re-write \cref{eq:densityRandomWalkerContinuousTime} to give the continuous-time Laplacian dynamics equation
\begin{equation}
	\label{eq:continuousTimeLaplacianDynamics}
	\timeDiff{p_{is}} = \sum_{jr} \left( A_{ijs} \delta_{sr} +\delta_{ij} C_{jsr} \right) \frac{p_{jr}}{\kappa_{jr}} - p_{is}
\end{equation}
The steady state distribution becomes $p_{jr}^{*} = \kappa_{jr}/2\mu$, where $2\mu = \sum_{jr} \kappa_{jr}$ \cite{MRM+10}.
Notice that the $\rho_{is \given jr}$ term accounts for two contributions, one from the intra-slice step where $s=r$, and the other from the inter-slice step where $i=j$ \cite{MRM+10}.
The intra-slice step contribution is given by $\left( k_{is}/2m_{s} \right) \left( k_{js}/\kappa_{js} \right)$, whilst the inter-slice step contribution is given by $\left( C_{isr}/c_{ir} \right) \left( c_{ir}/\kappa_{ir} \right)$, where $m_{s} = \sum_{i} k_{is}$ \cite{MRM+10}.
We can now write the stationary distribution as
\begin{equation}
	\label{eq:continuousTimeLaplacianDynamicsStationaryDistribution}
	\rho_{is \given jr} p_{jr}^{*} = \left( \frac{k_{is}}{2m_{s}} \frac{k_{jr}}{\kappa_{jr}} \delta_{sr} + \frac{C_{jsr}}{c_{jr}} \frac{c_{jr}}{\kappa_{jr}} \delta_{ij} \right) \frac{\kappa_{jr}}{2\mu} = \frac{1}{2\mu} \left( \frac{k_{is}k_{jr}}{2m_{s}} \delta_{sr} + C_{jsr} \delta_{ij} \right)
\end{equation}
Substituting \cref{eq:continuousTimeLaplacianDynamicsStationaryDistribution} into \cref{eq:multisliceStability} and again considering a linear approximation provides the following expression
\begin{equation}
	\label{eq:multisliceModularity}
	Q = \frac{1}{2\mu} \sum_{ijsr} \left[ \left( A_{ijs} - \gamma \frac{k_{is}k_{js}}{2m_{s}} \right) \delta_{sr} + (1-\gamma) \delta_{ij} C_{jsr} \right] \delta(g_{is},g_{jr})
\end{equation}
We can also reweight the conditional probabilities which will allow for more flexible resolution parameters, where we have a different resolution parameter for each slice \cite{MRM+10}.
This gives our final expression for the multislice generalisation of modularity
\begin{equation}
	\label{eq:multisliceModularityReweighted}
	Q = \frac{1}{2\mu} \sum_{ijsr} \left[ \left( A_{ijs} - \gamma_{s} \frac{k_{is}k_{js}}{2m_{s}} \right) \delta_{sr} + \delta_{ij} C_{jsr} \right] \delta(g_{is},g_{jr})
\end{equation}
where the resolution parameters for the inter-slice couplings have been absorbed into the magnitude of the elements of $C_{jsr}$ \cite{MRM+10}.
As in \cite{MRM+10}, we assume, for simplicity, $C_{jsr} \in \{ 0,\omega \}$ indicating the presence ($\omega$) or absence (0) of inter-slice edges.
Therefore, only the variable $\omega$ is required to adjust the couplings between communities in different network slices \cite{BPW+13}.

%-----------------------------------------------------
%   Generalised Louvain Method Sub Section
%-----------------------------------------------------

\subsection{Generalised Louvain Method}
\label{subsec:generalisedLouvainMethod}

We have seen the derivation for the generalised modularity function which we can use to partition dynamic networks, such as time-evolving financial networks.
However, as we have seen with standard modularity optimisation, it is NP-hard to find the global optimum partition, thus we seek approximation methods which give near-optimal solutions.
One such algorithm is the generalised Louvain method described in \cite{MRM+10,MFF+11}.
As the name suggests, it uses a procedure similar to the Louvain method we have discussed throughout the report, but works for dynamic networks also.
This algorithm has been implemented by Mucha et al. in MATLAB, and the code is available from \cite{GenLou}.
We remark that the decision of the parameters $\gamma_{s}$ and $\omega$ are important and do alter the communities generated, and intelligent choice of such parameters depending on the specific application is required to provide `good' partitions.

This algorithm has been applied to real-world networks previously.
For instance \cite{MRM+10} detected communities in a time-dependent networks constructed from roll call voting in the Unites States Senate.
In \cite{HGH+12}, the technique was applied to a network based on geographic and social information as well as a computationally-expensive application on an image segmentation problem.
More recently, Bassett et al. \cite{BPW+13}, analysed empirical neuroscience data in the form of brain and behavioural networks.

%-----------------------------------------------------
%   Synthetic Data Testing Section
%-----------------------------------------------------

\section{Synthetic Data Testing}
\label{sec:temporalEvolutionSyntheticDataTesting}

Similarly for the case of static networks in \cref{sec:syntheticDataTesting}, we wish to test the algorithm on synthetic data before applying to our empirical data.
We consider a benchmark data set of correlation matrices with 100 time series, each with 2500 data points, divided into 10 communities of 10 correlated time series.
The set consists of correlation matrices generated with different levels of correlations between the groups and within a group reflecting different signal-to-noise ratios (SNR), where a lower value for SNR results in more difficulty in identifying the ground-truth partition.
For this synthetic data set, we consider four different values for the SNR, $\{0.5, 0.6, 0.7, 0.8\}$.
We can then use this data to construct time-evolving financial networks, by applying the rolling-window procedure outlined in \cref{sec:timeEvolvingFinancialNetwork}, where we choose the window length to be $T = 100$ and the overlap period to be equal to 10.
This generates 240 correlation matrices that combined represent the temporal evolution of the correlation structure forming our synthetic data set. 
We then utilise the modified modularity techniques discussed in \cref{cha:communityDetectionFinancialNetworks} to construct the modified modularity matrix from each network slice (i.e. from each of the 240 correlation matrices) to finally generate our time-evolving modified modularity matrices, which we will be used as input to the generalised Louvain method.

We wish to use benchmark data to find good values for the parameters of the generalised Louvain method.
Since the computation time for this algorithm is rather long, we choose to simply let $\gamma_{s}=\gamma=1$ for all $s$.
Choosing the best parameter value for $\omega$ is trickier, and thus we decide to consider four different values, $\omega \in \{ 0.25,0.5,0.75,1 \}$.
\Cref{tab:generalisedLouvainMethodModularitiesSyntheticData} shows the modularities obtained by applying the generalised Louvain method with the different values of $\omega$ and $\gamma=1$ to the synthetic data set of different SNRs.
The first, and obvious, observation is that the modularity values obtained increase, on the whole, as the SNR increases, matching the intuition that the community detection problem is simpler as we increase the inter-community correlations and decrease the intra-community correlations.
A more insightful observation is that the modularities for the algorithm with $\omega=0.75$ and $\omega=1$ are the best across the varying SNRs.
This suggests either one of these two values for the parameter would be appropriate.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Modularities obtained for the generalised Louvain method with different values of $\omega$ and common parameter $\gamma = 1$ obtained from applying the methods to synthetic data generated with different SNR values.}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    \textbf{SNR} & \multicolumn{4}{|c|}{\textbf{Generalised Louvain Method Modularities}}\\
    \hline
    & $\bm{\omega = 0.25}$ & $\bm{\omega = 0.50}$ & $\bm{\omega = 0.75}$ & $\bm{\omega = 1.0}$\\
    0.5   & 0.586862 & 0.596196 & 0.602225 & 0.602948 \\
    0.6   & 0.615603 & 0.624678 & 0.631786 & 0.629889 \\
    0.7   & 0.650051 & 0.658629 & 0.665545 & 0.660805 \\
    0.8   & 0.659997 & 0.667846 & 0.676168 & 0.668351 \\
    \hline
    \end{tabular}%
  \label{tab:generalisedLouvainMethodModularitiesSyntheticData}%
\end{table}%

In order to analyse the performance with respect to different values of $\omega$ from a different perspective, and to further back up the observations made from \cref{tab:generalisedLouvainMethodModularitiesSyntheticData}, we compare the community partitions using the \emphT{normalised variation of information}, which we denote by $V$ \cite{MM03,FPM+10}.
The \emphT{entropy} of a partition $\graphvar{P}$ of graph, consisting of $n$ nodes, into $k$ communities, $\{ G_{1},\dots,G_{k} \}$, is defined as
\begin{equation}
	\label{eq:entropy}
	S(\graphvar{P}) = - \sum_{i=1}^{k} p(i) \natlog{p(i)}
\end{equation}
where $p(i) = \cardinality{G_{i}}/n$ is the probability of a randomly selected node belonging to community $i$ \cite{MM03,FPM+10}.
Intuitively, entropy for a partition indicates the uncertainty in the community membership of a node \cite{FPM+10}.
The \emphT{mutual information} between two partitions, $\graphvar{P}$ and $\graphvar{P'}$ is defined as
\begin{equation}
	\label{eq:mutualInformation}
	I(\graphvar{P},\graphvar{P'}) = \sum_{i=1}^{k} \sum_{j=1}^{k'} p(i,j) \natlog{\frac{p(i,j)}{p(i)p(j)}}
\end{equation}
where we assume the partition $\graphvar{P'}$ groups the nodes into $k'$ communities and $p(i,j) = \cardinality{G_{i} \cap G_{j}}/n$.
The mutual information is the amount by which the knowledge of a node community membership in $\graphvar{P}$  reduces the uncertainty of its community membership in $\graphvar{P'}$ \cite{FPM+10}.
We can now define the normalised variation of information between $\graphvar{P}$ and $\graphvar{P'}$ as
\begin{equation}
	\label{eq:normalisedVariationOfInformation}
	V(\graphvar{P},\graphvar{P'}) = \frac{S(\graphvar{P}) + S(\graphvar{P'}) - 2I(\graphvar{P},\graphvar{P'})}{\natlog{n}}
\end{equation}
The division by $\natlog{n}$ enables $V(\graphvar{P},\graphvar{P'}) \in [0,1]$ (when comparing data sets with the same size), with a value of 0 indicating identical partitions and a value of 1 indicating that all nodes are in individual communities in one partition and in a single community in the other \cite{FPM+10}.
As \cite{FPM+10} explains, the normalised variation of information is a useful measure for quantifying the difference between partitions, since it is metric on the space of community assignments and satisfies the triangle inequality.
Therefore, if two partitions of a network are close to a third partition, they cannot differ too much from one another \cite{FPM+10}.
Hence, we can compare the normalised variation of information between each of the partitions generated by the generalised Louvain method (for the same prescription of parameters as described previously) with the ground truth partition that we used to generate the synthetic data.
Note that lower values indicate the generated partition is close to the ground-truth node assignments and thus performed better.
In order to establish a comparison between the generalised Louvain method and just applying the Louvain method for each network slice individually, we run the modified modularity Louvain method also (note this essentially corresponds to the special case of $\omega = 0$ of the generalised Louvain method, but the computation time to run the algorithm is reduced).
In order to present a comparison, we average the normalise variation of information values for all five methods over all synthetic data sets (i.e. across all SNR values) in order to provide a measure of performance across varying difficulties of the problem.
The results are plotted in \cref{fig:averagedNormalisedVariationOfInformation}.
We notice from \cref{fig:averagedNormalisedVariationOfInformation} that, once more, the best choice seems to be $\omega = 0.75$ or $\omega = 1$.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/averagedNormalisedVariationOfInformationTemporalSyntheticData.png}
	\caption[Plots of the normalised variation of information obtained by applying the generalised Louvain method with different parameters.]{\label{fig:averagedNormalisedVariationOfInformation} Plots of the normalised variation of information obtained by applying the generalised Louvain method (denoted as GLM in the key) with different parameters and comparing the output partitions with the ground-truth node assignments used to generate the synthetic data. We also apply the modified modularity based Louvain method to individual network slices to provide a baseline comparison. The values for $V$ are averaged over synthetic data sets with varying SNRs, to provide a measure of performance across varying difficulties of the problem.}
\end{figure}

We therefore conclude from the synthetic testing that we shall use the generalised Louvain method, with parameters $\gamma=1$ and $\omega=1$, with the modified modularity matrices, at each network slice, passed as input in order to obtain the best partitions possible for the application on our real-world data set.
We also noticed that the generalised Louvain method is more computationally expensive that just applying the modified modularity based Louvain method for every network slice independently, as the observation of \cite{HGH+12} suggested.
Therefore we shall also apply modified modularity based Louvain method to our time-evolving financial networks and compare both the output partitions and run time.

%-------------------------------------------------------------------------
%   Community Detection in Real-world Time-evolving Financial Networks Section
%-------------------------------------------------------------------------

\section{Community Detection in Real-world Time-evolving  Financial Networks}
\label{sec:communityDetectionRealWorldTimeEvolvingFinancialNetworks}

Using the generalised Louvain method, tested on synthetically generated data in \cref{sec:temporalEvolutionSyntheticDataTesting} and used to maximise the generalised modularity function derived in \cref{sec:generalisedModularityOptimisation}, we wish to identify dynamic communities in the time-evolving financial networks (constructed from our FTSE 100 stock price data set by following the procedure outlined in \cref{sec:timeEvolvingFinancialNetwork}).
Once more, we utilise the MATALB code for the generalised Louvain method provided by \cite{GenLou}.
The data is processed with a window length of $T = 100$ and an overlap of 10 which generated 240 correlation matrices, each representing a financial network slice.
We choose parameters for the generalised Louvain method by setting $\gamma = 1$ and $\omega = 1$ (this is appropriate considering our analysis in the previous section of the performance on synthetic data with across a variation of parameters).
We shall also apply our modified modularity based Louvain method, treating each network slice (i.e. each correlation matrix) as a static network, and unifying the communities detected in order to provide a baseline for time-evolving partitions.

The output when the standard Louvain method was applied is shown in \cref{fig:standardLouvainMethodTemporalCommunities}, whilst the output when the generalised Louvain method was applied is shown in \cref{fig:generalisedLouvainMethodTemporalCommunities}.
We plotted both figures with each stock ticker representing the vertical axis, and time, the horizontal axis (i.e. one data point for each stock and time window).
The tickers are those listed in detail in \cref{app:listFTSE100Stocks}.
Community memberships are identified by colour, which allows us to understand the temporal community memberships associated with each individual stock.
We can make several remarks.
Firstly, the plot for the standard Louvain method is pretty noisy with partitions changing rapidly between some time windows, but the plot for the generalised Louvain method is a lot smoother.
This is an encouraging observation given that the latter algorithm is designed for community detection in time-evolving networks.
Also, this enables us to more easily analyse the changes community structure globally across time since the information is presented more clearly.
Secondly, we notice a change in structure at around the 2008-2009 time period, where there are fewer communities generated, whereas in the pre-2007 and post 2011 periods, there are 3 or 4 distinct communities.
This relates to the intuition of higher correlations and thus fewer communities when the market, as a whole, is experiencing a downturn.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/testLouvainMethodTemporalCommunities.png}
	\caption[Plot of the temporal community memberships generated by applying the standard Louvain method to each of the networks slices of our time-evolving financial system.]{\label{fig:standardLouvainMethodTemporalCommunities} Plot of the temporal community memberships generated by applying the standard Louvain method to each of the networks slices of our time-evolving financial system. The vertical axis compromise of the 80 tickers associated with the stocks we consider, whilst the horizontal axis represents time. The tickers are those listed in detail in \cref{app:listFTSE100Stocks}. Community memberships are represented by colour and the key is shown to the right of the figure. Hence, we can understand the temporal community memberships of each individual stock for each time window, and relate a global picture of the communities associated with the system throughout various time periods.}
\end{figure}

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{figures/testMultiSliceLouvainMethodCommunities_omega_1.png}
	\caption[Plot of the temporal community memberships generated by applying the generalised Louvain method to each of the networks slices of our time-evolving financial system.]{\label{fig:generalisedLouvainMethodTemporalCommunities} Plot of the temporal community memberships generated by applying the generalised Louvain method, with $\gamma = 1$ and $\omega=1$, to each of the networks slices of our time-evolving financial system. The vertical axis compromise of the 80 tickers associated with the stocks we consider, whilst the horizontal axis represents time. The tickers are those listed in detail in \cref{app:listFTSE100Stocks}. Community memberships are represented by colour and the key is shown to the right of the figure. Hence, we can understand the temporal community memberships of each individual stock for each time window, and relate a global picture of the communities associated with the system throughout various time periods.}
\end{figure}

We have also calculated the normalised variation of information, denoted by $V$, between each of the partitions generated by the two algorithm for each time window.
The results are shown in \cref{fig:normalisedVariationOfInformationTemporalCommunities}.
An interesting feature of this plot is the sudden drop in $V$ in the period 2008-2009, and the subsequent increase from 2010 onwards.
This means the partitions generated by the two algorithms were unusually close between 2008 and 2009, which again implies in stronger correlations and fewer communities apparent in the system, since the community detection problem for those slices in this period seemed to be simpler.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/normalisedVariationOfInformationTemporalCommunities.png}
	\caption[Plot of the normalised variation of information between the partitions generated by the standard and generalised Louvain methods for each time window.]{\label{fig:normalisedVariationOfInformationTemporalCommunities} Plot of the normalised variation of information between the partitions generated by the standard Louvain method and the generalised Louvain method (with $\gamma = 1$ and $\omega=1$) for each time window.}
\end{figure}

To investigate this last point further, we calculate the modified modularity, using \cref{eq:newModularityFinancialNetworks}, for each partition (i.e. the community assignments for all time windows) for both algorithms, and plot the results in \cref{fig:modifiedModularitiesEachPartition}.
We observe two important features from the graph.
Firstly the modified modularities obtained by both algorithms reaches maxima in the period between 2008 and 2009, suggesting our understanding from analysing \cref{fig:normalisedVariationOfInformationTemporalCommunities} is sound.
Therefore, from these network diagnostics based on community structure, we do observe significant changes in the correlation structure within this market in the time period 2008-2009.
Secondly, we find the modified modularity curve for the generalised Louvain method to be a lot smoother, and this again reflects how it was designed to detect communities in dynamic networks.

%---   FIGURE
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/modifiedModularitiesEachPartition.png}
	\caption[Plot of the modified modularity for each partition generated by both the generalised and standard Louvain methods.]{\label{fig:modifiedModularitiesEachPartition} Plot of the modified modularity, calculated using \cref{eq:newModularityFinancialNetworks}, for each partition generated by both the generalised and standard Louvain methods, with parameters set as previously considered.}
\end{figure}

We summarise by recommending that the generalised Louvain method should be considered as a more robust technique than by simply applying traditional, static network, modularity optimisation methods to each slice when detecting communities in dynamic financial networks.
Although, we should bear in mind the generalised Louvain method has a longer computation time and we must determine appropriate values for the parameters $\gamma$ and $\omega$.