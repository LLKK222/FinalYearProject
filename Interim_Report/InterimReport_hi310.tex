\documentclass[12pt]{article}

%----------------------------------------------------------------------------------------
%	PACKAGES
%----------------------------------------------------------------------------------------

\usepackage{cite}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{isomath}
\usepackage{hyperref}
\usepackage[hypcap]{caption}
\usepackage[nottoc,notlof]{tocbibind}

%----------------------------------------------------------------------------------------
%	PAGE & LINKS SETUP
%----------------------------------------------------------------------------------------

% Default margins are too wide all the way around. I reset them here
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

% Graphics folder path
\graphicspath{ {./images/} }

% Hyperlinks and URL setup
\hypersetup{
    bookmarks=true, % show bookmarks bar?
    unicode=false, % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true, % show Acrobat’s toolbar?
    pdfmenubar=true, % show Acrobat’s menu?
    pdffitwindow=false, % window fit to page when opened
    pdfstartview={FitH}, % fits the width of the page to the window
    pdftitle={Final Year Project: Interim Report}, % title
    pdfauthor={Hesam Ipakchi}, % author
    pdfsubject={Final Year Project: Interim Report}, % subject of the document
    pdfcreator={Hesam Ipakchi}, % creator of the document
    pdfproducer={Hesam Ipakchi}, % producer of the document
    pdfkeywords={Hesam Ipakchi, Interim Report}, % list of keywords
    pdfnewwindow=true, % links in new window
    colorlinks=true, % false: boxed links; true: colored links
    linkcolor=black, % color of internal links (change box color with linkbordercolor)
    citecolor=black, % color of links to bibliography
    filecolor=black, % color of file links
    urlcolor=black  % color of external links
}
\urlstyle{same}

% Subreferences Setup
\captionsetup{subrefformat=parens}

%----------------------------------------------------------------------------------------
%	DEFINITIONS OF EQUATION, THEOREM ETC.
%----------------------------------------------------------------------------------------

% Use these for equations, theorems, lemmas, proofs, etc.
\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}[theorem]{Exercise}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

%----------------------------------------------------------------------------------------
%	DEFINITIONS OF NEW COMMANDS
%----------------------------------------------------------------------------------------

% mean command
\newcommand*\mean[1]{\langle#1\rangle}

% absolute value command
\newcommand*\abs[1]{\left\vert#1\right\vert}

%----------------------------------------------------------------------------------------
%	BEGIN DOCUMENT
%----------------------------------------------------------------------------------------

\begin{document}

% Consider all references (including those not cited) for biliography
\nocite{*}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{\textbf{Final Year Project Interim Report}}
\author{Hesam Ipakchi (00648378)\\Imperial College London}
\date{\today}
% make title page without page number in footer
\clearpage\maketitle
\thispagestyle{empty}

%----------------------------------------------------------------------------------------
%	CONTENTS PAGE
%----------------------------------------------------------------------------------------

% Add table of contents on new page
\newpage
\thispagestyle{plain}
\mbox{}

% start page numbers roman style (i,ii,iii...)
\pagenumbering{roman}

% make sure "Contents" is not listed
\tableofcontents

%----------------------------------------------------------------------------------------
%	FIGURES PAGE
%----------------------------------------------------------------------------------------

% Add list of figures on new page
\newpage
\thispagestyle{plain}
\mbox{}
\listoffigures

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\newpage
\thispagestyle{plain}
\mbox{}

% start page numbers arabic style (1,2,3...)
\pagenumbering{arabic} 

\section {Introduction}
\label{sec:introduction}

Networks have been studied extensively to model many interesting complex systems, including the Internet, social networks, financial networks and biological networks \cite{New06a,DKM+13,MG13}. Any network consists of \textit{nodes} which represent items of interest, and \textit{edges} which represent the connectivity between pairs of nodes. For example, considering social networks, nodes are the users and the edges correspond to interactions between the users. An interesting feature many networks exhibit is \textit{community structure}, which involves the natural dividing of nodes into groups, called \textit{communities}, where there are denser connections within a group, and sparser connections between different groups \cite{New06a,DKM+13,For10,New06b}. This particular type of community structure is also known as \textit{assortative} \cite{DKM+13}. For instance, social networks contain communities corresponding to real-life communities consisting of the members, such as friendship or family circles. The problem of detecting communities within networks is known as \textit{community detection}, and we aim to develop algorithms as a solution.

In order to provide a theoretical setting to test and compare different community detection algorithms, generative models of random graphs are very useful, and one such commonly used model is the \textit{stochastic block model} \cite{DKM+13,NN12}. We will investigate several community detection algorithms, and will use the stochastic block model in order to analyse and reason about them.

The underlying ingredients of the community detection algorithms have other interesting applications also, with one being in the problem of \textit{task allocation} and \textit{inference} within crowdsourcing systems. Crowdsourcing systems involve items being rated by several users, known collectively as a crowd, where the user's responses are used to approximate the true rating of the item \cite{KOS13,EHR12}. The users are humans who are paid by the system for their ratings, and these systems have been shown to be effective in solving problems such as image classification, character recognition and recommendation \cite{KOS13}. We wish to use the crowdsourcing system to gain accurate ratings, but also want to reduce the total cost paid out for labour, thus designing algorithms for inference that are budget-optimal is very useful.

We will investigate different crowdsourcing algorithms and use the theoretical approach and setup considered by Karger et al.\cite{KOS13} and Dalvi et al. \cite{DDK+13} to compare and contrast between them.

This report is organised as follows. In section \ref{sec:projectSpecification} we will state clearly the project deliverables. In section \ref{sec:background}, we will outline all the necessary background required to understand the problems studied in the project. In section \ref{sec:implementationPlan}, we will provide a detailed breakdown of work that has been already done and remaining work that is to be done during the rest of the project. Finally, in section \ref{sec:evaluationPlan}, we will detail how the success of the project may be measured.

%----------------------------------------------------------------------------------------
%	PROJECT SPECIFICATION
%----------------------------------------------------------------------------------------

\newpage
\thispagestyle{plain}
\mbox{}
\section {Project Specification}
\label{sec:projectSpecification}

In the following section we will provide a high-level overview of the tasks involved and skills gained during the project in addition to summarising the project deliverables.

A high-level overview of the tasks involved during the project include gaining knowledge of mathematical concepts in graph theory, linear algebra and probability in order to understand common mathematical frameworks of problems such as community detection and crowdsourcing; the ability to analyse random algorithms (in a technical sense) as well as implement them and run simulations through the generation of synthetic data (specific to the framework) in order to draw conclusions regarding performance. This, alongside testing within a real-world setting, enables the provision of sensible recommendations for solving the specific problem.

Succinct project deliverables, alongside detailed explanations, are given below.
\begin{itemize}
	\item \textbf{Understand and analyse different community detection algorithms.}

	We investigate the problem of community detection in graphs, as described in section \ref{subsec:communityDetection}, and analyse different algorithms. These include spectral clustering approaches using \textit{Laplacian} \cite{Spi07,Spi12} and \textit{Modularity} \cite{New06a,New06b} matrices as well as message passing algorithms such as \textit{belief propagation} \cite{DKM+13} and \textit{approximate message passing} \cite{BM11}. We shall use the stochastic block model to generate random graphs that we treat as input to all these algorithms in order to compare them. In particular, we wish to alter the parameters of the generative model to represent different cases in which we can understand the different behaviour of the algorithms. This represents testing using synthetic data.

	\item \textbf{Understand crowdsourcing systems and analyse budget-optimal algorithms.}

	We investigate crowdsourcing systems, as described in section \ref{subsec:crowdsourcingSystems}, and analyse algorithms which aim to optimise cost of such systems whilst providing an accurate estimate to the ground-truth solution. This includes a naive algorithm such as \textit{majority voting}, and algorithms proposed by Karger et al. \cite{KOS13}, Dalvi et al. \cite{DDK+13} and Ghosh et al. \cite{GKM11}. We aim to use the framework previously studied by \cite{KOS13,DDK+13} as a setting to compare the algorithms by generating synthetic data to run simulations.

	\item \textbf{Investigate behaviour of different community detection algorithms using real-world data.}

	We aim to apply community detection algorithms to real-world data, for example, in the form of an Amazon product co-purchasing network made available by Yang and Leskovec \cite{YL12}, or a financial network made up of correlations between different financial assets. By applying the algorithms to these datasets, we can get a better appreciation of the behaviour of algorithms in real-world settings.
\end{itemize}

%----------------------------------------------------------------------------------------
%	BACKGROUND
%----------------------------------------------------------------------------------------

\newpage
\thispagestyle{plain}
\mbox{}
\section {Background}
\label{sec:background}

In the following section we will describe all the technical background required to understand the settings of the problems we investigate for the project. Initially, we will highlight some basic results in graph theory, which the reader may already be familiar with. Then, we will outline the problem of community detection and present a common model used to generate random graphs with community structure to be used as a testing playground for algorithms. Following this, we will discuss crowdsourcing systems and provide information regarding the real-world networks which we wish to consider.

%---------------------
%---   Basics Section   ---
%---------------------
\subsection{Basics}
\label{subsec:backgroundBasics}

We assume the reader is familiar with some basic concepts in linear algebra such as matrix multiplication, eigenvectors and eigenvalues of matrices. Rather, we will cover some basic tools within spectral graph theory using definitions from \cite{For10,New06a, Spi12, Spi07}. Spectral graph theory is the study of graphs through the eigenvalues and eigenvectors of matrices associated with the graphs \cite{Spi12}. We begin by defining some basic notions about graphs.
\begin{definition}
\label{def:graph}
	A graph $\mathcal{G}$ is a pair of sets (V,E), where V is a set of vertices or nodes and $E \subset V^{2}$, the set of unordered pairs of elements of V. The elements of E are called edges or links.
\end{definition}
\begin{definition}
\label{def:undirectedGraph}
	A graph $\mathcal{G} = (V,E)$ is called undirected if for all $v,w \in V$: $(v,w) \in E \iff (w,v) \in E$. Otherwise, $\mathcal{G}$ is called directed.
\end{definition}
\begin{definition}
\label{def:weightedGraph}
	A weighted graph is a graph where a number (weight) is assigned to each edge.
\end{definition}
We will assume, without loss of generality, that $V = \{1,\dots,n\}$. See Figure \ref{fig:exampleGraphSmall} for an example of an undirected graph with seven vertices and eleven edges.
\begin{definition}
\label{def:subGraph}
	A graph $\mathcal{G}^{\prime} = (V^{\prime},E^{\prime})$ is a subgraph of $\mathcal{G} = (V,E)$ if $V^{\prime} \subset V$ and $E^{\prime} \subset E$. If $\mathcal{G}^{\prime}$ contains all edges of $\mathcal{G}$ that join vertices of $V^{\prime}$, one says that the subgraph $\mathcal{G}^{\prime}$ is induced or spanned by $V^{\prime}$.
\end{definition}
\begin{definition}
\label{def:cuts}
	A partition of the vertex set V in two subsets S and $V-S$ is called a cut. The cut size is the number of edges of $\mathcal{G}$ joining vertices of S with vertices of $V-S$.
\end{definition}
\begin{definition}
\label{def:neighbourhoodNode}
	Two vertices are adjacent or neighbours if they are connected by an edge. The set of neighbours of a vertex $v$ is called neighbourhood, and denoted by $\Gamma(v)$.
\end{definition}
\begin{definition}
\label{def:degreeNode}
	The degree $d_{v}$ of a vertex $v$ is the number of its neighbours, $\abs{\Gamma(v)}$.
\end{definition}
We will be interested in using certain graphs is the models, such as bipartite graphs.
\begin{definition}
\label{def:bipartiteGraph}
	A bipartite graph, $\mathcal{G} = (V,E)$, is a graph whose vertices can be decomposed into two disjoint sets such that no two vertices within the same set are adjacent.
\end{definition}
An example of an undirected bipartite graph with nine vertices and eight edges is shown in Figure \ref{fig:exampleGraphBipartite}.


%---   UNCOMMENT: Figure of a example undirected graphs   ---
%\begin{figure}
%\centering
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=0.8\linewidth]{exampleGraphSmall.png}
%		\caption{}
%		\label{fig:exampleGraphSmall}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=0.8\linewidth]{exampleGraphBipartite.png}
%		\caption{}
%		\label{fig:exampleGraphBipartite}
%	\end{subfigure}
%	\caption[Visualisations of example undirected graphs]{\label{fig:exampleGraphs} A set of visualisations of undirected graphs. In \subref{fig:exampleGraphSmall}, the graph has seven nodes and eleven edges. In \subref{fig:exampleGraphBipartite}, a bipartite graph, with nine nodes (elements of disjoint sets are coloured black and red denoting membership) and eight edges, is shown.}
%\end{figure}

There is a very close connection between graphs and matrices, since the whole information about the topology of a graph can be entailed in matrix form, called the \textit{adjacency matrix}.
\begin{definition}
	\label{def:adjacencyMatrix}
	The adjacency matrix, $\mathbfit{A} \in \{0,1\}^{n \times n}$ of a graph $\mathcal{G} = (V,E)$, is a $n\times n$ matrix whose element $A_{ij}$ equals 1 if there exists an edge joining vertices i and j in $\mathcal{G}$, and zero otherwise.
\end{definition}
From definition \ref{def:adjacencyMatrix} it follows that elements of the adjacency matrix, $\mathbf{A}$, can be written as
\begin{equation*}
	A_{ij} =
	\begin{cases}
		1 & \text{if } (i,j) \in E\\
		0 & \text{otherwise}.
	\end{cases}
\end{equation*}
Note that the sum of elements of the $i$-th row of the adjacency matrix yields the degree of node $i$ of the graph, $d_{i} = \sum_{j} A_{ij}$. Also, the adjacency matrix is symmetric if the graph is undirected.

There are other matrices that have also been studied extensively in spectral graph theory, including the Laplacian which is applied in topics such as graph partitioning, synchronisation and graph connectivity \cite{For10}.
\begin{definition}
\label{def:degreeMatrix}
	The degree matrix, $\mathbfit{D}$ of a graph $\mathcal{G} = (V,E)$, is a $n \times n$ diagonal matrix whose element $D_{ii}$ equals the degree of vertex $i$.
\end{definition}
From definition \ref{def:degreeMatrix} it follows that elements of the degree matrix, $\mathbf{D}$, can be written as
\begin{equation*}
	 D_{ij} =
	\begin{cases}
		d_{i} & \text{if } i = j\\
		0 & \text{otherwise}.
	\end{cases}
\end{equation*}
\begin{definition}
\label{def:unnormalisedLaplacianMatrix}
	The matrix $\mathbfit{L} = \mathbfit{D}  - \mathbfit{A} $ is called the unnormalised Laplacian matrix.
\end{definition}
From definition \ref{def:unnormalisedLaplacianMatrix} it follows that elements of the unnormalised Laplacian matrix of a graph $\mathcal{G} = (V,E)$, $\mathbf{L}$, can be written as
\begin{equation*}
	L_{ij} =
	\begin{cases}
		d_{i} & \text{if } i = j\\
		-1 & \text{if } i \neq j \text{ and }  (i,j) \in E\\
		0 & \text{otherwise}.
	\end{cases}
\end{equation*}
\begin{definition}
\label{def:normalisedLaplacianMatrix}
	The matrix $\mathbfit{L_{norm}} = \mathbfit{I}  - \mathbfit{D}^{-1/2}\mathbfit{A}\mathbfit{D}^{-1/2}$ is called the normalised Laplacian matrix, where $\mathbfit{I}$ is the $n \times n$ identity matrix.
\end{definition}
Note that from definitions \ref{def:unnormalisedLaplacianMatrix} and \ref{def:normalisedLaplacianMatrix}, the normalised Laplacian matrix can also be written as $\mathbfit{L_{norm}} = \mathbfit{D}^{-1/2}\mathbfit{L}\mathbfit{D}^{-1/2}$.

An important property of adjacency and Laplacian matrices is their spectra, which we will use, later in the project, to motivate and develop a spectral clustering algorithm for community detection.
\begin{definition}
\label{def:spectrum}
	The spectrum of a graph $\mathcal{G}$ is the set of eigenvalues of its adjacency matrix, $\{\lambda_{1},\dots,\lambda_{n}\}$.
\end{definition}
\begin{definition}
\label{def:spectralRadius}
	Let $\lambda_{1},\dots,\lambda_{n}$ be the eigenvalues of a matrix $\mathbfit{M} \in \mathbb{R}^{n \times n}$. The spectral radius is defined as $\rho(\mathbfit{M}) = \max\limits_{i} \abs{\lambda_{i}}$.
\end{definition}

%-----------------------------------
%---   Community Detection Section   ----
%-----------------------------------
\subsection{Community Detection}
\label{subsec:communityDetection}

An intuitive notion of communities within graphs involves the assignment of nodes to communities such that there are denser connections between nodes belonging to the same community, and sparser connections between nodes belonging to different communities. If a graph exhibits this property, it is said to contain assortative community structure \cite{New06a,DKM+13,For10,New06b}. For instance, within social networks where nodes are users and edges between nodes represent interactions between the users, community structure within the graph corresponds to real-life communities consisting of the users. The aim of community detection algorithms is to estimate or recover the node assignments. The algorithms need to be efficient due to the large size of graphs in real-world networks, so we require the computationally complexity to not be worse than nearly linear in the number of edges in the graph (approximately $O(n^{2}\log n)$ where $n$ represents the number of nodes in the graph).

In order to help provide a setting where different algorithms may be compared, we wish to study particular models which generate random graphs. One popular model is called the stochastic block model. Many special cases of this model have been studied, but we define it using the version considered by Decellle et al. \cite{DKM+13} and Nadakuditi et al. \cite{NN12}, also known as the \textit{planted partition model}. We then consider an interesting phase transition within graphs generated by the stochastic block model.

%---------------------------------------
%---   Stochastic Block Model Sub-Section   ---
%---------------------------------------
\subsubsection{Stochastic Block Model}
\label{subsubsec:stochasticBlockModel}

The stochastic block model has parameters: $n$, $k$, $p_{in}$ and $p_{out}$. $n$ represents the number of nodes, $k$ represents the number of communities, $p_{in}$ represents the probability of an edge occurring between two nodes belonging to the same community and $p_{out}$ represents the probability of an edge occurring between two nodes belonging to the different communities. Since we are focused on assortative community structure, we consider the case where $p_{in} > p_{out}$. We then assign, to each node of the graph, a label indicating which community the node belongs to; so let $\sigma_{i} \in \{1,\dots,k\}$ be the label of node $i$, for $i = 1,\dots,n$. Also, let $\mathbfit{\sigma} = [\sigma_{1},\dots,\sigma_{n}]^{T}$ be the node assignments of the graph. We proceed to generate a random graph, $\mathcal{G}$, on $n$ nodes, $k$ communities and node assignments, $\mathbfit{\sigma} = [\sigma_{1},\dots,\sigma_{n}]^{T}$, with the adjacency matrix of the graph, $\mathbf{A}$ whose elements $(A_{ij})$ are defined as
\begin{equation}
	A_{ij} =
	\begin{cases}
		0 & \text{if } i = j\\
		X & \text{otherwise},
	\end{cases}
\end{equation}
where $X \sim Be(p_{ij})$ and {$p_{ij}$} is defined as
\begin{equation}
	p_{ij} =
	\begin{cases}
		p_{in} & \text{if } \sigma_{i} = \sigma_{j}\\
		p_{out} & \text{otherwise}.
	\end{cases}
\end{equation}
This formulation matches the intuition of assortative community structure, that the connectivity between two nodes depends solely on the community memeberships of the two nodes; specifically there is a greater probability of an edge occurring between nodes belonging to the same community than with belonging to different communities. Note, also, that we do not allow loops in the model.

Typically, community detection within dense graphs (those with a dense adjacency matrix) is straightforward \cite{DKM+13}, thus we are especially interested in community detection within sparse graphs (those with a sparse adjacency matrix) where $p_{in}$, $p_{out} = O(1/n)$. When we consider the sparse regime, it will be more convenient to work with $c_{in} = np_{in}$ and $c_{out} = np_{out}$.

An example of a random graph generated by the stochastic block model is shown in Figure \ref{fig:unlabelledAdjacencyMatrixPlot}. We labelled nodes using $\sigma_{i} = 1 + (i \bmod{k})$ for $i = 1,\dots,n$ and generated the graph with $n = 1000$, $k = 3$, $p_{in} = 0.7$, $p_{out} = 0.3$. The adjacency matrix of this graph is plotted with a pixel shaded red if the element in the adjacency matrix, corresponding to the location of the pixel, equals 1, while a pixel is shaded white if the element equals 0. Since we know the ground truth labelling of nodes, we can, without loss of generality, reorder the rows and columns of the adjacency matrix, such that it consists of blocks of nodes associated with the node community memberships. This is plotted in Figure \ref{fig:labelledAdjacencyMatrixPlot}. Note that since $k = 3$, there are $3 \times 3 = 9$ blocks, where the blocks are denser along the main diagonal since these correspond to edges between nodes belonging to the same community and $p_{in} > p_{out}$.

%---   UNCOMMENT: Figure of adjacency matrix of random graph generated by stochastic block model   ---
%\begin{figure}
%	\centering
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=0.8\linewidth]{adjacencyMatrix_dense.png}
%		\caption{}
%		\label{fig:unlabelledAdjacencyMatrixPlot}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=0.8\linewidth]{labelledAdjacencyMatrix_dense.png}
%		\caption{}
%		\label{fig:labelledAdjacencyMatrixPlot}
%	\end{subfigure}
%	\caption[Plots of adjacency matrix of graph generated by stocahstic block model]{\label{fig:adjacencyMatricesPlots} A set of plots for unlabelled, \subref{fig:unlabelledAdjacencyMatrixPlot}, and labelled, \subref{fig:labelledAdjacencyMatrixPlot}, adjacency matrices for random graph generated by stochastic block model in the dense regime. The graphs were generated with $n = 1000$, $k = 3$, $p_{in} = 0.7$ and $p_{out} = 0.3$.}
%\end{figure}

See Figure \ref{fig:exampleGraphStochasticBlockModel} for a visualisation of an instance of a random graph generated by the stochastic block model with $n = 240$, $k = 3$, $p_{in} = 0.2$, $p_{out} = 0.01$.

%---   UNCOMMENT: Figure of a graph generated by stochastic block model   ---
%\begin{figure}
%	\centering
%	\includegraphics[width=0.6\linewidth]{exampleGraphStochasticBlockModel.png}
%	\caption[Visualisation of a graph generated by the stochastic block model]{\label{fig:exampleGraphStochasticBlockModel} A visualisation of an instance of a random graph generated by the stochastic block model with $n = 240$, $k = 3$, $p_{in} = 0.2$ and $p_{out} = 0.01$.}
%\end{figure}

%-----------------------------------
%---   Phase Transitions Sub-Section   ---
%-----------------------------------
\subsubsection{Phase Transitions}
\label{subsubsec:phaseTransitions}

Decelle et al. \cite{DKM+11} conjectured a phase transition for sparse graphs generated from the stochastic block model, using non-rigorous ideas from statistical physics \cite{MNS12}. Nadakuditi et al. \cite{NN12} used methods from random matrix theory to present an asymptotic analysis of spectra of random graphs to also demonstrate the presence of a phase transition. Essentially, we can distinguish between a \textit{detectable} phase where it is possible to learn node assignments in a way that is correlated with the ground-truth node assignments of the graph, and an \textit{undetectable} phase, where learning is impossible. Consider a graph, generated by the stochastic block model as described in section \ref{subsubsec:stochasticBlockModel}, and following the argument of \cite{NN12}, which we will not explain, one finds a transition occurring at the point
\begin{equation}
\label{eq:phaseTransitionK}
	c_{in} - c_{out} = \sqrt{k(c_{in} + (k-1)c_{out})}.
\end{equation}
In particular, let us consider the case where $k = 2$, so we find a transition at
\begin{equation}
\label{eq:phaseTransitionK=2}
	c_{in} - c_{out} = \sqrt{2(c_{in} + c_{out})}.
\end{equation}
Mossel, Neeman and Sly \cite{MNS12} proved the undetectable phase region of the conjecture given by equation \eqref{eq:phaseTransitionK=2} (that is to say, it is impossible to meaningfully recover the node assignments when $ c_{in} - c_{out} < \sqrt{2(c_{in} + c_{out})}$. Massouli\'e \cite{Mas13} and then, independently using a different proof, Mossel et al. \cite{MNS13b} proved the detectable phase region of the conjecture, meaning it is possible to recover node assignments positively correlated with the ground-truth when $ c_{in} - c_{out} > \sqrt{2(c_{in} + c_{out})}$. The techniques used to prove these results are beyond the scope of the project, however these results provide a very important limit on the ability of algorithms to detect communities. In particular, this motivates the development of algorithms which can efficiently (in nearly linear time) detect communities, in the sparse regime, up to the limit.

%------------------------------------
%---   Crowdsourcing Systems Section   ---
%------------------------------------
\subsection{Crowdsourcing Systems}
\label{subsec:crowdsourcingSystems}

Crowdsourcing systems, such as Amazon Mechanical Turk (\url{http://www.mturk.com}), aim to solve tasks using human-powered problem solving, where humans, which we will call users (collectively known as the crowd), are paid a small fee for their rating of an item \cite{KOS13,EHR12}. The aim of crowdsourcing algorithms is to interpret the user's responses to infer the true rating. Many platforms including Amazon Mechanical Turk have a large, anonymous, crowd as a workforce so it is difficult to leverage the fee paid on correct ratings since they may be subjective and true ratings not known \cite{KOS13}. Therefore, most crowdsourcing systems introduce redundancy by asking for a rating to each item from several users and aggregating the results by giving each user's response the same weighting \cite{GKM11,KOS13,DDK+13}.

The problem of assigning items to users (i.e. deciding which set of items each user should rate) is called task allocation \cite{KOS13}. Even though users are not identifiable and their reliability (ability to rate items correctly) is not known, we aim to develop an algorithm that can estimate user's reliabilities in order to apply an appropriate weighting to user's ratings to get a more accurate estimate. The problem of estimating the true rating of an item from the set of user's ratings is called inference \cite{KOS13}. In particular, we will be interested in binary ratings, where there are only two possibilities (e.g. good or bad) \cite{GKM11}. An example of such a problem is image classification where we have a set of images (the items), and each one can be classified into two possible states by the user (analogous to the process of rating the item) \cite{KOS13, DKF13}. Crowdsourcing is used to solve this task since humans can distinguish visual features of images and thus should be able to classify them accurately \cite{DKF13}.

%------------------------------------
%---   Probabilistic Model Sub-Section   ---
%------------------------------------
\subsubsection{Probabilistic Model}
\label{subsubsec:crowdsourcingProbabilisticModel}

We will now describe the probabilistic model considered by Karger et al. \cite{KOS13}, which is very similar to those considered by \cite{GKM11,DDK+13}.

There are a set of $m$ binary items with unobserved ratings, $\mathbfit{t} = [t_{1},\dots,t_{m}]^{T}$, where the true rating of the $i$-th item is $t_{i} \in \{-1,1\}$ $\text{ for } i=1,\dots,m$. Assume we need $n$ users in the crowd, of whom we allocate tasks to. Referring to task allocation, denote $T_{j} \subseteq \{1,\dots,m\}$ as the set of items assigned to the $j$-th user, with a limit of $r$ items that can be assigned to an individual to avoid overloading a user, meaning $\abs{T_{j}} \le r \text{ for } j=1,\dots,n$.

Let us denote the reliability (probability of rating an item correctly) of the $j$-th user by $p_{j} \in [0,1]$, with the crowd reliability characterised as $\mathbf{p} = [p_{1},\dots,p_{n}]^{T}$. Note that the reliability of each user is the same no matter which item is being attempted, so here we do not consider any notion of difficulty of rating items. We also emphasize that the user reliability values $p_{j}$ are unknown to the system (and thus the algorithms), and accurate approximation of them within algorithms will be important for achieving low error rates.

The matrix of user's responses is called the \textit{response matrix}, $\mathbf{R} \in \{-1,0,1\}^{m \times n}$, and is defined by
\begin{equation}
	R_{ij} =
	\begin{cases}
		0 & \text{if } i \not \in T_{j}\\
		t_{i} & \text{with probability }  p_{j} \text{ if } i \in T_{j}\\
		-t_{i} & \text{otherwise}.
	\end{cases}
\end{equation}

We assume the user's reliabilities are independent and identically distributed random variables as defined by the \textit{spammer-hammer model} and a `collective quality' parameter denoted by $q$,
\begin{equation}
	p_{j} =
	\begin{cases}
		1/2 & \text{with probability } 1-q\\
		1 & \text{otherwise}.
	\end{cases}
\end{equation}
This model assumes the crowd consists of only two types of users, a `hammer' and a `spammer'. A hammer rates all items correctly, so can be thought of as perfectly reliable users with their reliability equal to $1$. A spammer rates items by giving random responses, essentially flipping a fair coin and using the result as their answer, meaning their reliability is equal to $1/2$.  Moreover, since the distribution of $\mathbf{p}$ characterises the crowd, we can define the collective quality parameter by
\begin{equation}
	q = \mathbb{E}[(2\mathbf{p}-1)^{2}].
\end{equation}
Note that the parameter in the spammer-hammer model is consistent with this definition of $q$.
Clearly, using this model with larger values of $q$ increases the portion of hammers and thus the intelligence of the crowd. We will be interested how the error rate of different algorithms vary as we alter the value of $q$.

We would like to incorporate a notion of cost, where we pay for each user's response. Let us assume each item gets allocated $\ell$ times, where we can increase $\ell$ if we have more budget. Recall that we have $m$ items and must estimate a rating for all of them, and that each user completes $r$ tasks in order to not overload any one individual. Using this, we can easily determine the number of users we require, $n = m\ell/r$. Note that, intuitively, we expect the error rate to decrease as we increase $\ell$, since we have a larger budget, but we will look at the error rates of different algorithms vary as we alter $\ell$.

Furthermore, we can associate the task allocation scheme and the response matrix with a bipartite graph, as follows. Consider a bipartite graph, $\mathcal{G} = (V,E)$, with $m$ `item' nodes and $n$ `user' nodes, where we can decompose $V=\{1,\dots,m+n\}$ into two disjoint sets, $V_{t}$ and $V_{u}$, where $\abs{V_{t}} = m$, $\abs{V_{u}} = n$, $V_{t}$ consists only of item nodes and $V_{u}$ consists only of user nodes. Then we can place an edge within this graph whenever the $i$-th item is assigned to the $j$-th user, with a weight on the edge equal to the user's response, essentially the corresponding entry of the response matrix, $R_{ij}$. A task allocation scheme is, therefore, just a method of placing edges within $\mathcal{G}$ in accordance with the constrained degrees of item and user nodes. We will investigate the performance of different inference algorithms based upon different task allocation schemes.

%----------------------------------------
%---   Real-world data applications Section   ---
%----------------------------------------
\subsection{Real-world data applications}
\label{subsec:communityDetectionApplicationToRealWorldData}

We will apply the community detection algorithms to real-world networks which exhibit community structure.

%-------------------------------------------------------
%---   Amazon Product Co-Purchasing Network Sub-Section   ---
%-------------------------------------------------------
\subsubsection{Amazon Product Co-Purchasing Network}
\label{subsubsec:amazonNetwork}

One example is an Amazon product co-purchasing network made available by Yang and Leskovec \cite{YL12}. The data is available to download from \url{http://snap.stanford.edu/data/com-Amazon.html}, and as explained on the site, the network was collected by crawling the Amazon website, and creating an undirected graph with nodes representing products and edges present between node $i$ and $j$ if product $i$ is frequently co-purchased with product $j$. They regard each connected component in a product category as separate ground-truth community, and provide the top 5,000 communities with `highest quality' as described in \cite{YL12}. Overall, this network has 334,683 nodes and 925,872 edges.

%------------------------------------
%---   Financial Networks Sub-Section   ---
%------------------------------------
\subsubsection{Financial Networks}
\label{subsubsec:financialNetworks}

Another example of an application is the use of community detection in financial networks, where the aim is to identify groups of nodes with stronger correlation internally than with the rest of the network \cite{MG13}. We will investigate the problem where the network is a weighted, undirected and fully-connected (there is an edge connecting every pair of nodes) graph, with nodes representing assets and the weight of an edge representing the correlation between the assets associated with the nodes connected by that edge. Contrary to graphs with community structure considered earlier in the project, the weights of the edges are crucial in determining community memberships. We will consider the following model used by \cite{OCK+02,OKK03,FPM+10,MG13}.

Firstly, consider $n$ nodes in the graph, where each node represents a financial asset (e.g. stock), and assign to the $i$-th node a single time series, $X_{i}$, defined as
\begin{equation}
\label{eq:singleTimeSeries}
	X_{i} = \{x_{i}(1),\dots,x_{i}(T)\}
\end{equation}
This time series describes the evolution of the logarithmic return of the asset over $T$ time steps, trading days for instance, so that $x_{i}(t) = \ln p_{i}(t) - \ln p_{i}(t-1)$ where $p_{i}(t)$ is the price of asset $i$ at time $t$. We then model the weight of an edge connecting nodes $i$ and $j$ of the graph as the cross-correlation between the time series associated with those nodes, and specifically define a cross-correlation matrix, $\mathbf{C}$, whose elements $(C_{ij})$ are
\begin{equation}
\label{eq:crossCorrelationMatrix}
	C_{ij} = \frac{\mean{X_{i} X_{j}} - \mean{X_{i}} \mean{X_{j}}}{\sigma_{i} \cdot \sigma_{j}}
\end{equation}
where
\begin{equation}
\label{eq:temporalVariance}
	\sigma_{i}^{2} = \mean{X_{i}^{2}} - \mean{X_{i}}^{2}
\end{equation}
is the sample variance of $X_{i}$, and the $\mean{\cdot}$ notation denotes a time average, so
\begin{equation}
\label{eq:temporalMean}
	\mean{X_{i}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}(t)
\end{equation}
\begin{equation}
\label{eq:temporalMeanSquare}
	\mean{X_{i}^{2}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}^{2}(t)
\end{equation}
\begin{equation}
\label{eq:temporalMeanProduct}
	\mean{X_{i}X_{j}} = \frac{1}{T} \sum_{t=1}^{T} x_{i}(t)x_{j}(t)
\end{equation}
We also assume each time series $X_{i}$ has been standardised (by using $X_{i} \coloneqq (X_{i} - \mean{X_{i}}) / \sigma_{i}$), so that
\begin{equation}
\label{eq:zeroTemporalMean}
	\mean{X_{i}} = 0
\end{equation}
\begin{equation}
\label{eq:unitTemporalVariance}
	\sigma_{i} = 1
\end{equation}

Now that we have defined our model, we aim to partition the network into communities of assets whose members have a larger correlation between themselves than with the rest of the graph.

In order to find such partitions, we will modify the community detection algorithms we considered for the first part of the project (the modification is necessary since we no longer consider the connectivity of nodes in the graph but rather the weights of the edges). We will use datasets corresponding to different indexes, in order to test the behaviour of algorithms in recovering community structure. For example, one dataset is available from \url{http://jponnela.com}, which contains 4787 price quotes for 116 NYSE stocks from the S\&P 500 index. Also, approximately 2500 daily closing prices of 87 FTSE 100 stocks (from 2004 to 2013) have been collected from \url{http://uk.finance.yahoo.com}, and parsed to form another dataset.

%----------------------------------------------------------------------------------------
%	IMPLEMENTATION PLAN
%----------------------------------------------------------------------------------------

\newpage
\thispagestyle{plain}
\mbox{}
\section {Implementation Plan}
\label{sec:implementationPlan}

In this section we will outline the work that is to be done in the remainder of the project as well as the work that has already been completed. We provide a set of milestones and additionally an estimated timescale when each one should be completed. Moreover, we discuss fallback positions in case certain milestones can not be achieved as planned.

Details of the work that has been done so far is given below.
\begin{itemize}
	\item \textbf{Research, understand and implement community detection algorithms.}

	\textit{Status: Almost Completed.}	

	We have spent several months researching the problem of community detection in graphs. Firstly, we analysed the problem specifications, essentially the literature summarised in section \ref{subsec:communityDetection}. Then we proceeded to survey different community detection algorithms, including spectral clustering (using Laplacian and Modularity matrices) and message passing (such as belief propagation and approximate message passing). We have also implemented some of these methods (spectral clustering so far) and applied them to synthetic data generated by the stochastic block model, using MATLAB and C++ implementations.

	\item \textbf{Research, understand and implement crowdsourcing algorithms.}

	\textit{Status: Completed.}

	Recently, we turned our attention to the crowdsourcing problem. Starting by surveying the relevant literature explaining the problem, as summarised in section \ref{subsec:crowdsourcingSystems}, we proceeded to implement the basic setting and algorithms such as majority voting, singular vector and iterative message passing.
\end{itemize}

Details of work that will be done in the remainder of the project is given below.
\begin{itemize}
	\item \textbf{Finish implementing message passing algorithms for community detection.}

	\textit{Status: Estimated completion date of March 2014.}	

	We need to finish implementing the message passing community detection algorithms, so that we can apply them to the synthetic data for comparison across different settings.

	\item \textbf{Analysis and comparison of community detection algorithms using synthetic data.}

	\textit{Status: Estimated completion date of April 2014.}	

	We aim to utilise our implementations of these algorithms in order to test them among different synthetic data settings (by altering parameters of model such as sparsity, number of communities and size of communities). The results of these tests, within the generative model as described in section \ref{subsubsec:stochasticBlockModel}, will allow us to thoroughly compare the algorithms and provide recommendations (i.e. which algorithm we believe will perform best) for specific problem settings.

	\item \textbf{Analysis of different algorithms for crowdsourcing systems using synthetic data.}

	\textit{Status: Estimated completion date of May 2014.}	

	We wish to analyse the algorithms for crowdsourcing systems using synthetic data generated by the model described in section \ref{subsubsec:crowdsourcingProbabilisticModel}.

	\item \textbf{Apply community detection algorithms to real-world data.}

	\textit{Status: Estimated completion date of June 2014.}	

	We wish to apply some of the community detection algorithms we have studied to real-world networks described in section \ref{subsec:communityDetectionApplicationToRealWorldData}. This involves parsing the data taken into a suitable format and using the Stanford Network Analysis Platform (SNAP) C++ library, available from \url{http://snap.stanford.edu/snap/index.html}, to represent the large network and implement suitable algorithms.
\end{itemize}

Details of fall back options in case any of the planned milestones can not be achieved is summarised below.
\begin{itemize}
	\item \textbf{Only consider financial networks for real-world data applications.}

	The difficulty in apply algorithms to real-world datasets revolves around the size of the graphs considered. For example, the Amazon co-purchasing network, described in section \ref{subsubsec:amazonNetwork}, has almost a million edges and storing and operating on this data requires far more attention and care than smaller graphs (below 1000 nodes for instance) that we would consider for financial networks. Thus, we would implement modified version of the algorithms to financial networks, with some examples are mentioned in section \ref{subsubsec:financialNetworks}, and not on social network data.
\end{itemize}

%----------------------------------------------------------------------------------------
%	EVALUATION PLAN
%----------------------------------------------------------------------------------------

\newpage
\thispagestyle{plain}
\mbox{}
\section {Evaluation Plan}
\label{sec:evaluationPlan}

In this section we will explain how the success of the project may be measured.

The best way to measure the success of the project involves reference to the project deliverables described in section \ref{sec:projectSpecification}.

Firstly, we intend to deliver a detailed analysis of community detection algorithms, where we compare them in terms of their respective performance when applied to synthetic networks. Thus, for this deliverable, evaluating the quality of the survey including appropriate demonstrations of performance under selective conditions (obtained by tuning the parameters in the generative model), suffices.

Secondly, we intend to understand and compare different crowdsourcing algorithms; hence evaluating the quality of the report comparing these algorithms including applications to synthetically generated data, would be a good measure.

Finally, we wish to apply community detection algorithms to real-world networks. In particular, evaluating the success of detecting groups of correlated assets in financial networks (created by the datasets described in section \ref{subsubsec:financialNetworks}) will be important. Discussions, centred around the findings created by applying modified community detection algorithms and, in particular, their importance in portfolio optimisation, represent the essence of this deliverable.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% Add complete bibliography on new page
\newpage
\thispagestyle{plain}
\mbox{}
\bibliographystyle{hieeetr.bst}
\bibliography{interimReportBibliography}

%----------------------------------------------------------------------------------------
%	END DOCUMENT
%----------------------------------------------------------------------------------------

\end{document}